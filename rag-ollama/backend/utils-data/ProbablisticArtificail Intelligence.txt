Probabilistic
Artificial Intelligence
Andreas Krause, Jonas Hübotter

Institute for Machine Learning
Department of Computer Science

arXiv:2502.05244v1  [cs.AI]  7 Feb 2025



Compiled on February 11, 2025.

This manuscript is based on the course Probabilistic Artificial Intelligence (263-5210-00L) at ETH Zürich.

This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.

© 2025 ETH Zürich. All rights reserved.



Preface

Artificial intelligence commonly refers to the science and engineering
of artificial systems that can carry out tasks generally associated with
requiring aspects of human intelligence, such as playing games, trans-
lating languages, and driving cars. In recent years, there have been
exciting advances in learning-based, data-driven approaches towards
AI, and machine learning and deep learning have enabled computer
systems to perceive the world in unprecedented ways. Reinforcement
learning has enabled breakthroughs in complex games such as Go and
challenging robotics tasks such as quadrupedal locomotion.

A key aspect of intelligence is to not only make predictions, but reason
about the uncertainty in these predictions, and to consider this uncer-
tainty when making decisions. This is what “Probabilistic Artificial
Intelligence” is about. The first part covers probabilistic approaches to
machine learning. We discuss the differentiation between “epistemic”
uncertainty due to lack of data and “aleatoric” uncertainty, which is
irreducible and stems, e.g., from noisy observations and outcomes.
We discuss concrete approaches towards probabilistic inference, such
as Bayesian linear regression, Gaussian process models and Bayesian
neural networks. Often, inference and making predictions with such
models is intractable, and we discuss modern approaches to efficient
approximate inference.

The second part of the manuscript is about taking uncertainty into
account in sequential decision tasks. We consider active learning and
Bayesian optimization — approaches that collect data by proposing ex-
periments that are informative for reducing the epistemic uncertainty.
We then consider reinforcement learning, a rich formalism for mod-
eling agents that learn to act in uncertain environments. After cov-
ering the basic formalism of Markov Decision Processes, we consider
modern deep RL approaches that use neural network function approx-
imation. We close by discussing modern approaches in model-based
RL, which harness epistemic and aleatoric uncertainty to guide explo-
ration, while also reasoning about safety.



iv

Guide to the Reader

The material covered in this manuscript may support a one semester
graduate introduction to probabilistic machine learning and sequential
decision-making. We welcome readers from all backgrounds. How-
ever, we assume familiarity with basic concepts in probability, calculus,
linear algebra, and machine learning (e.g., neural networks) as covered
in a typical introductory course to machine learning. In Chapter 1, we
give a gentle introduction to probabilistic inference, which serves as
the foundation for the rest of the manuscript. As part of this first chap-
ter, we also review key concepts from probability theory. We provide
a chapter reviewing key concepts of further mathematical background
in the back of the manuscript.

Throughout the manuscript, we focus on key concepts and ideas rather
than their historical development. We encourage you to consult the
provided references for further reading and historical context to delve
deeper into the covered topics.

Finally, we have included a set of exercises at the end of each chapter.
When we highlight an exercise throughout the text, we use this ques-
tion mark: ? — so don’t be surprised when you stumble upon it. You Problem 1.1
will find solutions to all exercises in the back of the manuscript.

We hope you will find this resource useful.

Contributing

We encourage you to raise issues and suggest fixes for anything you
think can be improved. We are thankful for any such feedback!
Contact: pai-script@lists.inf.ethz.ch

Acknowledgements

We are grateful to Sebastian Curi for creating the original Jupyter note-
books that accompany the course at ETH Zürich and which were in-
strumental in the creation of many figures. We thank Hado van Hasselt
for kindly contributing Figure 12.1, and thank Tuomas Haarnoja (Haarnoja
et al., 2018a) and Roberto Calandra (Chua et al., 2018) for kindly agree-
ing to have their figures included in this manuscript. Furthermore,
many of the exercises in these notes are adapted from iterations of the
course at ETH Zürich. Special thanks to all instructors that contributed
to the course material over the years. We also thank all students of
the course in the Fall of 2022, 2023, and 2024 who provided valuable
feedback on various iterations of this manuscript and corrected many



v

mistakes. Finally, we thank Zhiyuan Hu, Shyam Sundhar Ramesh,
Leander Diaz-Bone, Nicolas Menet, and Ido Hakimi for proofreading
parts of various drafts of this text.






Contents

1 Fundamentals of Inference 1

1.1 Probability 2

1.2 Probabilistic Inference 15

1.3 Supervised Learning and Point Estimates 22

1.4 Outlook: Decision Theory 29

I Probabilistic Machine Learning 35

2 Linear Regression 39

2.1 Weight-space View 40

2.2 Aleatoric and Epistemic Uncertainty 44

2.3 Non-linear Regression 45

2.4 Function-space View 45

3 Filtering 51

3.1 Conditioning and Prediction 53

3.2 Kalman Filters 54

4 Gaussian Processes 59

4.1 Learning and Inference 60

4.2 Sampling 61

4.3 Kernel Functions 62

4.4 Model Selection 67

4.5 Approximations 70



viii

5 Variational Inference 83

5.1 Laplace Approximation 83

5.2 Predictions with a Variational Posterior 87

5.3 Blueprint of Variational Inference 88

5.4 Information Theoretic Aspects of Uncertainty 89

5.5 Evidence Lower Bound 100

6 Markov Chain Monte Carlo Methods 113

6.1 Markov Chains 114

6.2 Elementary Sampling Methods 121

6.3 Sampling using Gradients 124

7 Deep Learning 139

7.1 Artificial Neural Networks 139

7.2 Bayesian Neural Networks 142

7.3 Approximate Probabilistic Inference 144

7.4 Calibration 152

II Sequential Decision-Making 157

8 Active Learning 161

8.1 Conditional Entropy 161

8.2 Mutual Information 163

8.3 Submodularity of Mutual Information 166

8.4 Maximizing Mutual Information 168

8.5 Learning Locally: Transductive Active Learning 172

9 Bayesian Optimization 177

9.1 Exploration-Exploitation Dilemma 177

9.2 Online Learning and Bandits 178

9.3 Acquisition Functions 180

10 Markov Decision Processes 197

10.1 Bellman Expectation Equation 199

10.2 Policy Evaluation 201



ix

10.3 Policy Optimization 203

10.4 Partial Observability 209

11 Tabular Reinforcement Learning 217

11.1 The Reinforcement Learning Problem 217

11.2 Model-based Approaches 219

11.3 Balancing Exploration and Exploitation 220

11.4 Model-free Approaches 224

12 Model-free Reinforcement Learning 233

12.1 Tabular Reinforcement Learning as Optimization 233

12.2 Value Function Approximation 235

12.3 Policy Approximation 238

12.4 On-policy Actor-Critics 244

12.5 Off-policy Actor-Critics 251

12.6 Maximum Entropy Reinforcement Learning 256

12.7 Learning from Preferences 260

13 Model-based Reinforcement Learning 273

13.1 Planning 274

13.2 Learning 281

13.3 Exploration 287

A Mathematical Background 299

A.1 Probability 299

A.2 Quadratic Forms and Gaussians 301

A.3 Parameter Estimation 302

A.4 Optimization 313

A.5 Useful Matrix Identities and Inequalities 319

B Solutions 321

Bibliography 385

Summary of Notation 393



x

Acronyms 399

Index 401



1
Fundamentals of Inference

Boolean logic is the algebra of statements which are either true or false.
Consider, for example, the statements

“If it is raining, the ground is wet.” and “It is raining.”

A quite remarkable property of Boolean logic is that we can combine
these premises to draw logical inferences which are new (true) state-
ments. In the above example, we can conclude that the ground must
be wet. This is an example of logical reasoning which is commonly
referred to as logical inference, and the study of artificial systems that
are able to perform logical inference is known as symbolic artificial in-
telligence.

But is it really raining? Perhaps it is hard to tell by looking out of the
window. Or we have seen it rain earlier, but some time has passed
since we have last looked out of the window. And is it really true that
if it rains, the ground is wet? Perhaps the rain is just light enough that
it is absorbed quickly, and therefore the ground still appears dry.

This goes to show that in our experience, the real world is rarely black
and white. We are frequently (if not usually) uncertain about the truth
of statements, and yet we are able to reason about the world and make
predictions. We will see that the principles of Boolean logic can be ex-
tended to reason in the face of uncertainty. The mathematical frame-
work that allows us to do this is probability theory, which — as we
will find in this first chapter — can be seen as a natural extension of
Boolean logic from the domain of certainty to the domain of uncer-
tainty. In fact, in the 20th century, Richard Cox and Edwin Thompson
Jaynes have done early work to formalize probability theory as the
“logic under uncertainty” (Cox, 1961; Jaynes, 2002).

In this first chapter, we will briefly recall the fundamentals of prob-
ability theory, and we will see how probabilistic inference can be used



2 probabilistic artificial intelligence

to reason about the world. In the remaining chapters, we will then
discuss how probabilistic inference can be performed efficiently given
limited computational resources and limited time, which is the key
challenge in probabilistic artificial intelligence.

1.1 Probability

Probability is commonly interpreted in two different ways. In the fre-
quentist interpretation, one interprets the probability of an event (say
a coin coming up “heads” when flipping it) as the limit of relative
frequencies in repeated independent experiments. That is,

# events happening in N trials
Probability = lim .

N→∞ N

This interpretation is natural, but has a few issues. It is not very dif-
ficult to conceive of settings where repeated experiments do not make
sense. Consider the outcome:

“Person X will live for at least 80 years.”

There is no way in which we could conduct multiple independent ex-
periments in this case. Still, this statement is going to turn out either
true or false, as humans we are just not able to determine its truth
value beforehand. Nevertheless, humans commonly have beliefs about
statements of this kind. We also commonly reason about statements
such as

“The Beatles were more groundbreaking than The Monkees.”

This statement does not even have an objective truth value, and yet we
as humans tend to have opinions about it.

While it is natural to consider the relative frequency of the outcome
in repeated experiments as our belief, if we are not able to conduct
repeated experiments, our notion of probability is simply a subjective
measure of uncertainty about outcomes. In the early 20th century,
Bruno De Finetti has done foundational work to formalize this notion
which is commonly called Bayesian reasoning or the Bayesian interpre-
tation of probability (De Finetti, 1970).

We will see that modern approaches to probabilistic inference often
lend themselves to a Bayesian interpretation, even if such an interpre-
tation is not strictly necessary. For our purposes, probabilities will be
a means to an end: the end usually being solving some task. This task
may be to make a prediction or to take an action with an uncertain
outcome, and we can evaluate methods according to how well they
perform on this task. No matter the interpretation, the mathematical



fundamentals of inference 3

framework of probability theory which we will formally introduce in
the following is the same.

1.1.1 Probability Spaces

A probability space is a mathematical model for a random experiment.
The set of all possible outcomes of the experiment Ω is called sample
space. An event A ⊆ Ω of interest may be any combination of possible
outcomes. The set of all events A ⊆ P(Ω) that we are interested in
is often called the event space of the experiment.1 This set of events is 1 We use P(Ω) to denote the power set
required to be a σ-algebra over the sample space. (set of all subsets) of Ω.

Definition 1.1 (σ-algebra). Given the set Ω, the set A ⊆ P(Ω) is a
σ-algebra over Ω if the following properties are satisfied:
1. Ω ∈ A;
2. if A ∈ A, then A ∈ A (closedness und⋃er complements); and
3. if we have Ai ∈ A for all i, then ∞

i=1 Ai ∈ A (closedness under
countable unions).

Note that the three properties of σ-algebras correspond to character-
istics we universally expect when working with random experiments.
Namely, that we are able to reason about the event Ω that any of the
possible outcomes occur, that we are able to reason about an event
not occurring, and that we are able to reason about events that are
composed of multiple (smaller) events.

Example 1.2: Event space of throwing a die
The event space A can also be thought of as “how much infor-
mation is available about the experiment”. For example, if the
experiment is a throw of a die and Ω is the set of possible values
on the die: Ω = {1, . . . , 6}, then the following A implies that the
observer cannot distinguish between 1 and 3:

A .
= {∅, Ω, {1, 3, 5}, {2, 4, 6}}.

Intuitively, the observer only understands the parity of the face of
the die.

Definition 1.3 (Probability measure). Given the set Ω and the σ-algebra
A over Ω, the function

P : A → R

is a probability measure on A if the Kolmogorov axioms are satisfied:
1. 0 ≤ P(A) ≤ 1 for any A ∈ A;
2. P(Ω) = 1; and



4 probabilistic artificial intelligence

⋃
3. P( ∞

i=1 Ai) = ∑∞
i=1 P(Ai) for any countable set of mutually disjoint

events {Ai ∈ A}i.2 2 We say that a set of sets {Ai}i is disjoint
if for all i ̸= j we have Ai ∩ Aj = ∅.

Remarkably, all further statements about probability follow from these
three natural axioms. For an event A ∈ A, we call P(A) the probability
of A. We are now ready to define a probability space.

Definition 1.4 (Probability space). A probability space is a triple (Ω,A, P)
where
• Ω is a sample space,
• A is a σ-algebra over Ω, and
• P is a probability measure on A.

Example 1.5: Borel σ-algebra over R

In our context, we often have that Ω is the set of real numbers R

or a compact subset of it. In this case, a natural event space is the
σ-algebra generated by the set of events

A .
= {x′ ∈ Ω : x′x ≤ x}.

The smallest σ-algebra A containing all sets Ax is called the Borel
σ-algebra. A contains all “reasonable” subsets of Ω (except for
some pathological examples). For example, A includes all single-
ton sets {x}, as well as all countable unions of intervals.

In the case of discrete Ω, in fact A = P(Ω), i.e., the Borel σ-
algebra contains all subsets of Ω.

1.1.2 Random Variables

The set Ω is often rather complex. For example, take Ω to be the set of
all possible graphs on n vertices. Then the outcome of our experiment
is a graph. Usually, we are not interested in a specific graph but rather
a property such as the number of edges, which is shared by many
graphs. A function that maps a graph to its number of edges is a
random variable.

Definition 1.6 (Random variable). A random variable X is a function

X : Ω→ T

where T is called target space of the random variable,3 and where X 3 For a random variable that maps a
respects the information available in the σ-algebra A. That is,4 graph to its number of edges, T = N0.

For our purposes, you can generally as-
sume T ⊆ R.

∀S ⊆ T : {ω ∈ Ω : X(ω) ∈ S} ∈ A. (1.1) 4 In our example of throwing a die, X
should assign the same value to the out-
comes 1, 3, 5.



fundamentals of inference 5

Concrete values x of a random variable X are often referred to as states
or realizations of X. The probability that X takes on a value in S ⊆ T is

P(X ∈ S) = P({ω ∈ Ω : X(ω) ∈ S}). (1.2)

1.1.3 Distributions

Consider a random variable X on a probability space (Ω,A, P), where
Ω is a compact subset of R, and A the Borel σ-algebra.

In this case, we can refer to the probability that X assumes a particular
state or set of states by writing

p .
X(x) = P(X = x) (in the discrete setting), (1.3)

PX(x .
) = P(X ≤ x). (1.4)

Note that “X = x” and “X ≤ x” are merely events (that is, they char-
acterize subsets of the sample space Ω satisfying this condition) which
are in the Borel σ-algebra, and hence their probability is well-defined.

Hereby, pX and PX are referred to as the probability mass function
(PMF) and cumulative distribution function (CDF) of X, respectively.
Note that we can also implicitly define probability spaces through ran-
dom variables and their associated PMF/CDF, which is often very con-
venient.

We list some common examples of discrete distributions in Appendix A.1.1.
Further, note that for continuous variables, P(X = x) = 0. Here, in-
stead we typically use the probability density function (PDF), to which
we (with slight abuse of notation) also refer with pX . We discuss den-
sities in greater detail in Section 1.1.4.

We call the subset S ⊆ T of the domain of a PMF or PDF pX such that
all elements x ∈ S have positive probability, pX(x) > 0, the support of
the distribution pX . This quantity is denoted by X(Ω).

1.1.4 Continuous Distributions

As mentioned, a continuous random variable can be characterized by
its probability density function (PDF). But what is a density? We can
derive some intuition from physics.

Let M be a (non-homogeneous) physical object, e.g., a rock. We com-
monly use m(M) and vol(M) to refer to its mass and volume, respec-
tively. Now, consider for a point x ∈ M and a ball Br(x) around x with
radius r the following quantities:

lim vol(Br(x)) = 0 lim m(B )) = 0
r→ r(x .

r→0 0



6 probabilistic artificial intelligence

They appear utterly uninteresting at first, yet, if we divide them, we
get what is called the density of M at x.

m(B )
lim r(x) .

= ρ(x).
r→0 vol(Br(x))

We know that the relationship between density and mass is described
by the following formula: ∫

m(M) = ρ(x) dx.
M

In other words, the density is to be integrated. For a small region I
around x, we can approximate m(I) ≈ ρ(x) · vol(I).

Crucially, observe that even though the mass of any particular point
x is zero, i.e., m({x}) = 0, assigning a density ρ(x) to x is useful for
integration and approximation. The same idea applies to continuous
random variables, only that volume corresponds to intervals on the
real line and mass to probability. Recall that probability density func-
tions are normalized such that their probability mass across the entire N (x; 0, 1)

real line integrates to one. 0.4

0.3

Example 1.7: Normal distribution / Gaussian 0.2

A famous example of a continuous distribution is the normal dis- 0.1

tribution, also called Gaussian. We say, a random variable X is 0.0
normally distributed, X ∼ N (µ, σ2), if it(s PDF is ) −2 0 2

x

N . √ 1 (x− µ)2
(x; µ, σ2) = exp − . (1.5) Figure 1.1: PDF of the standard normal

2πσ2 2σ2
distribution. Observe that the PDF is
symmetric around the mode.

We have E[X] = µ and Var[X] = σ2. If µ = 0 and σ2 = 1, this dis-
tribution is called the standard normal distribution. The Gaussian
CDF cannot be expressed in closed-form.

Note that the mean of a Gaussian distribution coincides with the
maximizer of its PDF, also called mode of a distribution.

We will focus in the remainder of this chapter on continuous distribu-
tions, but the concepts we discuss extend mostly to discrete distribu-
tions simply by “replacing integrals by sums”.

1.1.5 Joint Probability

A joint probability (as opposed to a marginal probability) is the prob-
ability of two or more events occurring simultaneously:

P(A, B .
) = P(A ∩ B). (1.6)



fundamentals of inference 7

In terms of random variables, this concept extends to joint distribu-
tions. Instead of characterizing a single random variable, a joint dis-
tribution is a function p n

X : R → R, characterizing a random vector
X .
= [X1 · · · Xn]⊤. For example, if the Xi are discrete, the joint distri-

bution characterizes joint probabilities of the form

P(X = [x1, . . . , xn]) = P(X1 = x1, . . . , Xn = xn),

and hence describes the relationship among all variables Xi. For this
reason, a joint distribution is also called a generative model. We use Xi:j
to denote the random vector [Xi · · · X ⊤

j] .

We can “sum out” (respectively “integrate out”) variables from a joint
distribution in a process called “marginalization”:

Fact 1.8 (Sum rule). We have∫that

p(x1:i−1, xi+1:n) = p(x1:i−1, xi, xi+1:n) dxi. (1.7)
Xi(Ω)

1.1.6 Conditional Probability

Conditional probability updates the probability of an event A given
some new information, for example, after observing the event B.

Definition 1.9 (Conditional probability). Given two events A and B
such that P(B) > 0, the probability of A conditioned on B is given as

A
. P(A, B) B

P(A | B) = . (1.8)
P(B) Ω

Figure 1.2: Conditioning an event A on
Simply rearranging the terms yields, another event B can be understood as re-

placing the universe of all possible out-
P(A, B) = P(A | B) ·P(B) = P(B | A) ·P(A). (1.9) comes Ω by the observed outcomes B.

Then, the conditional probability is sim-
Thus, the probability that both A and B occur can be calculated by ply expressing the likelihood of A given

multiplying the probability of event A and the probability of B condi- that B occurred.

tional on A occurring.

We say Z ∼ X | Y = y (or simply Z ∼ X | y) if Z follows the conditional
distribution

(x, y)
p x | y . p

X|Y( ) = X,Y . (1.10)
pY(y)

If X and Y are discrete, we have that pX|Y(x | y) = P(X = x | Y = y)
as one would naturally expect.

Extending Equation (1.9) to arbitrary random vectors yields the prod-
uct rule (also called the chain rule of probability):



8 probabilistic artificial intelligence

Fact 1.10 (Product rule). Given random variables X1:n,
n

p(x1:n) = p(x1) ·∏ p(xi | x1:i−1). (1.11)
i=2

Combining sum rule and product rule, we can compute marginal
probabilities too∫: ∫

p(x) = p(x, y) dy = p(x | y) · p(y) dy (1.12) first using the sum rule (1.7) then the
Y(Ω) Y(Ω) product rule (1.11)

This is called the law of total probability (LOTP), which is colloquially
often referred to as conditioning on Y. If it is difficult to compute p(x)
directly, conditioning can be a useful technique when Y is chosen such
that the densities p(x | y) and p(y) are straightforward to understand.

1.1.7 Independence

Two random vectors X and Y are independent (denoted X ⊥ Y) if and
only if knowledge about the state of one random vector does not affect
the distribution of the other random vector, namely if their conditional
CDF (or in case they have a joint density, their conditional PDF) sim-
plifies to

PX|Y(x | y) = PX(x), pX|Y(x | y) = pX(x). (1.13)

For the conditional probabilities to be well-defined, we need to assume
that pY(y) > 0.

The more general characterization of independence is that X and Y are
independent if and only if their joint CDF (or in case they have a joint
density, their joint PDF) can be decomposed as follows:

PX,Y(x, y) = PX(x) · PY(y), pX,Y(x, y) = pX(x) · pY(y). (1.14)

The equivalence of the two characterizations (when pY(y) > 0) is eas-
ily proven using the product rule: pX,Y(x, y) = pY(y) · pX|Y(x | y).

A “weaker” notion of independence is conditional independence.5 5 We discuss in Remark 1.11 how
Two random vectors X and Y are conditionally independent given a ran- “weaker” is to be interpreted in this con-

text.
dom vector Z (denoted X ⊥ Y | Z) iff, given Z, knowledge about the
value of one random vector Y does not affect the distribution of the
other random vector X, namely if

PX|Y,Z(x | y, z) = PX|Z(x | z), (1.15a)
pX|Y,Z(x | y, z) = pX|Z(x | z). (1.15b)



fundamentals of inference 9

Similarly to independence, we have that X and Y are conditionally
independent given Z if and only if their joint CDF or joint PDF can be
decomposed as follows:

PX,Y|Z(x, y | z) = PX|Z(x | z) · PY|Z(y | z), (1.16a)
pX,Y|Z(x, y | z) = pX|Z(x | z) · pY|Z(y | z). (1.16b)

Remark 1.11: Common causes
How can conditional independence be understood as a “weaker”
notion of independence? Clearly, conditional independence does
not imply independence: a trivial example is X ⊥ X | X ≠⇒ X ⊥
X.6 Neither does independence imply conditional independence: 6 X ⊥ X | X is true trivially.
for example, X ⊥ Y ≠⇒ X ⊥ Y | X + Y.7 7 Knowing X and X + Y already implies

the value of Y, and hence, X ̸⊥ Y | X +
When we say that conditional independence is a weaker notion we Y.
mean to emphasize that X and Y can be “made” (conditionally)
independent by conditioning on the “right” Z even if X and Y are
dependent. This is known as Reichenbach’s common cause principle
which says that for any two random variables X ̸⊥ Y there exists a
random variable Z (which may be X or Y) that causally influences
both X and Y, and which is such that X ⊥ Y | Z.

1.1.8 Directed Graphical Models

Directed graphical models (also called Bayesian networks) are often
used to visually denote the (conditional) independence relationships
of a large number of random variables. They are a schematic repre-
sentation of the factorization of the generative model into a product
of conditional distributions as a directed acyclic graph. Given the se-
quence of random variables {Xi}n

i=1, their generative model can be
expressed as

n
p(x1:n) = ∏ p(xi | parents(xi)) (1.17) 8 More generally, vertices u and v are

i=1 conditionally independent given a set of
vertices Z if Z d-separates u and v, which

where parents(xi) is the set of parents of the vertex Xi in the directed we will not cover in depth here.
graphical model. In other words, the parenthood relationship encodes
a conditional independence of a random variable X with a random

c
variable Y given their parents:8 Y

X ⊥ Y | parents(X), parents(Y). (1.18)
X1 · · · Xn

Equation (1.17) simply uses the product rule and the conditional in-
dependence relationships to factorize the generative model. This can

a1 an
greatly reduce the model’s complexity, i.e., the length of the product.

Figure 1.3: Example of a directed
graphical model. The random vari-
ables X1, . . . , Xn are mutually indepen-
dent given the random variable Y. The
squared rectangular nodes are used to
represent dependencies on parameters
c, a1, . . . , an.



10 probabilistic artificial intelligence

An example of a directed graphical model is given in Figure 1.3. Cir-
cular vertices represent random quantities (i.e., random variables). In
contrast, square vertices are commonly used to represent determinis-
tic quantities (i.e., parameters that the distributions depend on). In Y
the given example, we have that Xi is conditionally independent of all c
other Xj given Y. Plate notation is a condensed notation used to repre-
sent repeated variables of a graphical model. An example is given in Xi

Figure 1.4.
ai

i ∈ 1 : n
1.1.9 Expectation

Figure 1.4: The same directed graphical

The expected value or mean E[X] of a random vector X is the (asymp- model as in Figure 1.3 using plate nota-
tion.

totic) arithmetic mean of an arbitrarily increasing number of indepen-
dent realizations of X. That is,9∫ 9 In infinite probability spaces, absolute

convergence of E[X] is necessary for the
E[X .

] = x · p(x) dx (1.19) existence of E[X].
X(Ω)

A very special and often used property of expectations is their lin-
earity, namely that for any random vectors X and Y in Rn and any
A ∈ Rm×n, b ∈ Rm it holds that

E[AX + b] = AE[X] + b and E[X + Y] = E[X] + E[Y]. (1.20)

Note that X and Y do not necessarily have to be independent! Further,
if X and Y are independe[nt the]n

E XY⊤ = E[X] · ⊤
E[Y] . (1.21)

The following intuitive lemma can be used to compute expectations of
transformed random variables.

Fact 1.12 (Law of the unconsc∫ious statistician, LOTUS).

E[g(X)] = g(x) · p(x) dx (1.22)
X(Ω)

where g : X(Ω)→ Rn is a “nice” function10 and X is a continuous ran- 10 g being a continuous function, which
dom vector. The analogous statement with a sum replacing the integral is either bo∫unded or absolutely inte-

grable (i.e., |g(x)| p(x) dx < ∞), is suf-
holds for discrete random variables. ficient. This is satisfied in most cases.

This is a nontrivial fact that can be proven using the change of variables
formula which we discuss in Section 1.1.11.

Similarly to conditional probability, we can also define conditional ex-
pectations. The expectation of a continuous random vector X given
that Y = y is defined as ∫

E[X | Y = y .
] = x · pX|Y(x | y) dx. (1.23)

X(Ω)



fundamentals of inference 11

Observe that E[X | Y = ·] defines a deterministic mapping from y to
E[X | Y = y]. Therefore, E[X | Y] is itself a random vector:

E[X | Y](ω) = E[X | Y = Y(ω)] (1.24)

where ω ∈ Ω. This random vector E[X | Y] is called the conditional
expectation of X given Y.

Analogously to the law of total probability (1.12), one can condition an
expectation on another random vector. This is known as the tower rule
or the law of total expectation (LOTE):

Theorem 1.13 (Tower rule). Given random vectors X and Y, we have

EY[EX[X | Y]] = E[X]. (1.25)

Proof sketch. We only prove the case where X and Y have a joint den-
sity. We have ∫ (∫ )

E[E[X | Y]] = ∫ ∫ x · p(x | y) dx p(y) dy

= ∫ ∫x · p(x, y) dx dy by definition of conditional densities
(1.10)

= ∫ x p(x, y) dy dx by Fubini’s theorem

= x · p(x) dx using the sum rule (1.7)

= E[X].

1.1.10 Covariance and Variance

Given two random vectors X in Rn and Y in Rm, their covariance is
defined as [ ]

Cov[X, Y .
] = E[(X−]E[X])(Y−E[Y])⊤ (1.26)

= E XY⊤ −E[X] ·E[Y]⊤ (1.27)

= Cov[Y, X]⊤ ∈ Rn×m. (1.28)

Covariance measures the linear dependence between two random vec-
tors since a direct consequence of its definition (1.26) is that given lin-
ear maps A ∈ Rn′×n, B ∈ Rm′×m, vectors c ∈ Rn′ , d ∈ Rm′ and random
vectors X in Rn and Y in Rm, we have that

Cov[AX + c, BY + d] = ACov[X, Y]B⊤. (1.29)

Two random vectors X and Y are said to be uncorrelated if and only
if Cov[X, Y] = 0. Note that if X and Y are independent, then Equa-
tion (1.21) implies that X and Y are uncorrelated. The reverse does not
hold in general.



12 probabilistic artificial intelligence

Remark 1.14: Correlation
The correlation of the random vectors X and Y is a normalized
covariance,

Cor[X, Y](i, j) √ [ ]
. Cov Xi, Yj
= [ ] ∈ [−1, 1]. (1.30)

Var[Xi]Var Yj

Two random vectors X and Y are therefore uncorrelated if and
only if Cor[X, Y] = 0.

There is also a nice geometric interpretation of covariance and
correlation. For zero mean random variables X and Y, Cov[X, Y]
is an inner product.11 11 That is,

• Cov[X, Y] is symmetric,
The cosine of the angle θ between X and Y (that are not determin- • Cov[X, Y] is linear (here we use
istic) coincides with their correlation, EX = EY = 0), and

• Cov[X, X] ≥ 0.
Cov[X, Y]

cos θ = ∥X∥ ∥Y∥ = Cor[X, Y]. (1.31) using the Euclidean inner product
formula, Cov[X, Y] = ∥X∥ ∥Y∥ cos θ

cos θ is also called a cosine similarity. Thus,

θ = arccos Cor[X, Y]. (1.32)

For example, if X and Y are uncorrelated, then they are orthogonal
in the inner product space. If Cor[X, Y] = −1 then θ ≡ π (that is,
X and Y “point in opposite directions”), whereas if Cor[X, Y] = 1
then θ ≡ 0 (that is, X and Y “point in the same direction”).

The covariance of a random vector X in Rn with itself is called its
variance:

Var .
[X] = Co[v[X, X] ] (1.33)

= E[(X−]E[X])(X−E[X])⊤ (1.34)

= E XX⊤ −E[X] ·E[X]⊤  (1.35)

Cov[X , X ] · · Cov X

 1 1 · [ 1, Xn]
.. . 

= . . .
. ..  . (1.36)

Cov[Xn, X1] · · · Cov[Xn, Xn]

The scalar variance Var[X] of a random variable X is a measure of un-
certainty about the value of X since it measures the average squared
deviation from E[X]. We will see that the eigenvalue spectrum of a
covariance matrix can serve as a measure of uncertainty in the multi-
variate setting.12 12 The multivariate setting (as opposed to

the univariate setting) studies the joint
distribution of multiple random vari-
ables.



fundamentals of inference 13

Remark 1.15: Standard deviation
The length of a random variable X in the inner product space
described in Remark√1.14 is called it√s standard deviation,

∥X∥ = Cov[X, X] = Var[X .
] = σ[X]. (1.37)

That is, the longer a random variable is in the inner product space,
the more “uncertain” we are about its value. If a random variable
has length 0, then it is deterministic.

The variance of a random vector X is also called the covariance matrix
of X and denoted by ΣX (or Σ if the correspondence to X is clear from
context). A covariance matrix is symmetric by definition due to the
symmetry of covariance, and is always positive semi-definite ? . Problem 1.4

Two useful properties of variance are the following:
• It follows from Equation (1.29) that for any linear map A ∈ Rm×n

and vector b ∈ Rm,

Var[AX + b] = AVar[X]A⊤. (1.38)

In particular, Var[−X] = Var[X].
• It follows from the definition of variance (1.34) that for any two

random vectors X and Y,

Var[X + Y] = Var[X] + Var[Y] + 2Cov[X, Y]. (1.39)

In particular, if X and Y are independent then the covariance term
vanishes and Var[X + Y] = Var[X] + Var[Y].

Analogously to conditional probability and conditional expectation,
we can also define conditional variance. The conditional variance of a
random vector X give[n another random vector Y is th∣e random vector

Var[X | Y .
] = E (X−E[X | Y])(X−E[X | Y])⊤ ∣∣ ]

Y . (1.40)

Intuitively, the conditional variance is the remaining variance when
we use E[X | Y] to predict X rather than if we used E[X]. One can
also condition a variance on another random vector, analogously to
the laws of total probability (1.12) and expectation (1.25).

Theorem 1.16 (Law of total variance, LOTV).

Var[X] = EY[VarX[X | Y]] + VarY[EX[X | Y]]. (1.41)

Here, the first term measures the average deviation from the mean of X
across realizations of Y and the second term measures the uncertainty



14 probabilistic artificial intelligence

in the mean of X across realizations of Y. In Section 2.2, we will see
that both terms have a meaningful characterization in the context of
probabilistic inference.
Proof sketch of LOTV. To simplify the notation, we present only a proof
for the univariate [setti]ng.

Var[X] = [X[2 −E[X]]]2E

= E[E X2 | Y −E[E[X | ]Y]]2 by the tower rule (1.25)

= E Var[X | Y] + E([X[ | Y]2 −E][E[X | Y]]2 ) by the definition of variance (1.35)

= E[Var[X | Y]] + E E[X | Y]2 −E[E[X | Y]]2

= E[Var[X | Y]] + Var[E[X | Y]]. by the definition of variance (1.35)

1.1.11 Change of Variables

It is often useful to understand the distribution of a transformed ran-
dom variable Y = g(X) that is defined in terms of a random variable
X, whose distribution is known. Let us first consider the univariate
setting. We would like to express the distribution of Y in terms of the
distribution of X, that is, we would like to fin(d )

PY(y) = P(Y ≤ y) = P(g(X) ≤ y) = P X ≤ g−1(y) . (1.42)

When the random variables are continuous, this probability can be ex-
pressed as an integration over the domain of X. We can then use the
substitution rule of integration to “change the variables” to an inte-
gration over the domain of Y. Taking the derivative yields the density
pY.13 There is an analogous change of variables formula for the multi- 13 The full proof of the change of vari-
variate setting. ables formula in the univariate setting

can be found in section 6.7.2 of “Math-
ematics for machine learning” (Deisen-

Fact 1.17 (Change of variables formula). Let X be a random vector roth et al., 2020).

in Rn with density pX and let g : Rn → Rn be a differentiable and
invertible function. Then Y = g(X) is another random variable, whose
density can be computed based on pX an∣ d g

∣∣ (as follows:)∣
pY(y) = pX(g−1(y)) · det Dg−1(y ∣

) ∣ (1.43)

where Dg−1(y) is the Jacobian of g−1 evaluated at y.

∣∣ ( )∣
Here, the term det Dg−1(y) ∣ measures how much a unit volume
changes when applying g. Intuitively, the change of vari∣∣able(s swaps )∣
the coordinate system over which we integrate. The factor det Dg−1(y) ∣
corrects for the change in volume that is caused by this change in co-
ordinates.



fundamentals of inference 15

Intuitively, you can think of the vector field g as a perturbation to X,
“pushing” the probability mass around. The perturbation of a density
pX by g is commonly denoted by the pushforward

g .
♯pX = pY where Y = g(X). (1.44)

This concludes our quick tour of probability theory, and we are well-
prepared to return to the topic of probabilistic inference.

1.2 Probabilistic Inference

Recall the logical implication “If it is raining, the ground is wet.” from
the beginning of this chapter. Suppose that we look outside a window
and see that it is not raining: will the ground be dry? Logical reason-
ing does not permit drawing an inference of this kind, as there might
be reasons other than rain for which the ground could be wet (e.g.,
sprinklers). However, intuitively, by observing that it is not raining,
we have just excluded the possibility that the ground is wet because of
rain, and therefore we would deem it “more likely” that the ground is
dry than before. In other words, if we were to walk outside now and
the ground was wet, we would be more surprised than we would have
been if we had not looked outside the window before.

As humans, we are constantly making such “plausible” inferences of
our beliefs: be it about the weather, the outcomes of our daily deci-
sions, or the behavior of other(s. P)robabilistic inference is t(he pro)cess of
updating such a prior belief P W to a posterior belief P W | R upon
observing R where — to reduce clutter — we write W for “The ground
is wet” and R for “It is raining”.

The central principle of probabilistic inference is Bayes’ rule:

Theorem 1.18 (Bayes’ rule). Given random vectors X in Rn and Y in
Rm, we have for any x ∈ Rn, y ∈ Rm that

p(x | p(y | x) · p(x)
y) = . (1.45)

p(y)

Proof. Bayes’ rule is a direct consequence of the definition of condi-
tional densities (1.10) and the product rule (1.11).

Let us consider the meaning of each term separately:
• the prior p(x) is the initial belief about x,
• the (conditional) likelihood p(y | x) describes how likely the observa-

tions y are under a given value x,



16 probabilistic artificial intelligence

• the posterior p(x | y) is the updated belief about x after observing y,
• the joint likelihood p(x, y) = p(y | x)p(x) combines prior and likeli-

hood,
• the marginal likelihood p(y) describes how likely the observations y

are across all values of x.

The marginal likelihood can be computed using the sum rule (1.7) or
the law of total probability (1.12),

∫
p(y) = p(y | x) · p(x) dx. (1.46)

X(Ω)

Note, however, that the marginal likelihood is simply normalizing the
conditional distribution to integrate to one, and therefore a constant
with respect to x. For this reason, p(y) is commonly called the normal-
izing constant.

Example 1.19: Plausible inferences
Let us confirm our intuition from the above example. The logical
implication “If it is raining, the ground is wet.” (denoted R→W)
can be succinctly expressed as P(W | R) = 1. Since P(W) ≤ 1, we
know that

P(W | R) ·P(R) P(R)
P(R |W) = = ≥ P(R).

P(W) P(W)

That is, observing that the ground is wet make(s it mo)re lik(ely) to
be raining. From P(R |W) ≥ P(R) we know P R |W ≤ ( )

P R ,14 14 since P X = 1−P(X)

which leads us to follow that

( ) ( )
W | P R |W( ·)P(W)

P R = ≤ P(W),
P R

that is, having observed it not to be raining made the ground less
likely to be wet.

Example 1.19 is called a plausible inference because the observation of R
does not completely determine the truth value of W, and hence, does
not permit logical inference. In the case, however, that logical inference
is permitted, it coincides with probabilistic inference.

Example 1.20: Logical inferences
For example, if we were to observe that the ground is not wet,
then logical inference implies that it must not be raining: W → R.
This is called the contrapositive of R→W.



fundamentals of inference 17

Indeed, by probabilistic inference, we obtain analogously

( ) ( )
R | P W | (R ·)P(R) (1−P(W(| R))) ·P(R)

P W = = = 0. as P(W | R) = 1
P W P W

Observe that a logical inference does not depend on the prior P(R):
Even if the prior was P(R) = 1 in Example 1.20, after observing that
the ground is not wet, we are forced to conclude that it is not raining to
maintain logical consistency. The examples highlight that while logical
inference does not require the notion of a prior, plausible (probabilistic!)
inference does.

1.2.1 Where do priors come from?

Bayes’ rule necessitates the specification of a prior p(x). Different pri-
ors can lead to the deduction of dramatically different posteriors, as
one can easily see by considering the extreme cases of a prior that is a
point density at x = x0 and a prior that is “uniform” over Rn.15 In the 15 The latter is not a valid probability dis-
former case, the posterior will be a point density at x0 regardless of tribution, but we can still derive mean-

ing from the posterior as we discuss in
the likelihood. In other words, no evidence can alter the “prior belief” Remark 1.22.
the learner ascribed to x. In the latter case, the learner has “no prior
belief”, and therefore the posterior will be proportional to the likeli-
hood. Both steps of probabilistic inference are perfectly valid, though
one might debate which prior is more reasonable.

Someone who follows the Bayesian interpretation of probability might
argue that everything is conditional, meaning that the prior is simply
a posterior of all former observations. While this might seem natural
(“my world view from today is the combination of my world view
from yesterday and the observations I made today”), this lacks an
explanation for “the first day”. Someone else who is more inclined
towards the frequentist interpretation might also object to the exis-
tence of a prior belief altogether, arguing that a prior is subjective and
therefore not a valid or desirable input to a learning algorithm. Put
differently, a frequentist “has the belief not to have any belief”. This is
perfectly compatible with probabilistic inference, as long as the prior
is chosen to be noninformative:

p(x) ∝ const. (1.47)

Choosing a noninformative prior in the absence of any evidence is
known as the principle of indifference or the principle of insufficient reason,
which dates back to the famous mathematician Pierre-Simon Laplace.



18 probabilistic artificial intelligence

Example 1.21: Why be indifferent?
Consider a criminal trial with three suspects, A, B, and C. The
collected evidence shows that suspect C can not have committed
the crime, however it does not yield any information about sus-
pects A and B. Clearly, any distribution respecting the data must
assign zero probability of having committed the crime to suspect
C. However, any distribution interpolating between (1, 0, 0) and
(0, 1, 0) respects the data. The principle of indifference suggests
that the desired distribution is ( 1

2 , 1
2 , 0), and indeed, any alterna-

tive distribution seems unreasonable.

Remark 1.22: Noninformative and improper priors
It is not necessarily required that the prior p(x) is a valid distri-
bution (i.e., integrates to 1). Consider for example, the noninfor-
mative prior p(x) ∝ 1{x ∈ I} where I ⊆ Rn is an infinitely large
interval. Such a prior which is not a valid distribution is called an
improper prior. We can still derive meaning from the posterior of a
given likelihood and (improper) prior as long as the posterior is a
valid distribution.

Laplace’s principle of indifference can be generalized to cases where
some evidence is available. The maximum entropy principle, originally
proposed by Jaynes (1968), states that one should choose as prior from
all possible distributions that are consistent with prior knowledge, the
one that makes the least “additional assumptions”, i.e., is the least
“informative”. In philosophy, this principle is known as Occam’s razor
or the principle of parsimony. The “informativeness” of a distribution p
is quantified by its entropy which is defined as

H[p .
] = Ex∼p[− log p(x)]. (1.48)

The more concentrated p is, the less is its entropy; the more diffuse p
is, the greater is its entropy.16 16 We give a thorough introduction to en-

tropy in Section 5.4.
In the absence of any prior knowledge, the uniform distribution has
the highest entropy,17 and hence, the maximum entropy principle sug- 17 This only holds true when the set
gests a noninformative prior (as does Laplace’s principle of indiffer- of possible outcomes of x finite (or

a bounded continuous interval), as in
ence). In contrast, if the evidence perfectly determines the value of x, this case, the noninformative prior is a
then the only consistent explanation is the point density at x. The proper distribution — the uniform dis-

maximum entropy principle characterizes a reasonable choice of prior tribution. In the “infinite case”, there is
no uniform distribution and the nonin-

for these two extreme cases and all cases in between. Bayes’ rule can formative prior can be attained from the
in fact be derived as a consequence of the maximum entropy principle maximum entropy principle as the lim-

iting solution as the number of possible
in the sense that the posterior is the least “informative” distribution outcomes of x is increased.
among all distributions that are consistent with the prior and the ob-



fundamentals of inference 19

servations ? . Problem 5.7

1.2.2 Conjugate Priors

If the prior p(x) and posterior p(x | y) are of the same family of distri-
butions, the prior is called a conjugate prior to the likelihood p(y | x).
This is a very desirable property, as it allows us to recursively ap-
ply the same learning algorithm implementing probabilistic inference.
We will see in Chapter 2 that under some conditions the Gaussian is
self-conjugate. That is, if we have a Gaussian prior and a Gaussian like-
lihood then our posterior will also be Gaussian. This will provide us
with the first efficient implementation of probabilistic inference.

Example 1.23: Conjugacy of beta and binomial distribution
As an example for conjugacy, we will show that the beta distribu-
tion is a conjugate prior to a binomial likelihood. Recall the PMF
of the binomial distribution ( )

n
Bin(k; n, θ) = θk(1− θ)n−k (1.49)

k

and the PDF of the beta distribution,

Beta(θ; α, β) ∝ θα−1(1− θ)β−1, (1.50)

We assume the prior θ ∼ Beta(α, β) and likelihood k | θ ∼ Bin(n, θ).
Let nH = k be the number of heads and nT = n− k the number of
tails in the binomial trial k. Then,

p(θ | k) ∝ p(k | θ)p(θ) using Bayes’ rule (1.45)

∝ θnH (1− θ)nT θα−1(1− θ)β−1

= θα+nH−1(1− θ)β+nT−1.

Thus, θ | k ∼ Beta(α + nH , β + nT).

This same conjugacy can be shown for the multivariate general-
ization of the beta distribution, the Dirichlet distribution, and the
multivariate generalization of the binomial distribution, the multi-
nomial distribution.

1.2.3 Tractable Inference with the Normal Distribution

Using arbitrary distributions for learning and inference is computa-
tionally very expensive when the number of dimensions is large —
even in the discrete setting. For example, computing marginal distri-
butions using the sum rule yields an exponentially long sum in the



20 probabilistic artificial intelligence

size of the random vector. Similarly, the normalizing constant of the X1 · · · Xn−1 Xn P(X1:n)

conditional distribution is a sum of exponential length. Even to rep- 0 · · · 0 0 0.01

resent any discrete joint probability distribution requires space that is 0 · · · 0 1 0.001
0 · · · 1 0 0.213

exponential in the number of dimensions (cf. Figure 1.5). .. .. . .
. . .. ..

One strategy to get around this computational blowup is to restrict 1 · · · 1 1 0.0003

the class of distributions. Gaussians are a popular choice for this pur- Figure 1.5: A table representing a joint
pose since they have extremely useful properties: they have a compact distribution of n binary random vari-
representation and — as we will see in Chapter 2 — they allow for ables. The table has 2n rows. The num-

ber of parameters is 2n − 1 since the fi-
closed-form probabilistic inference. nal probability is determined by all other

probabilities as they must sum to one.
In Equation (1.5), we have already seen the PDF of the univariate Gaus-
sian distribution. A random vector X in Rn is normally distributed,
X ∼ N (µ, Σ), if its PDF is

)
. √ (

N 1
(x; µ, Σ) = exp −1

(x− µ)⊤Σ−1(x− µ) (1.51)
det(2πΣ) 2

where µ ∈ Rn is the mean vector and Σ ∈ Rn×n the covariance matrix
? . We call .

Λ = Σ−1 the precision matrix. X is also called a Gaussian Problem 1.11
random vector (GRV). N (0, I) is the multivariate standard normal distri-
bution. We call a Gaussian isotropic if its covariance matrix is of the
form Σ = σ2 I for some σ2 ∈ R. In this case, the sublevel sets of the
PDF are perfect spheres as can be seen in Figure 1.6.

Figure 1.6: Shown are the PDFs of two-
dimensional Gaussians with mean 0 and
covarianc[e matr]ices [ ]

. 1 0 . 1 0.9
0.2 0.4 Σ1 = , Σ =

0 1 2 0.9 1
0.1 0.2

respectively.
0.0 0.0

2 2

−2 0 y −2 0 y
0

x −2 0 −2
2 x 2

(
n2)Note that a Gaussian can be represented using only O parameters.

In the case of a diagonal covariance matrix, which corresponds to n
independent univariate Gaussians ? , we just need O(n) parameters. Problem 1.8

In Equation (1.51), we assume that the covariance matrix Σ is invert-
ible, i.e., does not have the eigenvalue 0. This is not a restriction since
it can be shown that a covariance matrix has a zero eigenvalue if and
only if there exists a deterministic linear relationship between some
variables in the joint distribution ? . As we have already seen that a Problem 1.6



fundamentals of inference 21

covariance matrix does not have negative eigenvalues ? , this ensures Problem 1.4
that Σ and Λ are positive definite.18 18 The inverse of a positive definite ma-

trix is also positive definite.
An important property of the normal distribution is that it is closed
under marginalization and conditioning.

Theorem 1.24 (Marginal and conditional distribution). ? Con- Problem 1.9
sider the Gaussian random vector X and fix index sets A ⊆ [n] and
B ⊆ [n]. Then, we have that for any such marginal distribution,

XA ∼ N (µA, Σ AA), (1.52) By µA we denote [µi1 , . . . , µik ] where
A = {i1, . . . ik}. Σ AA is defined

and that for any such conditional distribution, analogously.

XA | XB = xB ∼ N (µA|B, Σ A|B) where (1.53a)
.

µA|B = µA + Σ ABΣ−1
BB(xB − µB), (1.53b) Here, µA characterizes the prior belief

. and Σ ABΣ−1
Σ −1 BB(xB − µB) represents “how

A|B = ΣAA − ΣABΣBBΣBA. (1.53c) different” xB is from what was
expected.

Theorem 1.24 provides a closed-form characterization of probabilistic
inference for the case that random variables are jointly Gaussian. We
will discuss in Chapter 2, how this can be turned into an efficient
inference algorithm.

Observe that upon inference, the variance can only shrink! Moreover,
how much the variance is reduced depends purely on where the obser-
vations are made (i.e., the choice of B) but not on what the observations
are. In contrast, the posterior mean µA|B depends affinely on µB. These
are special properties of the Gaussian and do not generally hold true
for other distributions.

It can be shown that Gaussians are additive and closed under affine
transformations ? . The closedness under affine transformations (1.78) Problem 1.10
implies that a Gaussian X ∼ N (µ, Σ) is equivalently characterized as

X 1
= Σ /2Y + µ. (1.54)

where Y ∼ N (0, I) and Σ1/2 is the square root of Σ.19 Importantly, this 19 More details on the square root of a
implies together with Theorem 1.24 and additivity (1.79) that: symmetric and positive definite matrix

can be found in Appendix A.2.

Any affine transformation of a Gaussian random vector
is a Gaussian random vector.

A consequence of this is that given any jointly Gaussian random vec-
tors XA and XB, XA can be expressed as an affine function of XB with
added independent Gaussian noise. Formally, we define

X .
A = AXB + b + ε where (1.55a)



22 probabilistic artificial intelligence

A .
= ΣABΣ−1

BB, (1.55b)
b .
= µA − Σ ABΣ−1

BBµB, (1.55c)
ε ∼ N (0, ΣA|B). (1.55d)

It directly follows from the closedness of Gaussians under affine trans-
formations (1.78) that the characterization of XA via Equation (1.55) is
equivalent to XA ∼ N (µA, ΣAA), and hence, any Gaussian XA can be
modeled as a so-called conditional linear Gaussian, i.e., an affine function
of another Gaussian XB with additional independent Gaussian noise.
We will use this fact frequently to represent Gaussians in a compact
form.

1.3 Supervised Learning and Point Estimates

Throughout the first part of this manuscript, we will focus mostly on
the supervised learning problem where we want to learn a function

f ⋆ : X → Y
from labeled training data. That is, we are given a collection of labeled
examples, D .

= {(xi, yi)}n
n i=1, where the xi ∈ X are inputs and the f ⋆

f
yi ∈ Y are outputs (called labels), and we want to find a function f̂ that f̂
best-approximates f ⋆. It is common to choose f̂ from a parameter-
ized function class F (Θ), where each function fθ is described by some F
parameters θ ∈ Θ.

Figure 1.7: Illustration of estimation er-
ror and approximation error. f ⋆ denotes

Remark 1.25: What this manuscript is about and not about the true function and f̂ is the best ap-
proximation from the function class F .

As illustrated in Figure 1.7, the restriction to a function class leads We do not specify here, how one could

to two sources of error: the estimation error of having “incorrectly” quantify “error”. For more details, see
Appendix A.3.5.

determined f̂ within the function class, and the approximation er-
ror of the function class itself. Choosing a “good” function class
/ architecture with small approximation error is therefore critical
for any practical application of machine learning. We will discuss
various function classes, from linear models to deep neural net-
works, however, determining the “right” function class will not
be the focus of this manuscript. To keep the exposition simple,
we will assume in the following that f ⋆ ∈ F (Θ) with parameters
θ⋆ ∈ Θ.

Instead, we will focus on the problem of estimation/inference
within a given function class. We will see that inference in smaller
function classes is often more computationally efficient since the
search space is smaller or — in the case of Gaussians — has a
known tractable structure. On the other hand, larger function



fundamentals of inference 23

classes are more expressive and therefore can typically better ap-
proximate the ground truth f ⋆.

We differentiate between the task of regression where Y .
= Rk,20 and 20 The labels are usually scalar, so k = 1.

the task of classification where Y .
= C and C is an m-element set of

classes. In other words, regression is the task of predicting a continu-
ous label, whereas classification is the task of predicting a discrete class
label. These two tasks are intimately related: in fact, we can think of
classification tasks as a regression problem where we learn a probabil-
ity distribution over class labels. In this regression problem, Y .

= ∆C
where ∆C denotes the set of all probability distributions over the set
of classes C which is an (m− 1)-dimensional convex polytope in the
m-dimensional space of probabilities [0, 1]m (cf. Appendix A.1.2).

For now, let us stick to the regression setting. We will assume that
the observations are noisy, that is, y i

i ∼id p(· | x , θ⋆i ) for some known
conditional distribution p(· | xi, θ) but unknown parameter θ⋆.21 Our 21 The case where the labels are deter-
assumption can equivalently be formulated as ministic is the special case of p(· | xi , θ⋆)

being a point density at f ⋆(xi).

yi = ︸fθ︷(︷xi︸) + ε︸i(︷x︷i︸) (1.56)
signal noise

where fθ(xi) is the mean of p(· | xi, θ) and εi(xi) = yi − fθ(xi) is some
independent zero-mean noise, for example (but not necessarily) Gaus-
sian.22 When the noise distribution may depend on xi, the noise is said 22 It is crucial that the assumed noise dis-
to be heteroscedastic and otherwise the noise is called homoscedastic. tribution accurately reflects the noise of

the data. For example, using a (light-
tailed) Gaussian noise model in the pres-

1.3.1 Maximum Likelihood Estimation ence of heavy-tailed noise will fail! We
discuss the distinction between light and
heavy tails in Appendix A.3.2.

A common approach to finding f̂ is to select the model f ∈ F (Θ) un-
der which the training data is most likely. This is called the maximum
likelihood estimate (or MLE):

.
θ̂MLE = arg max p(y1:n | x1:n, θ) (1.57)

θ∈Θ
n

= arg max ∏ p(yi | xi, θ). using the independence of the training
θ∈Θ i=1 data (1.56)

Such products of probabilities are often numerically unstable, which
is why one typically takes the logarithm:

n
= arg max og p | xi, θ) . (1.58)

θ∈Θ i︸∑ l
=1 ︷(︷yi ︸

log-likelihood

We will denote the negative log-likelihood by ℓnll(θ;Dn).



24 probabilistic artificial intelligence

The MLE is often used in practice due to its desirable asymptotic prop-
erties as the sample size n increases. We give a brief summary here
and provide additional background and definitions in Appendix A.3.
To give any guarantees on the convergence of the MLE, we neces-
sarily need to assume that θ⋆ is identifiable.23 If additionally, ℓnll is 23 That is, θ⋆ ̸= θ =⇒ f ⋆ ̸= fθ for any
“well-behaved” then standard results say that the MLE is consistent θ ∈ Θ. In words, there is no other pa-

rameter θ that yields the same function
and asymptotically normal (Van der Vaart, 2000): fθ as θ⋆.

θ̂MLE→P θ⋆ and D
θ̂MLE→N (θ⋆, Sn) as n→ ∞. (1.59)

Here, we denote by Sn the asymptotic variance of the MLE which
can be understood as measuring the “quality” of the estimate.24 This 24 A “smaller” variance means that we
implies in some sense that the MLE is asymptotically unbiased. More- can be more confident that the MLE is

close to the true parameter.
over, the MLE can be shown to be asymptotically efficient which is to
say that there exists no other consistent estimator with a “smaller”
asymptotic variance.25 25 see Appendix A.3.4.

The situation is quite different in the finite sample regime. Here, the
MLE need not be unbiased, and it is susceptible to overfitting to the
(finite) training data as we discuss in more detail in Appendix A.3.5.

1.3.2 Using Priors: Maximum a Posteriori Estimation

We can incorporate prior assumptions about the parameters θ⋆ into the
estimation procedure. One approach of this kind is to find the mode
of the posterior distribution, called the maximum a posteriori estimate (or
MAP estimate):

.
θ̂MAP = arg max p(θ | x1:n, y1:n) (1.60)

θ∈Θ

= arg max p(y1:n | x1:n, θ) · p(θ) (1.61) by Bayes’ rule (1.45)
θ∈Θ

n
= arg max log p(θ) + ∑ log p(yi | xi, θ) (1.62) taking the logarithm

θ∈Θ i=1

= arg min − log p(
θ∈Θ ︸ ︷︷ θ︸) + ℓ︸nll(︷θ︷;Dn︸) . (1.63)

regularization quality of fit

Here, the log-prior log p(θ) acts as a regularizer. Common regularizers
are given, for example, by
• p(θ) = N (θ; 0, (2λ)−1 I) which yields − log p(θ) = λ ∥θ∥2

2 + const,
• p(θ) = Laplace(θ; 0, λ−1) which yields − log p(θ) = λ ∥θ∥1 + const,
• a uniform prior (cf. Section 1.2.1) for which the MAP is equivalent

to the MLE. In other words, the MLE is merely the mode of the
posterior distribution under a uniform prior.

The Gaussian and Laplace regularizers act as simplicity biases, pre-
ferring simpler models over more complex ones, which empirically



fundamentals of inference 25

tends to reduce the risk of overfitting. However, one may also encode
more nuanced information about the (assumed) structure of θ⋆ into
the prior.

An alternative way of encoding a prior is by restricting the function
class to some Θ̃ ⊂ Θ, for example to rotation- and translation-invariant
models as often done when the inputs are images. This effectively
sets p(θ) = 0 for all θ ∈ Θ \ Θ̃ but is better suited for numerical
optimization than to impose this constraint directly on the prior.

Encoding prior assumptions into the function class or into the parame-
ter estimation can accelerate learning and improve generalization per-
formance dramatically, yet importantly, incorporating a prior can also
inhibit learning in case the prior is “wrong”. For example, when the
learning task is to differentiate images of cats from images of dogs,
consider the (stupid) prior that only permits models that exclusively
use the upper-left pixel for prediction. No such model will be able to
solve the task, and therefore starting from this prior makes the learn-
ing problem effectively unsolvable which illustrates that priors have to
be chosen with care.

1.3.3 When does the prior matter?

We have seen that the MLE has desirable asymptotic properties, and
that MAP estimation can be seen as a regularized MLE where the type
of regularization is encoded by the prior. Is it possible to derive similar
asymptotic results for the MAP estimate?

To answer this question, we will look at the asymptotic effect of the
prior on the posterior more generally. Doob’s consistency theorem states
that assuming parameters are identifiable,26 there exists Θ̃ ⊆ Θ with 26 This is akin to the assumption required
p(Θ̃) = 1 such that the posterior is consistent for any θ⋆ ∈ Θ̃ (Doob, for consistency of the MLE, cf. Equa-

tion (1.59).
1949; Miller, 2016, 2018):

θ | Dn→P θ⋆ as n→ ∞. (1.64)

In words, Doob’s consistency theorem tells us that for any prior dis-
tribution, the posterior is guaranteed to converge to a point density
in the (small) neighborhood θ⋆ ∈ B of the true parameter as long as
p(B) > 0.27 We call such a prior a well-specified prior. 27 B can for example be a ball of radius ϵ

around θ⋆ (with respect to some geome-
try of Θ).

Remark 1.26: Cromwell’s rule
In the case where |Θ| is finite, Doob’s consistency theorem strongly
suggests that the prior should not assign 0 probability (or proba-
bility 1 for that matter) to any individual parameter θ ∈ Θ, unless
we know with certainty that θ⋆ ̸= θ. This is called Cromwell’s rule,



26 probabilistic artificial intelligence

and a prior obeying by this rule is always well-specified.

Under the same assumption that the prior is well-specified (and reg-
ularity conditions28), the Bernstein-von Mises theorem, which was first 28 These regularity conditions are akin
discovered by Pierre-Simon Laplace in the early 19th century, estab- to the assumptions required for asymp-

totic normality of the MLE, cf. Equa-
lishes the asymptotic normality of the posterior distribution (Van der tion (1.59).
Vaart, 2000; Miller, 2016):

θ | Dn→D N (θ⋆, Sn) as n→ ∞ (1.65)

and where Sn is the same as the asymptotic variance of the MLE.29 29 This has also been called the “Bayesian
central limit theorem”, which is a bit of a

These results link probabilistic inference to maximum likelihood es- misnomer since the theorem also applies

timation in the asymptotic limit of infinite data. Intuitively, in the to likelihoods (when the prior is nonin-
formative) which are often used in fre-

limit of infinite data, the prior is “overwhelmed” by the observations quentist statistics.
and the posterior becomes equivalent to the limiting distribution of
the MLE.30 One can interpret the regime of infinite data as the regime 30 More examples and discussion can be
where computational resources and time are unlimited and plausible found in section 17.8 of Le Cam (1986),

chapter 8 of Le Cam and Yang (2000),
inferences evolve into logical inferences. This transition signifies a shift chapter 10 of Van der Vaart (2000), and
from the realm of uncertainty to that of certainty. The importance of in Tanner (1991).

the prior surfaces precisely in the non-asymptotic regime where plau-
sible inferences are necessary due to limited computational resources and
limited time.

1.3.4 Estimation vs Inference

You can interpret a single parameter vector θ ∈ Θ as “one possible
explanation” of the data. Maximum likelihood and maximum a pos-
teriori estimation are examples of estimation algorithms which return
a one such parameter vector — called a point estimate. That is, given
the training set Dn, they return a single parameter vector θ̂n. We give
a more detailed account of estimation in Appendix A.3.

Example 1.27: Point estimates and invalid logical inferences
To see why point estimates can be problematic, recall Example 1.19.
We have seen that the logical implication

“If it is raining, the ground is wet.”

can be expressed as P(W | R) = 1. Observing that “The ground
is wet.” does not permit logical inference, yet, the maximum like-
lihood estimate of R is R̂MLE = 1. This is logically inconsistent
since there might be other explanations for the ground to be wet,
such as a sprinkler! With only a finite sample (say independently
observing n times that the ground is wet), we cannot rule out with



fundamentals of inference 27

certainty that the ground is wet for other reasons than rain.

In practice, we never observe an infinite amount of data. Example 1.27

demonstrates that on a finite sample, point estimates may perform
invalid logical inferences, and can therefore lure us into a false sense
of certainty.

Remark 1.28: MLE and MAP are approximations of inference
The MLE and MAP estimate can be seen as a naive approximation
of probabilistic inference, represented by a point density which
“collapses” all probability mass at the mode of the posterior dis- p(θ | D)
tribution. This can be a relatively decent — even if overly simple 0.4

— approximation when the distribution is unimodal, symmetric, 0.3

and light-tailed as in Figure 1.8, but is usually a very poor approx- 0.2
imation for practical posteriors that are complex and multimodal.

0.1

In this manuscript, we will focus mainly on algorithms for probabilistic 0.0
−2 0 2

inference which compute or approximate the distribution p(θ | x1:n, y1:n) θ
over parameters. Returning a distribution over parameters is natural
since this acknowledges that given a finite sample with noisy observa- Figure 1.8: A the MLE/MAP are point

estimates at the mode θ̂ of the posterior
tions, more than one parameter vector can explain the data. distribution p(θ | D).

1.3.5 Probabilistic Inference and Prediction

The prior distribution p(θ) can be interpreted as the degree of our
belief that the model parameterized by θ “describes the (previously
seen) data best”. The likelihood captures how likely the training data
is under a particular model:

n
p(y1:n | x1:n, θ) = ∏ p(yi | xi, θ). (1.66)

i=1

The posterior then represents our belief about the best model after
seeing the training data. Using Bayes’ rule (1.45), we can write it as31 31 We generally assume that

n p(θ | x1:n) = p(θ).
p(θ | 1

x1:n, y1:n) = p(θ) y
Z ∏ p( i | xi, θ) where (1.67a)

∫ For our purposes, you can think of the
i=1 inputs x1:n as fixed deterministic param-

n eters, but one can also consider inputs
Z .
= p(θ)∏ p(yi | xi, θ) dθ (1.67b) drawn from a distribution over X .

Θ i=1

is the normalizing constant. We refer to this process of learning a model
from data as learning. We can then use our learned model for prediction
at a new input x⋆ by condi∫tioning on θ,

p(y⋆ | x⋆, x1:n, y1:n) = p(y⋆, θ | x⋆, x1:n, y1:n) dθ by the sum rule (1.7)
Θ



28 probabilistic artificial intelligence

∫
= p(y⋆ | x⋆, θ) · p(θ | x1:n, y1:n) dθ. (1.68) by the product rule (1.11) and

Θ y⋆ ⊥ x1:n, y1:n | θ

Here, the distribution over models p(θ | x1:n, y1:n) is called the posterior
and the distribution over predictions p(y⋆ | x⋆, x1:n, y1:n) is called the
predictive posterior. The predictive posterior quantifies our posterior
uncertainty about the “prediction” y⋆, however, since this is typically

⋆
a complex distribution, it is difficult to communicate this uncertainty p(y | x⋆,D)

0.20
to a human. One statistic that can be used for this purpose is the
smallest set Cδ(x⋆) ⊆ R for a fixed δ ∈ (0, 1) such that 0.15

0.10 C(x⋆)
P(y⋆ ∈ Cδ(x⋆) | x⋆, x1:n, y1:n) ≥ 1− δ. (1.69)

0.05

That is, we believe with “confidence” at least 1− δ that the true value 0.00

of y⋆ lies in Cδ(x⋆). Such a set Cδ(x⋆) is called a credible set. 0 5
y⋆

We have seen here that the tasks of learning and prediction are inti-
mately related. Indeed, “prediction” can be seen in many ways as a Figure 1.9: Example of a 95% credible

set at x⋆ where the predictive posterior is
natural by-product of “reasoning” (i.e., probabilistic inference), where Gaussian with mean µ(x⋆) and standard
we evaluate the likelihood of outcomes given our learned explana- deviation σ(x⋆). In this case, the gray

area integrates to ≈ 0.95 for
tions for the world. This intuition can be read off directly from Equa-

⋆
tion ( . ere p x⋆, to the likelihood of an out- C0.05(x ) = [µ(x⋆)± 1.96σ(x⋆)].

1 68) wh (y⋆ | θ) corresponds
come given the explanation θ and p(θ | x1:n, y1:n) corresponds to our
inferred belief about the world. We will see many more examples
of this link between probabilistic inference and prediction throughout
this manuscript.

The high-dimensional integrals of Equations (1.67b) and (1.68) are typ-
ically intractable, and represent the main computational challenge in
probabilistic inference. Throughout the first part of this manuscript,
we will describe settings where exact inference is tractable, as well
as modern approximate inference algorithms that can be used when
exact inference is intractable.

1.3.6 Recursive Probabilistic Inference and Memory

We have already alluded to the fact that probabilistic inference has a
recursive structure, which lends itself to continual learning and which
often leads to efficient algorithms. Let us denote by

p(t) .
(θ) = p(θ | x1:t, y1:t) (1.70)

the posterior after the first t observations with p(0)(θ) = p(θ). Now,
suppose that we have already computed p(t)(θ) and observe yt+1. We
can recursively update the posterior as follows,

p(t+1)(θ) = p(θ | y1:t+1)

∝ p(θ | y1:t) · p(yt+1 | θ, y1:t) using Bayes’ rule (1.45)



fundamentals of inference 29

= p(t)(θ) · p(yt+1 | θ). (1.71) using yt+1 ⊥ y1:t | θ, see Figure 2.3

Intuitively, the posterior distribution at time t “absorbs” or “summa-
rizes” all seen data.

By unrolling the recursion of Equation (1.71), we see that regardless
of the philosophical interpretation of probability, probabilistic infer-
ence is a fundamental mechanism of learning. Even the MLE which
performs naive approximate inference without a prior (i.e., a uniform
prior), is based on p(n)(θ) ∝ p(y1:n | x1:n, θ) which is the result of n
individual plausible inferences, where the (t + 1)-st inference uses the
posterior of the t-th inference as its prior.

So far we have been considering the supervised learning setting, where
all data is available a-priori. However, by sequentially obtaining the
new posterior and replacing our prior, we can also perform proba-
bilistic inference as data arrives online (i.e., in “real-time”). This is
analogous to recursive logical inference where derived consequences
are repeatedly added to the set of propositions to derive new conse-
quences. This also highlights the intimate connection between “rea-
soning” and “memory”. Indeed, the posterior distribution p(t)(θ) can
be seen as a form of memory that evolves with time t.

1.4 Outlook: Decision Theory

How can we use our predictions to make concrete decisions under
uncertainty? We will study this question extensively in Part II of this
manuscript, but briefly introduce some fundamental concepts here.
Making decisions using a probabilistic model p(y | x) of output y ∈ Y
given input x ∈ X , such as the ones we have discussed in the previous
section, is commonly formalized by
• a set of possible actions A, and
• a reward function r(y, a) ∈ R that computes the reward or utility of

taking action a ∈ A, assuming the true output is y ∈ Y .
Standard decision theory recommends picking the action with the
largest expected utility:

a⋆(x .
) = arg max Ey|x[r(y, a)]. (1.72)

a∈A

Here, a⋆ is called the optimal decision rule because, under the given
probabilistic model, no other rule can yield a higher expected utility.

Let us consider some examples of reward functions and their corre-
sponding optimal decisions:



30 probabilistic artificial intelligence

Example 1.29: Reward functions
Under the decision rule from Equation (1.72), different reward
functions r can lead to different decisions. Let us examine two
reward functions for the case where Y = A = R ? . Problem 1.13

• Alternatively to considering r as a reward function, we can in-
terpret −r as the loss of taking action a when the true output
is y. If our goal is for our actions a to “mimic” the output y, a
natural choice is the squared loss, −r(y, a) = (y− a)2. It turns
out that under the squared loss, the optimal decision is simply
the mean: a⋆(x) = E[y | x].

• To contrast this, we consider the asymmetric loss,

−r(y, a) = c1 m︸ ax{y︷︷− a, 0}︸ + c2 m︸ ax{a︷︷− y, 0}︸ ,
underestimation error overestimation error

which penalizes underestimation and overestimation differently.
When y | x ∼ N (µx, σ2

x) then the opti(mal deci)sion is

a⋆(x) = µx + σx ·Φ−1 c

︸ ︷︷ 1
c1 + c2 ︸

pessimism / optimism

where Φ is the CDF of the standard normal distribution.32 Note 32 Recall that the CDF Φ of the standard
that if c1 = c2, then the second term vanishes and the optimal normal distribution is a sigmoid with its

inverse satisfyingdecision is the same as under the squared loss. If c1 > c2, the
second term is positive (i.e., optimistic) to avoid underestima- < 0 if u < 0.5,

Φ−1(u)= 0 if u = 0.5,
tion, and if c1 < c2, the second term is negative (i.e., pessimistic)

> 0 if u > 0.5.
to avoid overestimation. We will find these notions of optimism
and pessimism to be useful in many decision-making scenarios.

While Equation (1.72) describes how to make optimal decisions given
a (posterior) probabilistic model, it does not tell us how to learn or
improve this model in the first place. That is, these decisions are only
optimal under the assumption that we cannot use their outcomes and
our resulting observations to update our model and inform future de-
cisions. When we start to consider the effect of our decisions on future
data and future posteriors, answering “how do I make optimal deci-
sions?” becomes more complex, and we will study this in Part II on
sequential decision-making.

Discussion

In this chapter, we have learned about the fundamental concepts of
probabilistic inference. We have seen that probabilistic inference is the



fundamentals of inference 31

natural extension of logical reasoning to domains with uncertainty. We
have also derived the central principle of probabilistic inference, Bayes’
rule, which is simple to state but often computationally challenging. In
the next part of this manuscript, we will explore settings where exact
inference is tractable, as well as modern approaches to approximate
probabilistic inference.

Overview of Mathematical Background

We have included brief summaries of the fundamentals of parame-
ter estimation (mean estimation in particular) and optimization in
Appendices A.3 and A.4, respectively, which we will refer back to
throughout the manuscript. Appendix A.2 discusses the correspon-
dence of Gaussians and quadratic forms. Appendix A.5 comprises a
list of useful matrix identities and inequalities.

Problems

1.1. Properties of probability.

Let (Ω,A, P) be a probability space. Derive the following properties
of probability from the Kolmogorov axioms:
1. For any A, B ∈ A,(if )A ⊆ B then P(A) ≤ P(B).
2. For any A ∈ A, P A = 1−P(A).
3. For any countable set o(f events

⋃ ){Ai ∈ A}i,

∞ ∞
P Ai ≤ ∑ P(Ai). (1.73)

i=1 i=1

which is called a union bound.

1.2. Random walks on graphs.

Let G be a simple connected finite graph. We start at a vertex u of G.
At every step, we move to one of the neighbors of the current vertex
uniformly at random, e.g., if the vertex has 3 neighbors, we move to
one of them, each with probability 1/3. What is the probability that
the walk visits a given vertex v eventually?

1.3. Law of total expectation.

Show that if {Ai}k
i=1 are a partition of Ω and X is a random vector,

k
E[X] = ∑ E[X | Ai] ·P(Ai). (1.74)

i=1



32 probabilistic artificial intelligence

1.4. Covariance matrices are positive semi-definite.

Prove that a covariance matrix Σ is always positive semi-definite. That
is, all of its eigenvalues are greater or equal to zero, or equivalently,
x⊤Σx ≥ 0 for any x ∈ Rn.

1.5. Probabilistic inference.

As a result of a medical screening, one of the tests revealed a serious
disease in a person. The test has a high accuracy of 99% (the prob-
ability of a positive response in the presence of a disease is 99% and
the probability of a negative response in the absence of a disease is
also 99%). However, the disease is quite rare and occurs only in one
person per 10 000. Calculate the probability of the examined person
having the identified disease.

1.6. Zero eigenvalues of covariance matrices.

We say that a random vector X in Rn is not linearly independent if for
some α ∈ Rn \ {0}, α⊤X = 0.
1. Show that if X is not linearly independent, then Var[X] has a zero

eigenvalue.
2. Show that if Var[X] has a zero eigenvalue, then X is not linearly

independent.
Hint: Consider the variance of λ⊤X where λ is the eigenvector corre-
sponding to the zero eigenvalue.

Thus, we have shown that Var[X] has a zero eigenvalue if and only if X
is not linearly independent.

1.7. Product of Gaussian PDFs.

Let µ1, µ2 ∈ Rn be mean vectors and Σ1, Σ2 ∈ Rn×n be covariance
matrices. Prove that

N (x; µ, Σ) ∝ N (x; µ1, Σ1) · N (x; µ2, Σ2) (1.75)

for some mean vector µ ∈ Rn and covariance matrix Σ ∈ Rn×n. That
is, show that the product of two Gaussian PDFs is proportional to the
PDF of a Gaussian.

1.8. Independence of Gaussians.

Show that two jointly Gaussian random vectors, X and Y, are indepen-
dent if and only if X and Y are uncorrelated.

1.9. Marginal / conditional distribution of a Gaussian.

Prove Theorem 1.24. That is, show that
1. every marginal of a Gaussian is Gaussian; and



fundamentals of inference 33

2. conditioning on a subset of variables of a joint Gaussian is Gaus-
sian

by finding their corresponding PDFs.

Hint: You may use that for matrices Σ and Λ such that Σ−1 = Λ,
• if Σ and Λ[are]symmetric,

⊤ [ ] [ ]
xA ΛAA ΛAB xA
xB ΛBA ΛBB xB

= x⊤AΛAAxA + x⊤AΛABxB + x⊤B ΛBAxA + x⊤B ΛBBxB

= x⊤A(ΛAA −ΛABΛ−1
BBΛBA)xA+

(xB + Λ−1
BBΛBAxA)

⊤ΛBB(xB + Λ−1
BBΛBAxA),

• Λ−1
BB = ΣBB − ΣBAΣ−1

AAΣAB,
• Λ−1

BBΛBA = −ΣBAΣ−1
AA.

The final two equations follow from the general characterization of the inverse
of a block matrix (Petersen et al., 2008, section 9.1.3).

1.10. Closedness properties of Gaussians.

Recall the notion of a moment-generating function (MGF) of a random
vector X in Rn which is define(d as )

φX(t
.

) = E exp t⊤X , for all t ∈ Rn. (1.76)

An MGF uniquely characterizes a distribution. The MGF of the multi-
variate Gaussian X ∼ N (µ, Σ) is( )

φX(t) = exp t⊤
1

µ + t⊤Σt . (1.77)
2

This generalizes the MGF of the univariate Gaussian from Equation (A.40).

Prove the following facts.
1. Closedness under affine transformations: Given an n-dimensional Gaus-

sian X ∼ N (µ, Σ), and A ∈ Rm×n and b ∈ Rm,

AX + b ∼ N (Aµ + b, AΣA⊤). (1.78)

2. Additivity: Given two independent Gaussian random vectors X ∼ N (µ, Σ)
and X′ ∼ N (µ′, Σ′) in Rn,

X + X′ ∼ N (µ + µ′, Σ + Σ′). (1.79)

These properties are unique to Gaussians and a reason for why they
are widely used for learning and inference.



34 probabilistic artificial intelligence

1.11. Expectation and variance of Gaussians.

Derive that E[X] = µ and Var[X] = Σ when X ∼ N (µ, Σ).
Hint: First derive the expectation and variance of a univariate standard nor-
mal random variable.

1.12. Non-affine transformations of Gaussians.

Answer the following questions with yes or no.
1. Does there exist any non-affine transformation of a Gaussian ran-

dom vector which is Gaussian? If yes, give an example.
2. Let X, Y, Z be independent standard normal random variables. Is
√X+YZ

1+Z2 Gaussian?

1.13. Decision theory.

Derive the optimal decisions under the squared loss and the asymmet-
ric loss from Example 1.29.



part I

Probabilistic Machine Learning






Preface to Part I

As humans, we constantly learn about the world around us. We learn
to interact with our physical surroundings. We deepen our under-
standing of the world by establishing relationships between actors, ob-
jects, and events. And we learn about ourselves by observing how
we interact with the world and with ourselves. We then continuously
use this knowledge to make inferences and predictions, be it about the
weather, the movement of a ball, or the behavior of a friend.

With limited computational resources, limited genetic information, and
limited life experience, we are not able to learn everything about the
world to complete certainty. We saw in Chapter 1 that probability the-
ory is the mathematical framework for reasoning with uncertainty in
the same way that logic is the mathematical framework for reasoning
with certainty. We will discuss two kinds of uncertainty: “aleatoric”
uncertainty which cannot be reduced under computational constraints,
and “epistemic” uncertainty which can be reduced by observing more
data.

world
An important aspect of learning is that we do not just learn once, but
continually. Bayes’ rule allows us to update our beliefs and reduce

perception
our uncertainty as we observe new data — a process that is called

D
probabilistic inference. By taking the former posterior as the new prior,
probabilistic inference can be performed continuously and repeated
indefinitely as we observe more and more data. model p(θ | D)

prior p(θ)
Our sensory information is often noisy and imperfect, which is another
source of uncertainty. The same is true for machines, even if they can
sometimes sense aspects of the world more accurately than humans. Figure 1.10: A schematic illustration of

probabilistic inference in the context of
We discuss how one can infer latent structure of the world from sensed the (supervised) learning of a model θ

data, such as the state of a dynamical system like a car, in a process from perceived data D. The prior model
p(θ) can equip the model with anything

that is called filtering. from substantial, to little, to no prior
knowledge.

In this first part of the manuscript, we examine how we can build ma-
chines that are capable of (continual) learning and inference. First, we
introduce probabilistic inference in the context of linear models which



38 probabilistic artificial intelligence

make predictions based on fixed (often hand-designed) features. We
then discuss how probabilistic inference can be scaled to kernel meth-
ods and Gaussian processes which use a large (potentially infinite)
number of features, and to deep neural networks which learn features
dynamically from data. In these models, exact inference is typically in-
tractable, and we discuss modern methods for approximate inference
such as variational inference and Markov chain Monte Carlo. We high-
light a tradeoff between curiosity (i.e., extrapolating beyond the given
data) and conformity (i.e., fitting the given data), which surfaces as a
fundamental principle of probabilistic inference in the regime where
the data and our computational resources are limited.



2
Linear Regression

As a first example of probabilistic inference, we will study linear mod-
els for regression1 which assume that the output y ∈ R is a linear 1 As we have discussed in Section 1.3,
function of the input x ∈ Rd: regression models can also be used for

classification. The canonical example of

y ≈ w⊤
a linear model for classification is logis-

x + w0 tic regression, which we will discuss in
Section 5.1.1.

where w ∈ Rd are the weights and w0 ∈ R is the intercept. Observe
that if we define the extended inputs x′ .

= (x, 1) and w′ .
= (w, w0),

then w′⊤x′ = w⊤x + w0, implying that without loss of generality it y

suffices to study linear functions without the intercept term w 2
0. We

will therefore consider the following function class of linear models
0

f (x; w .
) = w⊤x.

−2

We will consider the supervised learning task of learning weights w −2 0 2
from labeled training data {(xi, yi)}n

i=1. We define the design matrix, x

 
x⊤ Figure 2.1: Example of linear regression

with the least squares estimator (shown
X .

=  1
. ..  ∈ Rn×d, (2.1) in blue).

x⊤n

as the collection of inputs and the vector y .
= [y1 · · · yn]⊤ ∈ Rn as the

collection of labels. For each noisy observation (xi, yi), we define the
value of the approximation of our model, f .

i = w⊤xi. Our model at
the inputs X is described by the vector f .

= [ f1 · · · fn]⊤ which can be
expressed succinctly as f = Xw.

The most common way of estimating w from data is the least squares
estimator,

n
ŵ .

ls = arg min ∑(yi −w⊤xi)
2 = arg min ∥y− Xw∥2

2 , (2.2)
w∈Rd i=1 w∈Rd



40 probabilistic artificial intelligence

minimizing the squared difference between the labels and predictions
of the model. A slightly different estimator is used for ridge regression,

ŵ .
ridg = arg min ∥y− Xw∥2

e 2 + λ ∥w∥2
2 (2.3)

w∈Rd

where λ > 0. The squared L2 regularization term λ ∥w∥2
2 penalizes 2 Ridge regression is more robust to mul-

large w and thus reduces the “complexity” of the resulting model.2 ticollinearity than standard linear re-
gression. Multicollinearity occurs when

It can be shown that the unique solutions to least squares and ridge multiple independent inputs are highly
regression are given by correlated. In this case, their individual

effects on the predicted variable cannot
ŵls = (X⊤X)−1X⊤y and (2.4) be estimated well. Classical linear re-

gression is highly volatile to small input
ŵridge = (X⊤X + λI)−1X⊤y, (2.5) changes. The regularization of ridge re-

gression reduces this volatility by intro-
respectively if the Hessian of the loss is positive definite (i.e., the loss ducing a bias on the weights towards 0.

is strictly convex) ? which is the case as long as the columns of X Problem 2.1 (1)
are not linearly dependent. Least squares regression can be seen as
finding the orthogonal projection of y onto the column space of X, as
is illustrated in Figure 2.2 ? . Problem 2.1 (2) y

2.0.1 Maximum Likelihood Estimation

Since our function class comprises linear functions of the form w⊤x, Xŵls
the observation model from Equation (1.56) simplifies to

yi = w⋆⊤xi + εi (2.6)
span{X}

for some weight vector w, where for the purpose of this chapter we
will additionally assume that εi ∼ N (0, σ2

n) is homoscedastic Gaussian Figure 2.2: Least squares regression
finds the orthogonal projection of y onto

noise.3 This observation model is equivalently characterized by the span{X} (here illustrated as the plane).
Gaussian likelihood, 3 εi is called additive white Gaussian noise.

yi | xi, w ∼ N (w⊤xi, σ2
n). (2.7) using Equation (1.55)

Based on this likelihood we can compute the MLE (1.57) of the weights:
n n

ŵMLE = arg max ∑ log p(yi | xi, w) = arg min ∑(yi −w⊤xi)
2. plugging in the Gaussian likelihood and

w∈Rd i=1 w∈Rd i=1 simplifying

Note that therefore ŵMLE = ŵls.

In practice, the noise variance σ2
n is typically unknown and also has to

be determined, for example, through maximum likelihood estimation.
It is a straightforward exercise to check that the MLE of σ2

n given fixed
weights w is σ̂2

n = 1
n ∑n

i=1(yi −w⊤xi)
2 ? . Problem 2.2

2.1 Weight-space View

The most immediate and natural probabilistic interpretation of lin-
ear regression is to quantify uncertainty about the weights w. Recall



linear regression 41

that probabilistic inference requires specification of a generative model
comprised of prior and likelihood. Throughout this chapter, we will
use the Gaussian prior,

σ2
p

w ∼ N (0, σ2
p I), (2.8) w

σ2
and the Gaussian likelihood from Equation (2.7). We will discuss pos- n

sible (probabilistic) strategies for choosing hyperparameters such as yi

the prior variance σ2
p and the noise variance σ2

n in Section 4.4.
xi

Remark 2.1: Why a Gaussian prior? i ∈ 1 : n

Figure 2.3: Directed graphical model of
The choice of using a Gaussian prior may seem somewhat arbi- Bayesian linear regression in plate nota-
trary at first sight, except perhaps for the nice analytical proper- tion.

ties of Gaussians that we have seen in Section 1.2.3 and which will
prove useful. The maximum entropy principle (cf. Section 1.2.1)
provides a more fundamental justification for Gaussian priors since
turns out that N has the maximum entropy among all distribu-
tions on Rd with known mean and variance ? . Problem 5.6

Next, let us derive the posterior distribution over the weights.

log p(w | x1:n, y1:n)

= log p(w) + log p(y1:n | x1:n, w) + const by Bayes’ rule (1.45)
n

= log [p(w) + ∑ log p(yi | xi, w) + cons
i=1 ]t using independence of the samples

n
= −1

σ−2
p ∥w∥2

2 + σ−2
n ∑(y −w⊤i xi)

2 + const using the Gaussian prior and likelihood
2[ i=1 ]

= −1

[σ−2 ∥2 (∥y− Xw∥2
2 p ∥w 2 + σ−2

n 2 + const )] using ∑n
i=1(yi −w⊤xi)

2 = ∥y− Xw∥2
2

= −1

[σ−2 ⊤
p w w + σ−2 w⊤n X⊤Xw− 2y⊤Xw]+ y⊤y + const

2
= −1

w⊤(σ−2 ⊤X + σ−2 −2 ⊤ .9)
2 n X p I)w− 2σn y Xw + const. (2

Observe that the log-posterior is a quadratic form in w, so the posterior
distribution must be Gaussian:

w | x1:n, y1:n ∼ N (µ, Σ) (2.10a) see Equation (A.12)

where we can read off the mean and variance to be
.

µ = σ(−2
n ΣX⊤y, ) (2.10b)

. −1
Σ = σ−2 ⊤

n X X + σ−2
p I . (2.10c)

This also shows that Gaussians with known variance and linear like-
lihood are self-conjugate, a property that we had hinted at in Sec-
tion 1.2.2. It can be shown more generally that Gaussians with known



42 probabilistic artificial intelligence

variance are self-conjugate to any Gaussian likelihood (Murphy, 2007).
For other generative models, the posterior can typically not be ex-
pressed in closed-form — this is a very special property of Gaussians!

2.1.1 Maximum a Posteriori Estimation

Computing the MAP estimate for the weights,

ŵMAP = arg max log p(y1:n | x1:n, w) + log p(w)
w

2
= arg min ∥y− σ

Xw∥2 2
2 +

n w∥ , (2.11) us ng t at the likelihood and prior are
σ2 ∥ 2 i h

w p Gaussian

we observe that this is identical to ridge regression with weight decay θ2

.
= σ2 1.0

λ n/σ2
p: ŵMAP = ŵridge. Equation (2.11) is simply the MLE loss

with an additional L2-regularization (originating from the prior) that 0.5

encourages keeping weights small. Recall that the MAP estimate cor-
0.0

responds to the mode of the posterior distribution, which in the case
of a Gaussian is simply its mean µ. As to be expected, µ coincides with −0.5

the analytical solution to ridge regression from Equation (2.5). −1.0

−1 0 1

Example 2.2: Lasso as the MAP estimate with a Laplace prior θ1

One problem with ridge regression is that the contribution of Figure 2.4: Level sets of L2- (blue)
nearly-zero weights to the L2-regularization term is negligible. and L1-regularization (red), correspond-

ing to Gaussian and Laplace priors, re-
Thus, L2-regularization is typically not sufficient to perform vari- spectively. It can be seen that L1-
able selection (that is, set some weights to zero entirely), which is regularization is more effective in en-

couraging sparse solutions (that is, so-
often desirable for interpretability of the model. lutions where many components are set

to exactly 0).
A commonly used alternative to ridge regression is the least ab-
solute shrinkage and selection operator (or lasso), which regularizes
with the L1-norm:

ŵ .
lasso = arg min ∥y− Xw∥2

2 + λ ∥w∥1 . (2.12)
w∈Rd

It turns out that lasso can also be viewed as probabilistic infer-
ence, using a Laplace prior w ∼ Laplace(0, h) with length scale h
instead of a Gaussian prior.

Computing the MAP estimate for the weights yields,

ŵMAP = arg max log p(y1:n | x1:n, w) + log p(w)
w

n σ2
= arg min ∑(yi −w⊤x 2

i) + n ∥w∥ (2.13) si g that the likelihood is Gaussian
i h 1 u n

w =1 and the prior is Laplacian

which coincides with the lasso with weight decay .
λ = σ2

n/h.



linear regression 43

To make predictions at a test point x⋆, we define the (model-)predicted
point f ⋆ .

= ŵ⊤MAPx⋆ and obtain the label prediction

y⋆ | x⋆, x1:n, y1:n ∼ N ( f ⋆, σ2
n). (2.14)

Here we observe that using point estimates such as the MAP estimate
does not quantify uncertainty in the weights. The MAP estimate sim-
ply collapses all mass of the posterior around its mode. This can be
harmful when we are highly unsure about the best model, e.g., because
we have observed insufficient data.

2.1.2 Probabilistic Inference

Rather than selecting a single weight vector ŵ to make predictions, we
can use the full posterior distribution. This is known as Bayesian linear
regression (BLR) and illustrated with an example in Figure 2.5.

y y Figure 2.5: Comparison of linear regres-
3 6 sion (MLE), ridge regression (MAP es-

timate), and Bayesian linear regression
4

2 when the data is generated according to
2 y | w, x ∼ N (w⊤x, σ2

n).1
0 The true mean is shown in black, the

0 −2 MLE in blue, and the MAP estimate in
red. The dark gray area denotes the epis-

−1 −4 temic uncertainty of Bayesian linear re-
−0.5 0.0 0.5 1.0 1.5 −0.5 0.0 0.5 1.0 1.5 gression and the light gray area the addi-

x x tional homoscedastic noise. On the left,
σn = 0.15. On the right, σn = 0.7.

To make predictions at a test point x⋆, we let f ⋆ .
= w⊤x⋆ which has

the distribution

f ⋆ | x⋆, x1:n, y1:n ∼ N (µ⊤x⋆, x⋆⊤Σx⋆). (2.15) using the closedness of Gaussians
under linear transformations (1.78)

Note that this does not take into account the noise in the labels σ2
n. For

the label prediction y⋆, we obtain

y⋆ | x⋆, x1:n, y1:n ∼ N (µ⊤x⋆, x⋆⊤Σx⋆ + σ2
n). (2.16) using additivity of Gaussians (1.79)

2.1.3 Recursive Probabilistic Inference

We have already discussed the recursive properties of probabilistic in-
ference in Section 1.3.6. For Bayesian linear regression with a Gaussian
prior and likelihood, this principle can be used to derive an efficient
online algorithm since also the posterior is a Gaussian,

p(t)(w) = N (w; µ(t), Σ(t))(,
d2) (2.17)

which can be stored efficiently using only O parameters. This
leads to an efficient online algorithm for Bayesian linear regression



44 probabilistic artificial intelligence

with time(-independent(!) memory complexity O(d) and round com-
plexity O d2) ? . The interpretation of Bayesian linear regression as an Problem 2.5
online algorithm also highlights similarities to other sequential models
such as Kalman filters, which we discuss in Chapter 3. In Example 3.5,
we will learn that online Bayesian linear regression is, in fact, an ex-
ample of a Kalman filter.

2.2 Aleatoric and Epistemic Uncertainty

The predictive posterior distribution from Equation (2.16) highlights a
decomposition of uncertainty wherein x⋆⊤Σx⋆ corresponds to the un-
certainty about our model due to the lack of data (commonly referred
to as the epistemic uncertainty) and σ2

n corresponds to the uncertainty
about the labels that cannot be explained by the inputs and any model
from the model class (commonly referred to as the aleatoric uncertainty,
“irreducible noise”, or simply “(label) noise”) ? . Problem 2.6

A natural probabilistic approach is to represent epistemic uncertainty
with a probability distribution over models. Intuitively, the variance
of this distribution measures our uncertainty about the model and its
mode corresponds to our current best (point) estimate. The distri-
bution over weights of a linear model is one example, and we will
continue to explore this approach for other models in the following
chapters.

It is a practical modeling choice how much inaccuracy to attribute to
epistemic or aleatoric uncertainty. Generally, when a poor model is
used to explain a process, more inaccuracy has to be attributed to irre-
ducible noise. For example, when a linear model is used to “explain”
a nonlinear process, most uncertainty is aleatoric as the model cannot
explain the data well. As we use more expressive models, a larger
portion of the uncertainty can be explained by the data.

Epistemic and aleatoric uncertainty can be formally defined in terms
of the law of total variance (1.41),

Var[y⋆ | x⋆] = E︸ [ ]
⋆

θ Vary⋆︷[y︷⋆ | x , θ]︸ [ ︷ ]
+ V︸ arθ Ey⋆ [y︷⋆ | x⋆, θ]︸. (2.18)

aleatoric uncertainty epistemic uncertainty

Here, the mean variability of predictions y⋆ averaged across all mod-
els θ is the estimated aleatoric uncertainty. In contrast, the variability of
the mean prediction y⋆ under each model θ is the estimated epistemic
uncertainty. This decomposition of uncertainty will appear frequently
throughout this manuscript.



linear regression 45

2.3 Non-linear Regression
y

We can use linear regression not only to learn linear functions. The
trick is to apply a nonlinear transformation ϕ : Rd → Re to the fea- 10

tures xi, where d is the dimension of the input space and e is the 5

dimension of the designed feature space. We denote the design ma- 0
trix comprised of transformed features by Φ ∈ Rn×e. Note that if the −5
feature transformation ϕ is the identity function then Φ = X.

−2 0 2
x

Example 2.3: Polynomial regression
. Figure 2.6: Applying linear regression

Let ϕ(x) = [x2, x, 1] and w .
= [a, b, c]. Then the function that our with a feature space of polynomials of

model learns is given as degree 10. The least squares estimate is
shown in blue, ridge regression in red,

f = ax2 + bx + c. and lasso in green.

Thus, our model can exactly represent all polynomials up to de-
gree 2.

However, to learn polynomials of degree m in d input dimensions,
we need to apply the nonlinear transformation

ϕ(x) = [1, x1, . . . , xd, x2
1, . . . , x2

d, x1 · x2, . . . , xd−1 · xd,
. . . ,
xd−m+1 · · · · · xd].

Note that the feature dimension e is ∑m
i=0 (

d+i−1
i ) = Θ(dm).4 Thus, 4 Observe that the vector contains (d+i−1

i )

the dimension of the feature space grows exponentially in the de- monomials of degree i as this is the
number of ways to choose i times from

gree of polynomials and input dimensions. Even for relatively d items with replacement and without
small m and d, this becomes completely unmanageable. consideration of order. To see this, con-

sider the following encoding: We take
a sequence of d + i − 1 spots. Select-

The example of polynomials highlights that it may be inefficient to ing any subset of i spots, we interpret

keep track of the weights w ∈ Re when e is large, and that it may the remaining d − 1 spots as “barriers”
separating each of the d items. The se-

be useful to instead consider a reparameterization which is of dimen- lected spots correspond to the number
sion n rather than of the feature dimension. of times each item has been selected. For

example, if 2 items are to be selected out
of a total of 4 items with replacement,
one possible configuration is “◦ || ◦ |”

2.4 Function-space View where ◦ denotes a selected spot and |
denotes a barrier. This configuration en-

Let us now look at Bayesian linear regression through a different lens. codes that the first and third item have
each been chosen once. The number of

Previously, we have been interpreting it as a distribution over the possible configurations — each encoding
weights w of a linear function f = Φw. The key idea is that for a a unique outcome — is therefore (d+i−1

i ).

finite set of inputs (ensuring that the design matrix is well-defined),
we can equivalently consider a distribution directly over the estimated
function values f . We call this the function-space view of Bayesian linear
regression.



46 probabilistic artificial intelligence

Instead of considering a prior over the weights w ∼ N (0, σ2
p I) as we

have done previously, we now impose a prior directly on the values of
our model at the observations. Using that Gaussians are closed under
linear maps (1.78), we obtain the equivalent prior y

f | X ∼ N (ΦE[w], ΦVar[w]Φ⊤) = N (0, σ︸ 2
pΦ︷︷Φ⊤︸) (2.19) 2 fn

K 0

where K ∈ Rn×n is the so-called kernel matrix. Observe that the entries −2

of the kernel matrix can be expressed as K(i, j) = σ2
p ·ϕ(xi)

⊤ϕ(xj).
f

− 1
4

You may say that nothing has changed, and you would be right — x1 xn

that is precisely the point. Note, however, that the shape of the kernel x

matrix is n × n rather than the e × e covariance matrix over weights, Figure 2.7: An illustration of the
which becomes unmanageable when e is large. The kernel matrix K function-space view. The model is de-

has entries only for the finite set of observed inputs. However, in scribed by the points (xi , fi).

principle, we could have observed any input, and this motivates the
definition of the kernel function

k .
(x, x′) = σ2

p ·ϕ(x)⊤ϕ(x′) (2.20)

for arbitrary inputs x and x′. A kernel matrix is simply a finite “view”
of the kernel function,  

K k(x , x ) · · · (x , xn)

 1 1 k 1

= .. . . 
. . . ..  (2.21)

k(xn, x1) · · · k(xn, xn)

Observe that by definition of the kernel matrix in Equation (2.19), the
kernel matrix is a covariance matrix and the kernel function measures
the covariance of the function values f (x) and f (x′) given inputs x
and x′:

k(x, x′
[ ]

) = Cov f (x), f (x′) . (2.22)

Moreover, note that we have reformulated5 the learning algorithm 5 we often say “kernelized”
such that the feature space is now implicit in the choice of kernel, and
the kernel is defined by inner products of (nonlinearly transformed)
inputs. In other words, the choice of kernel implicitly determines the
class of functions that f is sampled from (without expressing the func-
tions explicitly in closed-form), which encodes our prior beliefs. This
is known as the kernel trick.

2.4.1 Learning and Predictions

We have already kernelized the Bayesian linear regression prior. The
posterior distribution f | X, y is again Gaussian due to the closedness



linear regression 47

properties of Gaussians, analogously to our derivation of the prior
kernel matrix in Equation (2.19).

It remains to show that we can also rely on the kernel trick for predic-
tions. Given the tes[t point x]⋆, we defi[ne ] [ ]

. Φ f
Φ̃ = , ỹ . y

, f̃ .
ϕ(x⋆)⊤

=
y⋆

= ⋆ .
f

We immediately obtain f̃ = Φ̃w. Analogously to our analysis of pre-
dictions from the weight-space view, we add the label noise to obtain
the estimate ỹ = f̃ + ε̃ where .

ε̃ = [ε1 · · · εn ε⋆]⊤ ∼ N (0, σ2
n I) is the

independent label noise. Applying the same reasoning as we did for
the prior, we obtain

f̃ | X, x⋆ ∼ N (0, K̃) (2.23)

where K̃ .
= σ2

pΦ̃Φ̃⊤. Adding the label noise yields

ỹ | X, x⋆ ∼ N (0, K̃ + σ2
n I). (2.24)

Finally, we can conclude from the closedness of Gaussian random vec-
tors under conditional distributions (1.53) that the predictive posterior
y⋆ | x⋆, X, y follows again a normal distribution. We will do a full
derivation of the posterior and predictive posterior in Section 4.1.

2.4.2 Efficient Polynomial Regression

But how does the kernel trick address our concerns about efficiency
raised in Section 2.3? After all, computing the kernel for a feature
space of dimension e still requires computing sums of length e which
is prohibitive when e is large. The kernel trick opens up a couple of
new doors for us:
1. For certain feature transformations ϕ, we may be able to find an

easier to compute expression equivalent to ϕ(x)⊤ϕ(x′).
2. If this is not possible, we could approximate the inner product by

an easier to compute expression.
3. Or, alternatively, we may decide not to care very much about the

exact feature transformation and simply experiment with kernels
that induce some feature space (which may even be infinitely di-
mensional).

We will explore the third approach when we revisit kernels in Sec-
tion 4.3. A polynomial feature transformation can be computed effi-
ciently in closed-form.

Fact 2.4. For the polynomial feature transformation ϕ up to degree m from
Example 2.3, it can be shown that up to constant factors,

ϕ(x)⊤ϕ(x′) = (1 + x⊤x′)m. (2.25)



48 probabilistic artificial intelligence

For example, for input dimension 2, the kernel (1 + x⊤x′)2
√ √ √ corre-

sponds to the feature vector ϕ(x) = [1 2x1 2x2 2x x 2 2
1 2 x1 x ⊤

2] .

Discussion

We have explored a probabilistic perspective on linear models, and
seen that classical approaches such as least squares and ridge regres-
sion can be interpreted as approximate probabilistic inference. We
then saw that we can even perform exact probabilistic inference effi-
ciently if we adopt a Gaussian prior and Gaussian noise assumption.
These are already powerful tools, which are often applied also to non-
linear models if we treat the latent feature space — which was either
human-designed or learned via deep learning — as fixed. In the next
chapter, we will digress briefly from the storyline on “learning” to see
how we can adopt a similar probabilistic perspective to track latent
states over time. Then, in Chapter 4, we will see how we can use the
function-space view and kernel trick to learn flexible nonlinear models
with exact probabilistic inference, without ever explicitly representing
the feature space.

Problems

2.1. Closed-form linear regression.
1. Derive the unique solutions to least squares and ridge regression

from Equations (2.4) and (2.5).
2. For an n×m matrix A and vector x ∈ Rm, we call ΠAx the orthogo-

nal projection of x onto span{A} = {Ax′ | x′ ∈ Rm}. In particular,
an orthogonal projection satisfies x−ΠAx ⊥ Ax′ for all x′ ∈ Rm.
Show that ŵls from Equation (2.4) is such that Xŵls is the unique
closest point to y on span{X}, i.e., it satisfies Xŵls = ΠX y.

2.2. MLE of noise variance.

Show that the MLE of σ2
n given fixed weights w is

1 n
σ̂2

n = (y )2. (2.26)
n ∑ i −w⊤xi

i=1

2.3. Variance of least squares around training data.

Show that the variance of a prediction at the point [1 x⋆]⊤ is small-
est when x⋆ is the mean of the training data. More formally, show
that if inputs are of the form xi =[ [1 xi]

⊤ where x]i ∈ R and ŵls is
the least squares estimate, then Var y⋆ | [1 x⋆]⊤, ŵls is minimized for
x⋆ = 1

n ∑n
i=1 xi.



linear regression 49

2.4. Bayesian linear regression.

Suppose you are given thefollowing observations

  
1 1

1 2


X  
, y 2.4

=
2 1 4.3

= 
3.1

2 2 4.9

and assume the data follows a linear model with homoscedastic noise
N (0, σ2

n) where σ2
n = 0.1.

1. Find the maximum likelihood estimate ŵMLE given the data.
2. Now assume that we have a prior p(w) = N (w; 0, σ2

p I) with σ2
p = 0.05.

Find the MAP estimate ŵMAP given the data and the prior.
3. Use the posterior p(w | X, y) to get a posterior prediction for the

label y⋆ at x⋆ = [3 3]⊤. Report the mean and the variance of this
prediction.

4. How would you have to change the prior p(w) such that

ŵMAP → ŵMLE?

2.5. Online Bayesian linear regression.
1. Can you design an algorithm that updates the posterior (as op-

posed to recalculating it from scratch using Equation (2.10)) in a
smarter way? The requirement is that the memory should not grow
as O(t).

2. If d is large, computing the inverse every round is very expen-
sive. Can you use the recursive structure you found in the previ-
ous questio(n to bring down the computational complexity of every
round to O d2)?

The resulting efficient online algorithm is known as online Bayesian
linear regression.

2.6. Aleatoric and epistemic uncertainty of BLR.

Prove for Bayesian linear regression that x⋆⊤Σx⋆ is the epistemic un-
certainty and σ2

n the aleatoric uncertainty in y⋆ under the decomposi-
tion of Equation (2.18).

2.7. Hyperpriors.

We consider a dataset {(xi, yi)}n
i=1 of size n, where xi ∈ Rd denotes

the feature vector and yi ∈ R denotes the label of the i-th data point.
Let εi be i.i.d. samples from the Gaussian distribution N (0, λ−1) for a
given λ > 0. We collect the labels in a vector y ∈ Rn, the features in
a matrix X ∈ Rn×d, and the noise in a vector ε ∈ Rn. The labels are
generated according to y = Xw + ε.



50 probabilistic artificial intelligence

To perform Bayesian Linear Regression, we consider the prior distribu-
tion over the parameter vector w to be N (µ, λ−1 Id), where Id denotes
the d-dimensional identity matrix and µ ∈ Rd is a hyperparameter.
1. Given this Bayesian data model, what is the conditional covariance

matrix .
Σy = Var[y | X, µ, λ]?

2. Calculate the maximum likelihood estimate of the hyperparame-
ter µ.

3. Since we are unsure about the hyperparameter µ, we decide to
model our uncertainty about µ by placing the “hyperprior” µ ∼ N (0, Id).
Is the posterior distribution p(µ | X, y, λ) a Gaussian distribution?
If yes, what are its mean vector and covariance matrix?

4. What is the posterior distribution p(λ | X, y, µ)?
Hint: For any a ∈ R, A ∈ Rn×n it holds that det(aA) = andet(A).



3
Filtering

Before we continue in Chapter 4 with the function-space view of re-
gression, we want to look at a seemingly different but very related
problem. We will study Bayesian learning and inference in the state
space model, where we want to keep track of the state of an agent over
time based on noisy observations. In this model, we have a sequence
of (hidden) states (Xt)t∈N0 where Xt is in Rd and a sequence of obser-
vations (Yt)t∈N0 where Yt is in Rm.

The process of keeping track of the state using noisy observations is
also known as Bayesian filtering or recursive Bayesian estimation. Fig-
ure 3.1 illustrates this process, where an agent perceives the current
state of the world and then updates its beliefs about the state based on
this observation.

Figure 3.1: Schematic view of Bayesian
filtering: An agent perceives the current
state of the world and updates its belief

worldt worldt+1 accordingly.

perception perception
Dt Dt+1

model p(θ | D1:t) model p(θ | D1:t+1)

We will discuss Bayesian filtering more broadly in the next section. A
Kalman filter is an important special case of a Bayes’ filter, which uses
a Gaussian prior over the states and conditional linear Gaussians to
describe the evolution of states and observations. Analogously to the



52 probabilistic artificial intelligence

previous chapter, we will see that inference in this model is tractable
due to the closedness properties of Gaussians.

Definition 3.1 (Kalman filter). A Kalman filter is specified by a Gaus-
sian prior over the states,

X0 ∼ N (µ, Σ), (3.1)

and a conditional linear Gaussian motion model and sensor model,

X .
t+ = FXt + εt F ∈ Rd×d

1 , εt ∼ N (0, Σx), (3.2)
Y .

= HXt + ηt H ∈ Rm×d
t , ηt ∼ N (0, Σy), (3.3)

respectively. The motion model is sometimes also called transition
model or dynamics model. Crucially, Kalman filters assume that F and X1 X2 X3 · · ·
H are known. In general, F and H may depend on t. Also, ε and η
may have a non-zero mean, commonly called a “drift”. Y1 Y2 Y3

Because Kalman filters use conditional linear Gaussians, which we Figure 3.2: Directed graphical model of a
Kalman filter with hidden states Xt and

have already seen in Equation (1.55), their joint distribution (over all observables Yt.
variables) is also Gaussian. This means that predicting the future states
of a Kalman filter is simply inference with multivariate Gaussians. In
Bayesian filtering, however, we do not only want to make predictions
occasionally. In Bayesian filtering, we want to keep track of states, that
is, predict the current state of an agent online.1 To do this efficiently, 1 Here, online is common terminology to
we need to update our belief about the state of the agent recursively, say that we want to perform inference

at time t without exposure to times t +
similarly to our recursive Bayesian updates in Bayesian linear regres- 1, t + 2, . . . , so in “real-time”.
sion (see Section 2.1.3).

From the directed graphical model of a Kalman filter shown in Fig-
ure 3.2, we can immediately gather the following conditional indepen-
dence relations,2 2 Alternatively, they follow from the def-

inition of the motion and sensor models
X as linear updates.

t+1 ⊥ X1:t−1, Y1:t−1 | Xt, (3.4)
Yt ⊥ X1:t−1 | Xt (3.5)
Yt ⊥ Y1:t−1 | Xt−1. (3.6)

The first conditional independence property is also known as the Markov
property, which we will return to later in our discussion of Markov
chains and Markov decision processes. This characterization of the
Kalman filter, yields the following factorization of the joint distribu-
tion:

t
p(x1:t, y1:t) = ∏ p(xi | x1:i−1)p(yi | x1:t, y1:i−1) using the product rule (1.11)

i=1
t

= p(x1)p(y1 | x1)∏ p(xi | xi−1)p(yi | xi). (3.7) using the conditional independence
i=2 properties from (3.4), (3.5), and (3.6)



filtering 53

3.1 Conditioning and Prediction

We can describe Bayesian filtering by the following recursive scheme
with the two phases, conditioning (also called “update”) and prediction:

Algorithm 3.2: Bayesian filtering
1 start with a prior over initial states p(x0)
2 for t = 1 to ∞ do
3 assume we have p(xt | y1:t−1)
4 conditioning: compute p(xt | y1:t) using the new observation yt
5 prediction: compute p(xt+1 | y1:t)

Let us consider the conditioning step first:

1
p(xt | y1:t) = p(x

Z t | y1:t−1)p(yt | xt, y1:t−1) using Bayes’ rule (1.45)

1
= p(x

Z t | y1:t−1)p(yt | xt). (3.8) using the conditional independence
structure (3.6)

For the prediction step, we∫obtain,

p(xt+1 | y1:t) = ∫ p(xt+1, xt | y1:t) dxt using the sum rule (1.7)

= ∫ p(xt+1 | xt, y1:t)p(xt | y1:t) dxt using the product rule (1.11)

= p(xt+1 | xt)p(xt | y1:t) dxt. (3.9) using the conditional independence
structure (3.4)

In general, these distributions can be very complicated, but for Gaus-
sians (i.e., Kalman filters) they can be expressed in closed-form.

Remark 3.3: Bayesian smoothing
Bayesian smoothing is a closely related task to Bayesian filtering.
While Bayesian filtering methods estimate the current state based
only on observations obtained before and at the current time step,
Bayesian smoothing computes the distribution of Xk | y1:t where
t > k. That is Bayesian smoothing estimates Xk based on data until
and beyond time k. Note that if k = t, then Bayesian smoothing
coincides with Bayesian filtering.

Analogously to Equation (3.8),

p(xk | y1:t) ∝ p(xk | y1:k)p(yk+1:t | xk). (3.10)

If we assume a Gaussian prior and conditional Gaussian transi-
tion and dynamics models (this is called Kalman smoothing), then



54 probabilistic artificial intelligence

by the closedness properties of Gaussians, Xk | y1:t is a Gaus-
sian. Indeed, all terms of Equation (3.10) are Gaussian PDFs and
as seen in Equation (1.75), the product of two Gaussian PDFs is
again proportional to a Gaussian PDF.

The first term, Xk | y1:k, is the marginal posterior of the hidden
states of the Kalman filter which can be obtained with Bayesian
filtering.

By conditioning∫on Xk+1, we have for the second term,

p(yk+1:t | xk) = ∫ p(yk+1:t | xk, xk+1)p(xk+1 | xk) dxk+1 using the sum rule (1.7) and product
rule (1.11)

= ∫ p(yk+1:t | xk+1)p(xk+1 | xk) dxk+1 using the conditional independence
structure (3.5)

= p(yk+1 | xk+1)p(yk+2:t | xk+1)p(xk+1 | xk) dxk+1 using the conditional independence
structure (3.6)

(3.11)

Let us have a look at the terms in the product:
• p(yk+1 | xk+1) is obtained from the sensor model,
• p(xk+1 | xk) is obtained from the transition model, and
• p(yk+2:t | xk+1) can be computed recursively backwards in time.
This recursion results in linear equations resembling a Kalman
filter running backwards in time.

Thus, in the setting of Kalman smoothing, both factors of Equa-
tion (3.10) can be computed efficiently: one using a (forward)
Kalman filter; the other using a “backward” Kalman filter. More
concretely, in time O(t), we can compute the two factors for all
k ∈ [t]. This approach is known as two-filter smoothing or the
forward-backward algorithm.

3.2 Kalman Filters

Let us return to the setting of Kalman filters where priors and likeli-
hoods are Gaussian. Here, we will see that the update and prediction
steps can be computed in closed form.

3.2.1 Conditioning

The conditioning operation in Kalman filters is also called the Kalman
update. Before introducing the general Kalman update, let us consider
a simpler example:



filtering 55

Example 3.4: Random walk in 1d

We use the simple motion and sensor models,3 3 This corresponds to F = H = I and a
drift of 0.

Xt+1 | xt ∼ N (xt, σ2
x), (3.12a)

Yt | xt ∼ N (xt, σ2
y ). (3.12b)

Let Xt | y1:t ∼ N (µt, σ2
t ) be our belief at time t. It can be shown

that Bayesian filtering yields the belief Xt+1 | y1:t+1 ∼ N (µt+1, σ2
t+1)

at time t + 1 where ? Problem 3.1

+ (σ2
. σ2

y µt t + σ2
x)yt+1 (σ2 + σ2

µt+1 = , t x)σ
2
y

σ2 .
= . (3.13)

σ2 2 2 t+1
t + σx + σy σ2

t + σ2
x + σ2

y

Although looking intimidating at first, this update has a very nat-
ural interpretation. Let us define the following quantity,

. σ2 σ2
λ = t + x σ2

= 1− y ∈ [0, 1]. (3.14)
σ2 2 2

t + σx + σy σ2
t + σ2

x + σ2
y

Using λ, we can write the updated mean as a convex combination
of the previous mean and the observation,

µt+1 = (1− λ)µt + λyt+1 (3.15)
= µt + λ(yt+1 − µt). (3.16)

Intuitively, λ is a form of “gain” that influences how much of the
new information should be incorporated into the updated mean.

1 2 3 4 5 6
For this reason, λ is also called Kalman gain. t

The updated variance can similarly be rewritten, Figure 3.3: Hidden states during a ran-
dom walk in one dimension.

σ2
t+1 = λσ2

y = (1− λ)(σ2
t + σ2

x). (3.17)

In particular, observe that if µt = yt+1 (i.e., we observe our predic-
tion), we have µt+1 = µt as there is no new information. Similarly,
for σ2

y → ∞ (i.e., we do not trust our observations), we have

λ→ 0, µ 2
t+1 = µt, σt+1 = σ2

t + σ2
x .

In contrast, for σ2
y → 0, we have

λ→ 1, µt+1 = yt+1, σ2
t+1 = 0.

The general formulas for the Kalman update follow the same logic as
in the above example of a one-dimensional random walk. Given the

x



56 probabilistic artificial intelligence

prior belief Xt | y1:t ∼ N (µt, Σt), we have

Xt+1 | y1:t+1 ∼ N (µt+1, Σt+1) where (3.18a)
.

µt+1 = Fµt + Kt+1(yt+1 − HFµt), (3.18b)
.

Σt+1 = (I − Kt+1H)(FΣtF⊤ + Σx). (3.18c)

Hereby, Kt+1 is the Kalman gain,

K .
t+1 = (FΣtF⊤ + Σx)H⊤(H(FΣtF⊤ + Σx)H⊤ + Σy)

−1 ∈ Rd×m.
(3.18d)

Note that Σt and Kt can be computed offline as they are independent
of the observation yt+1. Fµt represents the expected state at time t + 1,
and hence, HFµt corresponds to the expected observation. Therefore,
the term yt+1 − HFµt measures the error in the predicted observation
and the Kalman gain Kt+1 appears as a measure of relevance of the
new observation compared to the prediction.

Example 3.5: Bayesian linear regression as a Kalman filter
Even though they arise from a rather different setting, it turns
out that Kalman filters are a generalization of Bayesian linear re-
gression! To see this, recall the online Bayesian linear regression
algorithm from Section 2.1.3. Observe that by keeping attempting
to estimate the (hidden) weights w⋆ from sequential noisy obser-
vations yt, this algorithm performs Bayesian filtering! Moreover,
we have used a Gaussian prior and likelihood. This is precisely
the setting of a Kalman filter!

Concretely, we are estimating the constant (i.e., F = I, ε = 0)
hidden state xt = w(t) with prior w(0) ∼ N (0, σ2

p I).

Our sensor model is time-dependent, since in each iteration we
observe a different input xt. Furthermore, we only observe a
scalar-valued label yt.4 Formally, our sensor model is character- 4 That is, m = 1 in our general Kalman
ized by ht = x⊤t and noise ηt = εt with εt ∼ N (0, σ2 filter formulation from above.

n).

You will show in ? that the Kalman update (3.18) is the online Problem 3.2
equivalent to computing the posterior of the weights in Bayesian
linear regression.

3.2.2 Predicting

Using now that the marginal posterior of Xt is a Gaussian due to the
closedness properties of Gaussians, we have

Xt+1 | y1:t ∼ N (µ̂t+1, Σ̂t+1), (3.19)



filtering 57

and it suffices to compute the prediction mean µ̂t+1 and covariance
matrix Σ̂t+1.

For the mean,

µ̂t+1 = E[xt+1 | y1:t]

= E[Fxt + εt | y1:t] using the motion model (3.2)

= FE[xt | y1:t] using linearity of expectation (1.20) and
E[εt] = 0

= Fµt. (3.20) using the mean of the Kalman update

For the covariance[matrix, ∣
Σ̂t+1 = E ([xt+1 − µ̂t+1)(xt+1 − ∣µ̂ ⊤ ∣

t+1) ∣ ]
y

∣ ] 1:t [ ] using the definition of the covariance
matrix (1.36)

= FE (xt − µt)(x ⊤
t − µt) ∣ y ⊤

1:t F + E ε ε⊤t t using (3.20), the motion model (3.2) and
that εt is independent of the

= FΣtF⊤ + Σx. (3.21) observations

Optional Readings
Kalman filters and related models are often called temporal models.
For a broader look at such models, read chapter 15 of “Artificial
intelligence: a modern approach” (Russell and Norvig, 2002).

Discussion

In this chapter, we have introduced Kalman filters as a special case of
probabilistic filtering where probabilistic inference can be performed
in closed form. Similarly to Bayesian linear regression, probabilistic in-
ference is tractable due to assuming Gaussian priors and likelihoods.
Indeed, learning linear models and Kalman filters are very closely re-
lated as seen in Example 3.5, and we will further explore this relation-
ship in Problem 4.3. We will refer back to filtering in the second part
of this manuscript when we discuss sequential decision-making with
partial observability of the state space. Next, we return to the storyline
on “learning” using exact probabilistic inference.

Problems

3.1. Kalman update.

Derive the predictive distribution Xt+1 | y1:t+1 (3.13) of the Kalman fil-
ter described in the above example using your knowledge about mul-
tivariate Gaussians from Section 1.2.3.
Hint: First compute the predictive distribution Xt+1 | y1:t.



58 probabilistic artificial intelligence

3.2. Bayesian linear regression as a Kalman filter.

Recall the specific Kalman filter from Example 3.5. With this model
the Kalman update (3.18) simplifies to

Σ
k −1xt

t =
t , (3.22a)

x⊤t Σt−1xt + σ2
n

µt = µt−1 + kt(y ⊤
t − xt µt−1), (3.22b)

Σt = Σt− ⊤
1 − ktxt Σt−1, (3.22c)

with µ0 = 0 and Σ0 = σ2
p I. Note that the Kalman gain kt is a vector

in Rd. We assume σ2
n = σ2

p = 1 for simplicity.

Prove by induction that the (µt, Σt) produced by the Kalman update
are equivalent to (µ, Σ) from the posterior of Bayesian linear regres-
sion (2.10) given x1:t, y1:t. You may use that Σ−1

t kt = xt.

Hint: In the inductive step, first prove the equivalence of Σt and then expand
Σ−1

t µt to prove the equivalence of µt.

3.3. Parameter estimation using Kalman filters.

Suppose that we want to estimate the value of an unknown constant π
using uncorrelated measurements

yt = π + ηt, ηt ∼ N (0, σ2
y ).

1. How can this problem be formulated as a Kalman filter? Compute
closed form expressions for the Kalman gain and the variance of
the estimation error σ2

t in terms of t, σ2 2
y , and σ0 .

2. What is the Kalman filter when t→ ∞?
3. Suppose that one has no prior assumptions on π, meaning that

µ0 = 0 and σ2
0 → ∞. Which well-known estimator does the Kalman

filter reduce to in this case?



4
Gaussian Processes

Let us remember our first attempt from Chapter 2 at scaling up Bayesian
linear regression to nonlinear functions. We saw that we can model
nonlinear functions by transforming the input space to a suitable higher-
dimensional feature space, but found that this approach scales poorly
if we require a large number of features. We then found something
remarkable: by simply changing our perspective from a weight-space
view to a function-space view, we could implement Bayesian linear
regression without ever needing to compute the features explicitly.
Under the function-space view, the key object describing the class of
functions we can model is not the features ϕ(x), but instead the kernel
function which only implicitly defines a feature space. Our key ob-
servation in this chapter is that we can therefore stop reasoning about
feature spaces, and instead directly work with kernel functions that
describe “reasonable” classes of functions.

We are still concerned with the problem of estimating the value of
a function f : X → R at arbitrary points x⋆ ∈ X given training
data {xi, yi}n

i=1, where the labels are assumed to be corrupted by ho-
moscedastic Gaussian noise with variance σ2

n,

yi = f (xi) + εi, εi ∼ N (0, σ2
n).

As in Chapter 2 on Bayesian linear regression, we denote by X the
design matrix (collection of training inputs) and by y the vector of
training labels. We will represent the unknown function value at a
point x ∈ X by the random variable f .

x = f (x). The collection of these
random variables is then called a Gaussian process if any finite subset
of them is jointly Gaussian:

Definition 4.1 (Gaussian process, GP). A Gaussian process is an infinite
set of random variables such that any finite number of them are jointly
Gaussian and such that they are consistent under marginalization.1 1 That is, if you take a joint distribution

for n variables and marginalize out one
of them, you should recover the joint dis-
tribution for the remaining n − 1 vari-
ables.



60 probabilistic artificial intelligence

The fact that with a Gaussian process, any finite subset of the random
variables is jointly Gaussian is the key property allowing us to perform
exact probabilistic inference. Intuitively, a Gaussian process can be
interpreted as a normal distribution over functions — and is therefore
often called an “infinite-dimensional Gaussian”.

A Gaussian process is characterized by a mean function µ : X → R and
a covariance function (or kernel function) k : X × X → (x))

R such that for x p( f

any set of points A .
= {x1, . . . , xm} ⊆ X , we have Figure 4.1: A Gaussian process can be

interpreted as an infinite-dimensional
f .

A = [ fx n over functions. At any loca-
1 · · · fxm ]

⊤ ∼ N (µA, KAA) (4.1) Gaussia
tion x in the domain, this yields a dis-

where     tribution over values f (x) shown in red.
The blue line corresponds to the MAP

µ(x1) k(x x ) · k x , m estimate (i.e., mean function of the Gaus-
. 

µ =  .   1 · · ( 1 x )
. .  sian process), the dark gray region corre-

A .. , KAA =  1,
. . . .
. . ..  . (4.2) sponds to the epistemic uncertainty and

(x the light gray region denotes the addi-
µ m) k(xm, x1) · · · k(xm, xm) tional aleatoric uncertainty.

We write f ∼ GP(µ, k). In particular, given a mean function, covari-
ance function, and using the homoscedastic noise assumption,

y⋆ | x⋆ ∼ N (µ(x⋆), k(x⋆, x⋆) + σ2
n). (4.3)

Commonly, for notational simplicity, the mean function is taken to
be zero. Note that for a fixed mean this is not a restriction, as we
can simply apply the zero-mean Gaussian process to the difference
between the mean and the observations.2 2 For alternative ways of representing a

mean function, refer to section 2.7 of
“Gaussian processes for machine learn-

4.1 Learning and Inference ing” (Williams and Rasmussen, 2006).

First, let us look at learning and inference in the context of Gaussian
processes. With slight abuse of our previous notation, let us denote the
set of observed points by A .

= {x1, . . . , xn}. Given a prior f ∼ GP(µ, k)
and the noisy observations yi = f (xi) + εi with εi ∼ N (0, σ2

n), we can
then write the joint distribution of the observations y1:n and the noise-
fre[e p]rediction f ⋆ at a test point x⋆ as

y
⋆ | x⋆, X ∼ N (µ̃, K̃), where (4.4)

f [ ] [ ] 
k(x, x

. µ K 2  
1)

A +
, K̃ . n I kx⋆= = AA σ ,A k .

µ̃ =  ..
µ(x⋆) k⊤x⋆ ,A k(x⋆, x⋆

, x,A
)  

.  .
k(x, xn)

(4.5)

Deriving the conditional distribution using (1.53), we obtain that the
Gaussian process posterior is given by

f | x ′
1:n, y1:n ∼ GP(µ , k′), where (4.6)

y f (x)



gaussian processes 61

µ′ .
(x) = µ(x) + k⊤x,A(KAA + σ2

n I)−1(yA − µA), (4.7)
k′ .
(x, x′) = k(x, x′)− k⊤x,A(KAA + σ2

n I)−1kx′ ,A. (4.8)

Observe that analogously to Bayesian linear regression, the posterior
covariance can only decrease when conditioning on additional data,
and is independent of the observations yi.

We already studied inference in the function-space view of Bayesian
linear regression, but did not make the predictive posterior explicit.
Using Equation (4.6), the predictive posterior at x⋆ is simply

f ⋆ | x⋆, x1:n, y1:n ∼ N (µ′(x⋆), k′(x⋆, x⋆)). (4.9)

4.2 Sampling

Often, we are not interested in the full predictive posterior distribution,
but merely want to obtain samples of our Gaussian process model. We
will briefly examine two approaches.
1. For the first approach, consider a discretized subset of points

f .
= [ f1, . . . , fn]

that we want to sample.3 Note that f ∼ N (µ, K). We have already 3 For example, if we want to render the
seen in Equation (1.54) that function, the length of this vector could

be guided by the screen resolution.
f = K1/2ε + µ (4.10)

where K1/2 is the square root of K and ε ∼ N (0, I) is standard
Ga(us)sian noise.4 However, computing the square root of K takes 4 We discuss square roots of matrices in
O n3 time. Appendix A.2.

2. For the second approach, recall the product rule (1.11),
n

p( f1, . . . , fn) = ∏ p( fi | f1:i−1).
i=1

That is the joint distribution factorizes neatly into a product where
each factor only depends on the “outcomes” of preceding factors.
We can therefore obtain samples one-by-one, each time condition-
ing on one more observation:

f1 ∼ p( f1)

f2 ∼ p( f2 | f1)

f3 ∼ p( f3 | f1, f (4.11)
2)

...
This general approach is known as forward sampling. Due to the ma-
trix inverse i(n the formula of the GP posterior (4.6), this approach
also takes O n3) time.

We will discuss more efficient approximate sampling methods in Sec-
tion 4.5.



62 probabilistic artificial intelligence

4.3 Kernel Functions

We have seen that kernel functions are the key object describing the
class of functions a Gaussian process can model. Depending on the
kernel function, the “shape” of functions that are realized from a Gaus-
sian process varies greatly. Let us recap briefly from Section 2.4 what
a kernel function is:

Definition 4.2 (Kernel function). A kernel function k : X × X → R

satisfies
• k(x, x′) = k(x′, x) for any x, x′ ∈ X (symmetry), and
• KAA is positive semi-definite for any A ⊆ X .
The two defining conditions ensure that for any A ⊆ X , KAA is a valid
covariance matrix. We say that a kernel function is positive definite if
KAA is positive definite for any A ⊆ X .

Intuitively, the kernel function evaluated at locations x and x′ describes
how f (x) and f (x′) are related, whi[ch we can express formally as

k(x, x′
]

) = Cov f (x), f (x′) . (4.12)

If x and x′ are “close”, then f (x) and f (x′) are usually taken to be
positively correlated, encoding a “smooth” function.

f (x)
In the following, we will discuss some of the most common kernel
functions, how they can be combined to create “new” kernels, and
how we can characterize the class of functions they can model.

4.3.1 Common Kernels

First, we look into some of the most commonly used kernels. Often an
additional factor σ2 (output scale) is added, which we assume here to x
be 1 for simplicity.

Figure 4.2: Functions sampled according
1. The linear kernel is defined as to a Gaussian process with a linear ker-

nel and ϕ = id.
k(x, x′; .

ϕ) = ϕ(x)⊤ϕ(x′) (4.13)
f (x)

where ϕ is a nonlinear transformation as introduced in Section 2.3
or the identity.

Remark 4.3: GPs with linear kernel and BLR
A Gaussian process with a linear kernel is equivalent to Bayesian x
linear regression. This follows directly from the function-space Figure 4.3: Functions sampled accord-
view of Bayesian linear regression (see Section 2.4) and com- ing to a Gaussian process with a linear
paring the derived kernel function (2.20) with the definition of kernel and ϕ(x) = [1, x, x2] (left) and

ϕ(x) = sin(x) (right).
the linear kernel (4.13).



gaussian processes 63

2. The Gaussian kernel (also known as squared exponential kernel or ra-
dial basis function (RBF) kernel) is d(efined as )

. ∥x− x′∥2
k(x, x′; h) = exp − 2 (4.

2h2 14)

where h is its length scale. The larger the length scale h, the smoother 5 As the length scale is increased, the ex-
the resulting functions.5 Furthermore, it turns out that the feature ponent of the exponential increases, re-

sulting in a higher dependency between
space (think back to Section 2.4!) corresponding to the Gaussian locations.
kernel is “infinitely dimensional”, as you will show in ? . So the Problem 4.1
Gaussian kernel already encodes a function class that we were not
able to model under the weight-space view of Bayesian linear re-
gression.

f (x) Figure 4.4: Functions sampled according
to a Gaussian process with a Gaussian
kernel and length scales h = 5 (left) and
h = 1 (right).

k(x− x′)
1.00

0.75

x 0.50

0.25
3. The Laplace kernel (also known as(exponential ker

. ∥x− x′∥ )
nel) is defined as

0.00
−2 0 2

k(x, x′; h) = exp − 2 . (4.15)
h x− x′

Figure 4.5: Gaussian kernel with length

As can be seen in Figure 4.7, samples from a GP with Laplace scales h = 1, h = 0.5, and h = 0.2.

kernel are non-smooth as opposed to the samples from a GP with
Gaussian kernel.

f (x) Figure 4.7: Functions sampled accord-
ing to a Gaussian process with a Laplace
kernel and length scales h = 10 000 (left)
and h = 10 (right).

k(x− x′)
1.00

0.75

x 0.50

0.25

4. The Matérn kernel trades the smoothness of the Gaussian and the 0.00
Laplace kernels. As such, it is frequently used in practice to model −2 0 2

x− x′
Figure 4.6: Laplace kernel with length
scales h = 1, h = 0.5, and h = 0.2.



64 probabilistic artificial intelligence

“real world” functions(that are relativel
√ )y smo(oth. It is defined

√ )as

. 21− ν
ν 2ν ∥x− x′∥

= 2 2ν ∥x− x′
k(x, x′

∥
; ν, h) K 2

Γ ν
(ν) h h

(4.16)

where Γ is the Gamma function, Kν the modified Bessel function
of the second kind, and h a length scale parameter. For ν = 1/2, the
Matérn kernel is equivalent to the Laplace kernel. For ν→ ∞, the
Matérn kernel is equivalent to the Gaussian kernel. The resulting
functions are ⌈ν⌉ − 1 times mean square differentiable.6 In partic- 6 Refer to Remark A.12 for the defini-
ular, GPs with a Gaussian kernel are infinitely many times mean tions of mean square continuity and dif-

ferentiability.
square differentiable whereas GPs with a Laplace kernel are mean
square continuous but not mean square differentiable.

4.3.2 Composing Kernels

Given two kernels k1 : X × X → R and k2 : X × X → R, they can
be composed to obtain a new kernel k : X × X → R in the following
ways:
• k .

(x, x′) = k1(x, x′) + k2(x, x′),
• k(x, x′ .

) = k1(x, x′) · k2(x, x′),
• k(x, x′ .

) = c · k1(x, x′) for any c > 0,
• k(x, x′ .

) = f (k1(x, x′)) for any polynomial f with positive coeffi-
cients or f = exp.

For example, the additive structure of a function f .
(x) = f1(x) + f2(x)

can be easily encoded in GP models. Suppose that f1 ∼ GP(µ1, k1)
and f2 ∼ GP(µ2, k2), then the distribution of the sum of those two
functions f = f1 + f2 ∼ GP(µ1 + µ2, k1 + k2) is another GP.7 7 We use f .

= f1 + f2 to denote the func-
tion f (·) = f1(·) + f2(·).

Whereas the addition of two kernels k1 and k2 can be thought of as
an OR operation (i.e., the kernel has high value if either k1 or k2 have
high value), the multiplication of k1 and k2 can be thought of as an
AND operation (i.e., the kernel has high value if both k1 and k2 have
high value). For example, the product of two linear kernels results in
functions which are quadratic.

As mentioned previously, the constant c of a scaled kernel function
k′(x, x′ .

) = c · k(x, x′) is generally called the output scale of a kernel,
and it scales the variance Var[ f (x)] = c · k(x, x) of the predictions f (x)
from GP(µ, k′).

Optional Readings
For a broader introduction to how kernels can be used and com-
bined to model certain classes of functions, read



gaussian processes 65

• chapter 2 of “Automatic model construction with Gaussian pro-
cesses” (Duvenaud, 2014) also known as the “kernel cookbook”,

• chapter 4 of “Gaussian processes for machine learning” (Williams
and Rasmussen, 2006).

4.3.3 Stationarity and Isotropy

Kernel functions are commonly classified according to two properties:

Definition 4.4 (Stationarity and isotropy). A kernel k : Rd ×Rd → R

is called
• stationary (or shift-invariant) if there exists a function k̃ such that

k̃(x− x′) = k(x, x′), and
• isotropic if there exists a function k̃ such that k̃(∥x− x′∥) = k(x, x′)

with ∥·∥ any norm.

Note that stationarity is a necessary condition for isotropy. In other
words, isotropy implies stationarity.

Example 4.5: Stationarity and isotropy of kernels

stationary isotropic

linear kernel no no

Gaussian kernel yes yes

k x, x′ .
( ) = exp(−∥x− x′∥2

M)
yes no ∥·∥ otes the Mahalanobis norm

where M is positive semi-definite M den
induced by matrix M

For x′ = x, stationarity implies that the kernel must only depend
on 0. In other words, a stationary kernel must depend on relative
locations only. This is clearly not the case for the linear kernel,
which depends on the absolute locations of x and x′. Therefore,
the linear kernel cannot be isotropic either.

For the Gaussian kernel, isotropy follows immediately from its
definition.

The last kernel is clearly stationary by definition, but not isotropic
for general matrices M. Note that for M = I it is indeed isotropic.

Stationarity encodes the idea that relative location matters more than
absolute location: the process “looks the same” no matter where we
shift it in the input space. This is often appropriate when we believe
the same statistical behavior holds across the entire domain (e.g., no
region is special). Isotropy goes one step further by requiring that



66 probabilistic artificial intelligence

the kernel depends only on the distance between points, so that all
directions in the space are treated equally. In other words, there is no
preferred orientation or axis. This is especially useful in settings where
we expect uniform behavior in every direction (as with the Gaussian
kernel). Such kernels are simpler to specify and interpret since we
only need a single “scale” (like a length scale) rather than multiple
parameters or directions.

4.3.4 Reproducing Kernel Hilbert Spaces

We can characterize the precise class of functions that can be modeled
by a Gaussian process with a given kernel function. This correspond-
ing function space is called a reproducing kernel Hilbert space (RKHS),
and we will discuss it briefly in this section.

Recall that Gaussian processes keep track of a posterior distribution
f | x1:n, y1:n over functions. We will in fact show later that the corre-
sponding MAP estimate f̂ corresponds to the solution to a regularized
optimization problem in the RKHS space of functions. This duality
is similar to the duality between the MAP estimate of Bayesian linear
regression and ridge regression we observed in Chapter 2. So what is
the reproducing kernel Hilbert space of a kernel function k?

Definition 4.6 (Reproducing kernel Hilbert space, RKHS). Given a ker-
nel k : X ×X → R, its corresponding reproducing kernel Hilbert space is
the space of fu{nctions f defined as }

n
Hk(X .

) = f (·) = ∑ αik(xi, ·) : n ∈N, xi ∈ X , αi ∈ R . (4.17)
i=1

The inner product of the RKHS is defined as

n n′
⟨ f , g⟩ .

k = ∑ ∑ αiα
′
jk(xi, x′j), (4.18)

i=1 j=1 √
where g(·) = ∑n′

j=1 α′jk(x′j, ·), and induces the norm ∥ f ∥k = ⟨ f , f ⟩k.
You can think of the norm as measuring the “smoothness” or “com-
plexity” of f . ? Problem 4.4 (2)

It is straightforward to check that for all x ∈ X , k(x, ·) ∈ Hk(X ).
Moreover, the RKHS inner product ⟨·, ·⟩k satisfies for all x ∈ X and
f ∈ Hk(X ) that f (x) = ⟨ f (·), k(x, ·)⟩k which is also known as the
reproducing property ? . That is, evaluations of RKHS functions f are Problem 4.4 (1)
inner products in Hk(X ) parameterized by the “feature map” k(x, ·).
The representer theorem (Schölkopf et al., 2001) characterizes the solu-
tion to regularized optimization problems in RKHSs:



gaussian processes 67

Theorem 4.7 (Representer theorem). ? Let k be a kernel and let λ > 0. Problem 4.5
For f ∈ Hk(X ) and training data {(xi, f (xi))}n

i=1, let L( f (x1), . . . , f (xn)) ∈
R∪ {∞} denote any loss function which depends on f only through its eval-
uation at the training points. Then, any minimizer

f̂ ∈ arg minL( f (x1), . . . , f (xn)) + λ ∥ f ∥2
k (4.19)

f∈Hk(X )

admits a representation of the form

n
f̂ (x) = α̂⊤kx,{x n = α̂ k(x, x

i}i ∑ i i) for some α̂ ∈ Rn. (4.20)
=1

i=1

This statement is remarkable: the solutions to general regularized op-
timization problems over the generally infinite-dimensional space of
functions Hk(X ) can be represented as a linear combination of the
kernel functions evaluated at the training points. The representer the-
orem can be used to show that the MAP estimate of a Gaussian process
corresponds to the solution of a regularized linear regression problem
in the RKHS of the kernel function, namely, ? Problem 4.6

f̂ .
= arg min− 1

log p(y1:n | x1:n, f ) + ∥ f ∥2 . (4.21)
f∈Hk(X ) 2 k

Here, the first term corresponds to the likelihood, measuring the “qual-
ity of fit”. The regularization term limits the “complexity” of f̂ . Reg-
ularization is necessary to prevent overfitting since in an expressive
RKHSs, there may be many functions that interpolate the training data
perfectly. This shows the close link between Gaussian process regres-
sion and Bayesian linear regression, with the kernel function k gener-
alizing the inner product of feature maps to feature spaces of possi-
bly “infinite dimensionality”. Because solutions can be represented as
linear combinations of kernel evaluations at the training points, Gaus-
sian processes remain computationally tractable even though they can
model functions over “infinite-dimensional” feature spaces.

4.4 Model Selection

We have not yet discussed how to pick the hyperparameters θ (e.g.,
parameters of kernels). A common technique in supervised learning
is to select hyperparameters θ, such that the resulting function esti-
mate f̂θ leads to the most accurate predictions on hold-out validation
data. After reviewing this approach, we contrast it with a probabilistic
approach to model selection, which avoids using point estimates of f̂θ

and rather utilizes the full posterior.



68 probabilistic artificial intelligence

4.4.1 Optimizing Validation Set Performance

A common approach to model selection is to split our data D into
separate training set Dtrain .

= {(xtrain
i , ytrain

i )}n
i=1 and validation sets

Dval .
= {(xval

i , yval
i )}m

i=1. We then optimize the model for a parameter
candidate θj using the training set. This is usually done by picking a
point estimate (like the MAP estimate),

f̂ .
j = arg max p( f | xtrain tra n

1:n , y i
1:n ). (4.22)

f

Then, we score θj according to the performance of f̂ j on the validation
set,

.
θ̂ = arg max p(yval val

1:m | x1:m, f̂ j). (4.23)
θj

This ensures that f̂ j does not depend on Dval.

Remark 4.8: Approximating population risk
Why is it useful to separate the data into a training and a vali-
dation set? Recall from Appendix A.3.5 that minimizing the em-
pirical risk without separating training and validation data may
lead to overfitting as both the loss and f̂ j depend on the same
data D. In contrast, using independent training and validation
sets, f̂ j does not depend on Dval, and we have that

1 m [ ]
∑ ℓ(yval l

i | xva
i , f̂ j) ≈ E(x,y)∼P ℓ(y | x, f̂ j) , (4.24)

m i=1

using Monte Carlo sampling.8 In words, for reasonably large m, 8 We generally assume D i∼idP , in par-
minimizing the empirical risk as we do in Equation (4.23) approx- ticular, we assume that the individual

samples of the data are i.i.d.. Recall
imates minimizing the population risk. that in this setting, Hoeffding’s inequal-

ity (A.41) can be used to gauge how
large m should be.

While this approach often is quite effective at preventing overfitting
as compared to using the same data for training and picking θ̂, it still
collapses the uncertainty in f into a point estimate. Can we do better?

4.4.2 Maximizing the Marginal Likelihood

We have already seen for Bayesian linear regression, that picking a
point estimate loses a lot of information. Instead of optimizing the
effects of θ for a specific point estimate f̂ of the model f , maximizing
the marginal likelihood optimizes the effects of θ across all realizations
of f . In this approach, we obtain our hyperparameter estimate via

.
θ̂MLE = arg max p(y1:n | x1:n, θ) (4.25) using the definition of marginal

θ likelihood in Bayes’ rule (1.45)



gaussian processes 69

∫
= arg max ∫ p(y1:n, f | x1:n, θ) d f by conditioning on f using the sum rule

θ (1.7)

= arg max p(y1:n | x1:n, f , θ)p( f | θ) d f . (4.26) using the product rule (1.11)
θ

Remarkably, this approach typically avoids overfitting even though we
do not use a separate training and validation set. The following ta-
ble provides an intuitive argument for why maximizing the marginal
likelihood is a good strategy.

likelihood prior Table 4.1: The table gives an intuitive ex-
planation of effects of parameter choices

“underfit” model θ on the marginal likelihood. Note that
small for “almost all” f large words in quotation marks refer to in-

(too simple θ) tuitive quantities, as we have infinitely
many realizations of f .

“overfit” model large for “few” f
small

(too complex θ) small for “most” f

“just right” moderate for “many” f moderate simple

intermediate
For an “underfit” model, the likelihood is mostly small as the data

complex
cannot be well described, while the prior is large as there are “fewer”
functions to choose from. For an “overfit” model, the likelihood is
large for “some” functions (which would be picked if we were only all possible data sets

minimizing the training error and not doing cross validation) but small Figure 4.8: A schematic illustration of
for “most” functions. The prior is small, as the probability mass has the marginal likelihood of a simple, in-

to be distributed among “more” functions. Thus, in both cases, one termediate, and complex model across
all possible data sets.

term in the product will be small. Hence, maximizing the marginal
likelihood naturally encourages trading between a large likelihood and
a large prior.

In the context of Gaussian process regression, recall from Equation (4.3)
that

y1:n | x1:n, θ ∼ N (0, K f ,θ + σ2
n I) (4.27)

where K f ,θ denotes the kernel matrix at the inputs x1:n depending on
the kernel function parameterized by θ. We write K .

y,θ = K f ,θ + σ2
n I.

Continuing from Equation (4.25), we obtain

θ̂MLE = arg maxN (y; 0, Ky,θ)
θ

1 1 ) n
= arg min y⊤K−1 (

y,θy + log det K ,θ + log 2π (4.28) taking the negative logarithm
θ 2 2 y

) 2
1 1 (

= arg min y⊤K−1
θy + log det K (4.29) the last term is independent of θ

θ 2 y, 2 y,θ

The first term of the optimization objective describes the “goodness of
fit” (i.e., the “alignment” of y with Ky,θ). The second term character-
izes the “volume” of the model class. Thus, this optimization naturally
trades the aforementioned objectives.

marginal likelihood



70 probabilistic artificial intelligence

Marginal likelihood maximization is an empirical Bayes method. Often
it is simply referred to as empirical Bayes. It also has the nice property
that the gradient of its objective (the MLL loss) can be expressed in
closed-from ? , ( ) Problem 4.7

∂ 1
log p(y

∂θ 1:n | x1:n, θ) = tr (αα⊤ − −1 ∂K
K y,θ

j 2 y,θ) (4.30) 1.0
∂θj

0.5

where .
α = K−1

y,θy and tr(M) is the trace of a matrix M. This optimiza-
0.0

tion problem is, in general, non-convex. Figure 4.10 gives an example
of two local optima according to empirical Bayes. 0 100 200

# of iterations
Taking a step back, observe that taking a probabilistic perspective on
model selection naturally led us to consider all realizations of our Figure 4.9: An example of model selec-

tion by maximizing the log likelihood
model f instead of using point estimates. However, we are still us- (without hyperpriors) using a linear,
ing point estimates for our model parameters θ. Continuing on our quadratic, Laplace, Matérn (ν = 3/2),
probabilistic adventure, we could place a prior p(θ) on them too.9 We and Gaussian kernel, respectively. They

are used to learn the function
could use it to obtain the MAP estimate (still a point estimate!) which
adds an additional regularization term x 7→ sin(x)

+ ε, ε ∼ N (0, 0.01)
x

. using SGD with learning rate 0.1.
θ̂MAP = arg max p(θ | x1:n, y1:n) (4.31) 9 Such a prior is called hyperprior.

θ

= arg min− log p(θ)− log p(y1:n | x1:n, θ). (4.32) using Bayes’ rule (1.45) and then taking
θ the negative logarithm

An alternative approach is to consider the full posterior distribution
over parameters θ. The resulting predictive distribution is, however,
intractable, ∫ ∫
p(y⋆ | x⋆, x1:n, y1:n) = p(y⋆ | x⋆, f ) · p( f | x1:n, y1:n, θ) · p(θ) d f dθ.

(4.33)

Recall that as the mode of Gaussians coincides with their mean, the
MAP estimate corresponds to the mean of the predictive posterior.

As a final note, observe that in principle, there is nothing stopping us
from descending deeper in the probabilistic hierarchy. The prior on the
model parameters θ is likely to have parameters too. Ultimately, we
need to break out of this hierarchy of dependencies and choose a prior.

4.5 Approximations

To learn a Gaussian process(, we need to invert n× n matrices, hence
the computational cost is O n3). Compare this to Bayesian( linear re-
gression which allows us to learn a regression model in O nd2) time
(even online) where d is the feature dimension. It is therefore natural
to look for ways of approximating a Gaussian process.

MLL loss



gaussian processes 71

101 Figure 4.10: The top plot shows contour
lines of an empirical Bayes with two lo-
cal optima. The bottom two plots show
the Gaussian processes corresponding
to the two optimal models. The left
model with smaller lengthscale is chosen
within a more flexible class of models,
while the right model explains more ob-
servations through noise. Adapted from

100 figure 5.5 of “Gaussian processes for
machine learning” (Williams and Ras-
mussen, 2006).

10−1

100 101

lengthscale h

2 2

1 1

0 0

−1 −1

−2 −2

−5.0 −2.5 0.0 2.5 5.0 −5.0 −2.5 0.0 2.5 5.0
x x

noise standard deviation σ
f (x n

)



72 probabilistic artificial intelligence

4.5.1 Local Methods

Recall that during forward sampling, we had to condition on a larger
and larger number of previous samples. When sampling at a loca-
tion x, a very simple approximation is to only condition on those sam-
ples x′ that are “close” (where |k(x, x′)| ≥ τ for some τ > 0). Essen-
tially, this method “cuts off the tails” of the kernel function k. However,
τ has to be chosen carefully as if τ is chosen too large, samples become
essentially independent.

This is one example of a sparse approximation of a Gaussian process. We
will discuss more advanced sparse approximations known as “induc-
ing point methods” in Section 4.5.3.

4.5.2 Kernel Function Approximation

Another method is to approximate the kernel function directly. The
idea is to construct a “low-dimensional” feature map ϕ : Rd → Rm

that approximates the kernel,

k(x, x′) ≈ ϕ(x)⊤ϕ(x′). (4.34)

Then, we can( apply Bayesian linear regression, resulting in a time com-
plexity of O nm2 + m3).
One example of this approach are random Fourier features, which we
will discuss in the following.

Remark 4.9: Fourier transform
Im

First, let us remind ourselves of Fourier transformations. The
Fourier transform is a method of decomposing frequencies into i eiφ

their individual components.

Recall Euler’s formula which states that for any x ∈ R,
sin φ

φ
eix = cos x + i sin x (4.35) 0

cos φ

where i is the imaginary unit of complex numbers. The formula is
illustrated in Figure 4.11. Note that e−i2πx corresponds to rotating 0 1
clockwise around the unit circle in R2 — completing a rotation Re
whenever x ∈ R reaches the next natural number. Figure 4.11: Illustration of Euler’s for-

mula. It can be seen that eiφ corresponds
We can scale x by a frequency ξ: e−i2πξx. If x ∈ Rd, we can also to a (counter-clockwise) rotation on the
scale each component j of x by a different frequency ξ(j). Multi- unit circle as φ varies from 0 to 2π.

plying a function f : Rd → R with the rotation around the unit
circle with given frequencies ξ, yields a quantity that describes the



gaussian processes 73

f (x)
amplitude of the frequencie∫s ξ,

1

. ⊤
f̂ (ξ) = f (x)e−i2πξ x dx. (4.36)

Rd

f̂ is called the Fourier transform of f . f is called the inverse Fourier
transform of f̂ , and can be co∫mputed using

0

f (x) = f̂ (ξ)ei2πξ⊤x dξ. (4.37) −1 1
Rd x

It is common to write .
ω = 2πξ. See Figure 4.12 for an example. f̂ (ω)

2
Refer to “But what is the Fourier Transform? A visual introduc-
tion” (Sanderson, 2018) for a visual introduction.

Because a stationary kernel k : Rd ×Rd → R can be interpreted as a 0
function in one variable, it has an associated Fourier transform which
we denote by p(ω). That is, ∫ −π π

ω

k(x− x′
⊤

) = p(ω)eiω (x−x′) dω. (4.38)
Rd Figure 4.12: The Fourier transform of a

rectangular puls{e,

Fact 4.10 (Bochner’s theorem). A continuous stationary kernel on Rd is
f . 1 x ∈ [−1, 1]
(x) =

positive definite if and only if its Fourier transform p(ω) is non-negative. 0 otherwise,

Bochner’s theorem implies that when a continuous and stationary is given b∫y1 1 ( )
kernel is positive definite and scaled appropriately, its Fourier trans- f̂ (ω) = e−iωx dx = eiω − e−iω

−1 iω
form p(ω) is a proper probability distribution. In this case, p(ω) is 2 sin(ω)
called the spectral density of the kernel k. = .

ω

Remark 4.11: Eigenvalue spectrum of stationary kernels
When a kernel k is stationary (i.e., a univariate function of x− x′),
its eigenfunctions (with respect to the usual Lebesgue measure)
turn out to be the complex exponentials exp(iω⊤(x − x′)). In
simpler terms, you can think of these exponentials as “building
blocks” at different frequencies ω. The spectral density p(ω) as-
sociated with the kernel tells you how strongly each frequency
contributes, i.e., how large the corresponding eigenvalue is.

A key insight of this analysis is that the rate at which these magni-
tudes p(ω) decay with increasing frequency ω reveals the smooth-
ness of the processes governed by the kernel. If a kernel allocates
more “power” to high frequencies (meaning the spectral density
decays slowly), the resulting processes will appear “rougher”.
Conversely, if high-frequency components are suppressed, the pro-
cess will appear “smoother”.



74 probabilistic artificial intelligence

For an in-depth introduction to the eigenfunction analysis of ker-
nels, refer to section 4.3 of “Gaussian processes for machine learn-
ing” (Williams and Rasmussen, 2006).

Example 4.12: Spectral density of the Gaussian kernel
The Gaussian kerne∫l with length scale h has the spectral density

′
p(ω) = k(x− x′ ω⊤; h)e−i (x−x ) d(x− x′) using the definition of the Fourier

∫Rd ( ) transform (4.36)
∥x∥2

= exp − 2

(2 − iω⊤x dx using the definition of the Gaussian
Rd 2h ) kernel (4.14)

d
(2h2π) /2 ∥ω∥2

= exp −h2 2 . (4.39)
2

The key idea is ∫now to interpret the kernel as an expectation,

k(x− x′ iω⊤) = p([ω)e (x−x′) dω from Equation (4.38)
Rd

[ ⊤ ]
= E ∼ iω (x−x′)

ω p e ] by the definition of expectation (1.19)

= E ′
ω∼p cos(ω⊤x−ω⊤x ) + i sin(ω⊤x−ω⊤x′) . using Euler’s formula (4.35)

Observe th[at as both k and p are real, convergence of the integral im-
plies Eω∼p sin(ω⊤x−[ ω⊤

]
x′) = 0. Hen]ce,

= Eω∼p cos(ω⊤x−ω[⊤x′) ]
= Eω∼pEb∼Unif([0,2π]) [cos((ω⊤x + b)− (ω⊤x′ + b)) expanding with b− b

= Eω∼pEb∼Unif([0,2π]) cos(ω⊤x + b) cos(ω⊤x′ + b) ] using the angle subtraction identity,

[+ sin(ω⊤x + b) sin(ω⊤x′ ] cos(α− β) = cos α cos β + sin α sin β
+ b)

= Eω∼pEb∼Unif([0,2π][) 2 cos(ω⊤x + b)]cos(ω⊤x′ + b) using

= Eω∼p,b∼Unif([0,2π]) zω,b(x) · zω,b(x′) (4.40) Eb[cos(α + b) cos(β + b)]
= Eb[sin(α + b) sin(β + b)]

where z . √
ω,b(x) = 2 cos(ω⊤x + b),

for b ∼ Unif([0, 2π])

≈ 1 m
z

m ∑ ω(i),b(i)(x) · zω(i),b(i)(x′) (4.41) using Monte Carlo sampling to estimate
i=1 the expectation, see Example A.6

for independent samples ω(i) i∼id p and b(i) i∼id Unif([0, 2π]),

= z(x)⊤z(x′) (4.42)

where the (randomized) feature map of random Fourier features is

z .
(x) = √1

[z . . . , z )]⊤. (4.43)
m ω(1),b(1)(x), ω(m),b(m)(x



gaussian processes 75

Intuitively, each component of the feature map z(x) projects x onto a f (x)
random direction ω drawn from the (inverse) Fourier transform p(ω) 4

of k(x− x′), and wraps this line onto the unit circle in R2. After trans-
forming two points x and x′ in this way, their inner pr 2

√oduct is an unbi-
ased estimator of k(x− x′). The mapping zω,b(x) = 2 cos(ω⊤x + b)
additionally rotates the circle by a random amount b and projects the 0

points onto the interval [0, 1].
2 −5.0 −2.5 0.0 2.5 5.0

Rahimi et al. (2007) show that Bayesian linear regression with the fea-
ture map z approximates Gaussian processes with a stationary kernel: 1

Theorem 4.13 (Uniform convergence of Fourier features). Suppose M 0

is a compact subset of Rd with diameter diam(M). Then for a stationary −1

kernel k, the rand(om Fouri∣er features z, and any ϵ >∣ 0 it)holds that −5.0 −2.5 0.0 2.5 5.0
x

P sup ∣∣z(x)⊤z(x′)− k(x− x′ ∣)∣ ≥ ϵ (4.44) Figure 4.13: Example of random Fourier
x,x′∈(M ) (

mϵ2 ) features with where the number of fea-
tures m is 5 (top) and 10 (bottom), re-

≤ di m M 2
28 σp a ( )

exp − spectively. The noise-free true function
ϵ 8(d + 2) is shown in black and the mean of the

Gaussian process is shown in blue.
where σ2 . [

p = Eω∼p ω⊤
]

ω is the second moment of p, m is the dimension
of z(x), and d is the dimension of x. ? Problem 4.8

Note that the error probability decays exponentially fast in the dimen- f (x)
sion of the Fourier feature space.

2

4.5.3 Data Sampling 1

Another natural approach is to only consider a (random) subset of the 0
training data during learning. The naive approach is to subsample
uniformly at random. Not very surprisingly, we can do much better. −5 0 5

x
One subsampling method is the inducing points method (Quinonero-
Candela and Rasmussen, 2005). The idea is to summarize the data Figure 4.14: Inducing points u are shown

as vertical dotted red lines. The noise-
around so-called inducing points.10 For now, let us consider an arbi- free true function is shown in black and
trary set of inducing points, the mean of the Gaussian process is

shown in blue. Observe that the true
U .

= {x1, . . . , xk}. function is approximated “well” around
the inducing points.
10 The inducing points can be treated as

Then, the original Gaussian process can be recovered using marginal- hyperparameters.
ization, ∫ ∫

p( f ⋆, f ) = p( f ⋆, f , u) du = p( f ⋆, f | u)p(u) du, (4.45) using the sum rule (1.7) and product
Rk Rk rule (1.11)

where f .
= [ f (x1) · · · f (xn)]⊤ and f ⋆ .

= f (x⋆) at some evaluation
point x⋆ ∈ X . We use u .

= [ f (x1) · · · f (xk)]
⊤ ∈ Rk to denote the pre-

dictions of the model at the inducing points U. Due to the marginaliza-
tion property of Gaussian processes (4.1), we have that u ∼ N (0, KUU).



76 probabilistic artificial intelligence

The key idea is to approximate the joint prior, assuming that f ⋆ and f
are conditionally independ∫ent given u,

p( f ⋆, f ) ≈ p( f ⋆ | u)p( f | u)p(u) du. (4.46)
Rk

Here, p( f | u) and p( f ⋆ | u) are commonly called the training condi-
tional and the testing conditional, respectively. Still denoting the obser-
vations by A {x . , x .

= 1, . . n} and defining ⋆ = {x⋆}, we know, using
the closed-form expression for conditional Gaussians (1.53),

p( f | u) ∼ N ( f ; K 1
AUK−UUu, KAA −QAA), (4.47a)

p( f ⋆ | u) ∼ N ( f ⋆; K −1
⋆UKUUu, K⋆⋆ −Q⋆⋆) (4.47b)

where Q . −1
ab = KaUKUUKUb. Intuitively, KAA represents the prior co-

variance and QAA represents the covariance “explained” by the induc-
ing points.11 11 For more details, refer to section 2

of “A unifying view of sparse ap-
Computing the full covariance matrix is expensive. In the following, proximate Gaussian process regression”

(Quinonero-Candela and Rasmussen,
we mention two approximations to the covariance of the training con-

2005).
ditional (and testing conditional).

f (x)
Example 4.14: Subset of regressors 3

The subset of regressors (SoR) approximation is defined as 2

qSoR( f | u . 1
) = N ( f ; KAUK−1

UUu, 0), (4.48a)
0

q ⋆
SoR( f | u .

) = N ( f ⋆; K 1
⋆UK−UUu, 0). (4.48b) −1

Comparing to Equation (4.47), SoR simply forgets about all vari- −5.0 −2.5 0.0 2.5 5.0
2

ance and covariance.
0

Example 4.15: Fully independent training conditional
−2

The fully independent training conditional (FITC) approximation is −5.0 −2.5 0.0 2.5 5.0

defined as x

. Figure 4.15: Comparison of SoR (top)
qFITC( f | u) = N ( f ; KAUK−1

UUu, diag{KAA −QAA}), (4.49a) and FITC (bottom). The inducing points
q ⋆ . ( u are shown as vertical dotted red lines.

FITC( f | u) = N ( f ⋆; K 1
⋆UK−UUu, diag{K⋆⋆ −Q⋆⋆}). 4.49b)

The noise-free true function is shown in
black and the mean of the Gaussian pro-

In contrast to SoR, FITC keeps track of the variances but forgets cess is shown in blue.
about the covariance.

The computational cost for inducing point methods SoR and FITC is
dominated by the cost of inverting KUU . Thus, the time complexity is
cubic in the number of inducing points, but only linear in the number
of data points.



gaussian processes 77

Discussion

This chapter introduced Gaussian processes which leverage the function-
space view on linear regression to perform exact probabilistic inference
with flexible, nonlinear models. A Gaussian process can be seen as a
non-parametric model since it can represent an infinite-dimensional
parameter space. Instead, as we saw with the representer theorem,
such non-parametric (i.e., “function-space”) models are directly rep-
resented as functions of the data points. While this can make these
models more flexible than a simple linear parametric model in input
space, it also makes them computationally expensive as the number of
data points grows. To this end, we discussed several ways of approxi-
mating Gaussian processes.

Nevertheless, for today’s internet-scale datasets, modern machine learn-
ing typically relies on large parametric models that learn features from
data. These models can effectively amortize the cost of inference dur-
ing training by encoding information into a fixed set of parameters. In
the following chapters, we will start to explore approaches to approx-
imate probabilistic inference that can be applied to such models.

Problems

4.1. Feature space of Gaussian kernel.
1. Show that the univariate Gaussian kernel with length scale h = 1

implicitly defines afeatur

 e space with basis vectors

ϕ )

 0(x 
ϕ(x) = ϕ1(x) i h ϕj

.  1 2
w t (x) = √ e−

x
2 xj.

. j!

.

Hint: Use the Taylor series expansion of the exponential function, ex = ∑∞ xj
j=0 j! .

2. Note that the vector ϕ(x) is ∞-dimensional. Thus, taking the
function-space view allows us to perform regression in an infinite-
dimensional feature space. What is the effective dimension when
regressing n univariate data points with a Gaussian kernel?

4.2. Kernels on the circle.

Consider a dataset {(xi, yi)}n
i=1 with labels yi ∈ R and inputs xi which

lie on the unit circle S ⊂ R2. In particular, any element of S can
be identified with points in R2 of form (cos(θ), sin(θ)) or with the
respective angles θ ∈ [0, 2π).

You now want to use GP regression to learn an unknown mapping
from S to R using this dataset. Thus, you need a valid kernel k :



78 probabilistic artificial intelligence

S× S → R. First, we look at kernels k which can be understood as
analogous to the Gaussian kernel.
1. You think of the “extrinsic” ke(rnel ke : S× S→ R)defined by

. ∥x(θ)− x(θ′)∥2
ke(θ, θ′) = exp − 2

2κ2 ,

where x .
(θ) = (cos(θ), sin(θ)). Is ke positive semi-definite for all

values of κ > 0?
2. Then, you think of an “intrinsic” k(ernel ki : S×

. d(θ, θ′)2) S→ R defined by

ki(θ, θ′) = exp −
2κ2

where d .
(θ, θ′) = min(|θ − θ′|, |θ − θ′ − 2π|, |θ − θ′ + 2π|) is the

standard arc length distance on the circle S.
You would now like to test whether this kernel is positive semi-
definite. We pick κ = 2 and compute the kernel matrix K for the
points corresponding to the angles {0, π/2, π, 3π/2}. This kernel
matrix K has eigenvectors (1, 1, 1, 1) and (−1, 1,−1, 1).
Now compute the eigenvalue corresponding to the eigenvector
(−1, 1,−1, 1).

3. Is ki positive semi-definite for κ = 2?
4. A mathematician friend of yours suggests to you yet another kernel

for points on the circle S, called the heat kernel. The kernel itself has
a complicated expression(but can be accurately approx)imated by

. 1 L−1
kh(θ, θ′

κ2 2
) = 1 + e−

C ∑ 2 l 2 cos(l(θ − θ′)) ,
κ l=1

where L ∈ N controls the quality of approximation and Cκ > 0 is
a normalizing constant that depends only on κ.
Is kh is positive semi-definite for all values of κ > 0 and L ∈N?
Hint: Recall that cos(a− b) = cos(a) cos(b) + sin(a) sin(b).

4.3. A Kalman filter as a Gaussian process.

Next we will show that the Kalman filter from Example 3.4 can be seen
as a Gaussian process. To this end, we define

f : N0 → R, t 7→ Xt. (4.50)

Assuming that X0 ∼ N (0, .
σ2

0 ) and Xt+1 = Xt + εt with independent
noise εt ∼ N (0, σ2

x), show that

f ∼ GP(0, kKF) where (4.51)
k .

KF(t, t′) = σ2
0 + σ2

x min{t, t′}. (4.52)

This particular kernel k(t, t′ .
) = min{t, t′} but over the continuous-time

domain defines the Wiener process (also known as Brownian motion).



gaussian processes 79

4.4. Reproducing property and RKHS norm.
1. Derive the reproducing property.

Hint: Use k(x, x′) = ⟨k(x, ·), k(x′, ·)⟩k.
2. Show that the RKHS norm ∥·∥k is a measure of smoothness by

proving that for any f ∈ Hk(X ) and x, y ∈ X it holds that

| f (x)− f (y)| ≤ ∥ f ∥k ∥k(x, ·)− k(y, ·)∥k .

4.5. Representer theorem.

With this, we can now derive the representer theorem (4.20).

Hint: Recall
1. the reproducing property f (x) = ⟨ f , k(x, ·)⟩k with k(x, ·) ∈ Hk(X )

which holds for all f ∈ Hk(X ) and x ∈ Hk(X ), and
2. that the norm after projection is smaller or equal the norm before projec-

tion.
Then decompose f into parallel and orthogonal components with respect to
span{k(x1, ·), . . . , k(xn, ·)}.
4.6. MAP estimate of Gaussian processes.

Let us denote by A = {x1, . . . , xn} the set of training points. We will
now show that the MAP estimate of GP regression corresponds to
the solution of the regularized linear regression problem in the RKHS
stated in Equation (4.21):

f̂ .
= arg min− 1

log p(y1:n | x1:n, f ) + ∥ f ∥2
k .

f∈Hk(X ) 2

In the following, we abbreviate K = KAA. We will also assume that
the GP has a zero mean function.
1. Show that Equation (4.21) is equivalent to

.
α̂ = arg min ∥y− Kα∥2

2 + λ ∥α∥2
K (4.53)

α∈Rn

for some λ > 0 which is also known as kernel ridge regression. De-
termine λ.

2. Show that Equation (4.53) with the λ determined in (1) is equiva-
lent to the MAP estimate of GP regression.
Hint: Recall from Equation (4.6) that the MAP estimate at a point x⋆ is
E[ f ⋆ | x⋆, X, y] = k⊤x⋆ ,A(K + σ2

n I)−1y.

4.7. Gradient of the marginal likelihood.

In this exercise, we derive Equation (4.30).

Recall that we were considering a dataset (X, y) of noise-perturbed
evaluations yi = f (xi) + εi where εi ∼ N (0, σ2

n) and f is an unknown



80 probabilistic artificial intelligence

function. We make the hypothesis f ∼ GP(0, kθ) with a zero mean
function and the covariance function kθ. We are interested in finding
the hyperparameters θ that maximize the marginal likelihood p(y | X, θ).
1. Derive Equation (4.30).

Hint: You can use the following identities:
(a) for any invertible matrix M,

∂
M−1 = −M−1 ∂M

M−1 and (4.54)
∂θj ∂θj

(b) for any symmetric positive definite ma(trix M, )
∂ ∂M

log det(M) = tr M−1 . (4.55)
∂θj ∂θj

2. Assume now that the covariance function for the noisy targets (i.e.,
including the noise contribution) can be expressed as

ky,θ(x, x′) = θ0k̃(x, x′)

where k̃ is a valid kernel independent of θ0.12 12 That is, Ky,θ(i, j) = ky,θ(xi , xj).
Show that ∂

∂θ log p(y | X, θ) = 0 admits a closed-form solution for
0

θ0 which we denote by θ⋆0 .
3. How should the optimal parameter θ⋆0 be scaled if we scale the

labels y by a scalar s?

4.8. Uniform convergence of Fourier features.

In this exercise, we will prove Theorem 4.13.

Let s(x, x′ .
) = z(x .

)⊤z(x′) and f (x, x′) = s(x, x′)− k(x, x′). Observe that
both functions are shift invariant, and we will therefore denote them
as univariate functions with argument ∆ ≡ x− x′ ∈ M∆. Notice that
our goal is to bound the probability of the event sup∆(∈M |)f (∆)| ≥ ϵ.

∆

1. Show that for all ∆ ∈ M∆, P(| f (∆)| ≥ ϵ) ≤ 2 exp −mϵ2
4 .

What we have derived in (1) is known as a pointwise convergence guar-
antee. However, we are interested in bounding the uniform convergence
over the compact setM∆.

Our approach will be to “cover” the compact setM∆ using T balls of
radius r whose centers we denote by {∆i}T

i=1. It can be shown that
this is possible for some T ≤ (4 diam(M)/r)d. It can furthermore be
shown that

∀ ϵ
i. | f (∆i)|

ϵ
< and ∥∇ f (∆⋆)∥

2 2 < =⇒ sup | f (∆)| < ϵ
2r ∆∈M∆

where ∆⋆ = arg max∆∈M ∥∇ f (∆)∥ .
∆ 2



gaussian processes 81

( ) ( )
2. Prove P ∥∇ f r

(∆⋆)∥2 ≥ ϵ 2 σ 2
p

2r ≤ ϵ .
Hint: Re(call that the random Fourier feature approximation is unbiased,
i.e., E[s(∆⋃)] = k(∆). ) ( )

3. Prove P T
i=1 | f (∆i)| ≥ ϵ

2 ≤ 2T exp −mϵ2
16 .

4. Combine the results from (2) and (3) to prove Theorem 4.13.
Hint: You may use that

d
(a) αr−d + βr2 2 1

= 2β d+2 α d+2 for r = (α/β) d+2 and
(b) σpdiam(M)

ϵ ≥ 1.
5. Show that for the Gaussian kernel (4.14), σ2

p = d
h2 .

Hint: First show σ2
p = −tr(H∆k(0)).

4.9. Subset of regressors.
1. Using an SoR approx(im[atio]n, p[rove the follo]w)ing:

qSoR( f , f ⋆) = N f Q Q
, AA A⋆

f ⋆
; 0 (4.56)

Q⋆A Q⋆⋆

q ⋆
SoR( f | y) = N ( f ⋆; Q⋆AQ̃−1

AAy, Q⋆⋆ −Q⋆AQ̃−1
AAQA⋆) (4.57)

where Q̃ .
ab = Qab + σ2

n.
2. Derive that the resulting model is a degenerate Gaussian process

with covariance function

k .
SoR(x, x′) = k⊤x,UK−1

UUkx′ ,U . (4.58)






5
Variational Inference

We have seen how to perform (efficient) probabilistic inference with
Gaussians, exploiting their closed-form formulas for marginal and con-
ditional distributions. But what if we work with other distributions?

In this and the following chapter, we will discuss two methods of ap-
proximate inference. We begin by discussing variational (probabilistic)
inference, which aims to find a good approximation of the posterior
distribution from which it is easy to sample. In Chapter 6, we discuss
Markov chain Monte Carlo methods, which approximate the sampling
from the posterior distribution directly.

The fundamental idea behind variational inference is to approximate
the true posterior distribution using a “simpler” posterior that is as
close as possible to the true posterior:

1
p(θ | x1:n, y1:n) = p(θ, y

Z 1:n | x1:n) ≈ q(θ | .
λ) = qλ(θ) (5.1)

where λ represents the parameters of the variational posterior qλ, also
called variational parameters. In doing so, variational inference reduces
probabilistic inference — where the fundamental difficulty lies in solv-
ing high-dimensional integrals — to an optimization problem. Opti-
mizing (stochastic) objectives is a well-understood problem with effi-
cient algorithms that perform well in practice.1 1 We provide an overview of first-order

methods such as stochastic gradient de-
scent in Appendix A.4.

5.1 Laplace Approximation

Before introducing a general framework of variational inference, we
discuss a simpler method of approximate inference known as Laplace’s
method. This method was proposed as a method of approximating in-
tegrals as early as 1774 by Pierre-Simon Laplace. The idea is to use
a Gaussian approximation (that is, a second-order Taylor approxima-



84 probabilistic artificial intelligence

tion) of the posterior distribution around its mode. Let
.

ψ(θ) = log p(θ | x1:n, y1:n) (5.2)

denote the log-posterior. Then, using a second-order Taylor approx-
imation (A.53) around the mode θ̂ of ψ (i.e., the MAP estimate), we
obtain the approximation ψ̂ which is accurate for θ ≈ θ̂:

.
ψ(θ) ≈ ψ̂(θ) = ψ(θ̂) + (θ− θ̂)⊤∇ 1

ψ(θ̂) + (θ− θ̂)⊤Hψ(θ̂)(θ− θ̂)
2

1
= ψ(θ̂) + (θ− θ̂)⊤Hψ(θ̂)(θ− θ̂). (5.3) using ∇ψ(θ̂) = 0

2

Compare this expression to the log-PDF of a Gaussian:

logN (θ; θ̂, Λ−1) = −1
(θ− θ̂)⊤Λ(θ− θ̂) + const. (5.4)

2

Since ψ(θ̂) is constant with respect to θ,

ψ̂(θ) = logN (θ; θ̂,−Hψ(θ̂)
−1) + const. (5.5)

The Laplace approximation q of p is

q .
(θ) = N (θ; θ̂, Λ−1) ∝ exp(ψ̂(θ)) where (5.6a)

. ∣
Λ = −Hψ(θ̂) = −Hθ log p(θ | x1:n, y1:n)∣ . (5.6b)

θ=θ̂

Recall that for this approximation to be well-defined, the covariance
matrix Λ−1 (or equivalently the precision matrix Λ) needs to be sym-
metric and positive semi-definite. Let us verify that this is indeed the
case for sufficiently smooth ψ.2 In this case, the Hessian Λ is sym- 2 ψ being twice continuously differen-
metric since the order of differentiation does not matter. Moreover, by tiable around θ̂ is sufficient.

the second-order optimality condition, Hψ(θ̂) is negative semi-definite
since θ̂ is a maximum of ψ, which implies that Λ is positive semi-
definite.

Example 5.1: Laplace approximation of a Gaussian
Consider approximating the Gaussian density p(θ) = N (θ; µ, Σ)
using a Laplace approximation.

We know that the mode of p is µ, which we can verify by comput-
ing the gradient,

∇θ log p(θ) = −1
(2Σ−1θ− 2Σ−1 !

µ) = 0 ⇐⇒ θ = µ. (5.7)
2

For the Hessian of log p(θ), we get

Hθ log p(θ) = (Dθ(Σ
−1µ− Σ−1θ))⊤ = −(Σ−1)⊤ = −Σ−1. (5.8) using (A−1)⊤ = (A⊤)−1 and symmetry

of Σ



variational inference 85

0.8
We see that the Laplace approximation of a Gaussian p(θ) is ex-
act, which should not come as a surprise since the second-order 0.6 q

Taylor approximation of log p(θ) is exact for Gaussians. 0.4 p̂

0.2 p
The Laplace approximation matches the shape of the true posterior
around its mode but may not represent it accurately elsewhere — often 0.0

leading to extremely overconfident predictions. An example is given −2 0 2 4 6
x

in Figure 5.1. Nevertheless, the Laplace approximation has some de-
sirable properties such as being relatively easy to apply in a post-hoc Figure 5.1: The Laplace approximation

manner, that is, after having already computed the MAP estimate. It q greedily selects the mode of the true
posterior distribution p and matches the

preserves the MAP point estimate as its mean and just “adds” a lit- curvature around the mode p̂. As shown
tle uncertainty around it. However, the fact that it can be arbitrarily here, the Laplace approximation can be

extremely overconfident when p is not
different from the true posterior makes it unsuitable for approximate approximately Gaussian.
probabilistic inference.

5.1.1 Example: Bayesian Logistic Regression σ(z)
1.00

As an example, we will look at Laplace approximation in the context 0.75

of Bayesian logistic regression. Logistic regression learns a classifier 0.50
that decides for a given input whether it belongs to one of two classes.

0.25
A sigmoid function, typically the logistic function,

0.00

σ(z . 1
) = ∈ (0, 1), z = w⊤x, (5.9) −5 0 5

1 + exp(−z) z

is used to obtain the class probabilities. Bayesian logistic regression cor- Figure 5.2: The logistic function
squashes the linear function w⊤x onto

responds to Bayesian linear regression with a Bernoulli likelihood, the interval (0, 1).

y | x, w ∼ Bern(σ(w⊤x)), (5.10) x2

10
where y ∈ {−1, 1} is the binary class label.3 Observe that given a data w
point (x, y), the probability of a correct classification is 5

 0

p(y | x, w) = σ(w⊤x) if y = 1
= σ(yw⊤x), (5.11)

1− σ(w⊤
−5

x) if y = −1
−2 0 2

as the logistic function σ is symmetric around 0. Also, recall that x1

Bayesian linear regression used the prior( ) Figure 5.3: Logistic regression classifies
data into two classes with a linear deci-
sion boundary.

p(w) = N (w; 0, σ2
p I) ∝ exp − 1 ∥w 2

2σ2 ∥2 .
p 3 The same approach extends to Gaus-

sian processes where it is known as
Gaussian process classification, see Prob-

Let us first find the posterior mode, that is, the MAP estimate of the lem 5.2 and Hensman et al. (2015).
weights:

ŵ = arg max p(w | x1:n, y1:n)
w



86 probabilistic artificial intelligence

= arg max p(w)p(y1:n | x1:n, w) using Bayes’ rule (1.45)
w

= arg max log p(w) + log p(y1:n | x1:n, w) taking the logarithm
w

= arg max− 1 n
∥w∥2 log σ y

2σ2 2 + ∑ ( iw⊤xi) using independence of the observations
w p i=1 and Equation (5.11)

1 n
= arg min ∥w∥2 + log 1 e

2σ2 2 ∑ ( + xp(−yiw⊤xi)). (5.12) using the definition of σ (5.9)
w p i=1

Note that for λ = 1/2σ2
p, the above optimization is equivalent to stan-

dard (regularized) logistic regression where
.

ℓlog(w
⊤x; y) = log(1 + exp(−yw⊤x)) (5.13)

is called logistic loss. The gradient of the logistic loss is given by ? Problem 5.1 (1)

∇w ℓlog(w
⊤x; y) = −yx · σ(−yw⊤x). (5.14)

Recall that due to the symmetry of σ around 0, σ(−yw⊤x) is the prob-
ability that x was not classified as y. Intuitively, if the model is “sur-
prised” by the label, the gradient is large.

We can therefore use SGD with the (regularized) gradient step and
with batch size 1,

w← w(1− 2ληt) + ηtyxσ(−yw⊤x), (5.15)

for the data point (x, y) picked uniformly at random from the training
data. Here, 2ληt is due to the gradient of the regularization term, in
effect, performing weight decay.

Example 5.2: Laplace approx. of Bayesian logistic regression
We have already found the mode of the posterior distribution, ŵ.

Let us denote by
.

πi = P(yi = 1 | xi, ŵ) = σ(ŵ⊤xi) (5.16)

the probability of xi belonging to the positive class under the
model given by the MAP estimate of the weights. For the pre-
cision matrix, we then have

Λ = − Hw log p(w | x1:n, y1:n)|w=ŵ
= − Hw log p(y1:n | x1:n, w)|w=ŵ − Hw log p(w)|w=ŵ

n ∣
= ∑ H ⊤ ∣

wℓlog(w xi; yi)∣ + σ−2 I s n t e e n t
i w p u i g h d fi i ion of the logistic loss
=1 =ŵ (5.13)
n

= ∑ xix⊤i πi(1− πi) + σ−2
p I using the Hessian of the logistic loss

i=1 (5.73) which you derive in Problem 5.1
(2)



variational inference 87

= X⊤diagi∈[n]{πi(1− πi)}X + σ−2
p I. (5.17)

Observe that πi(1− πi) ≈ 0 if πi ≈ 1 or πi ≈ 0. That is, if a train-
ing example is “well-explained” by ŵ, then its contribution to the
precision matrix is small. In contrast, we have πi(1− πi) = 0.25
for πi = 0.5. Importantly, Λ does not depend on the normalization
constant of the posterior distribution which is hard to compute.

In summary, we have that N (ŵ, Λ−1) is the Laplace approxima-
tion of p(w | x1:n, y1:n).

5.2 Predictions with a Variational Posterior

How can we make predictions using our variational approximation?
We simply approximate the (intractable) true posterior with our varia-
tional posterior: ∫

p(y⋆ | x⋆, x1:n, y1:n) = ∫ p(y⋆ | x⋆, θ)p(θ | x1:n, y1:n) dθ using the sum rule (1.7)

≈ p(y⋆ | x⋆, θ)qλ(θ) dθ. (5.18)

A straightforward approach is to observe that Equation (5.18) can be
viewed as an expectation over the variational posterior qλ and approx-
imated via Monte Carlo sampling:

= Eθ∼q [p(y⋆ | x⋆, θ)] (5.19)
λ

m
≈ 1

∑ p(y⋆ | x⋆, θj) (5.20)
m j=1

where i∼idθj qλ.

Example 5.3: Predictions in Bayesian logistic regression
In the case of Bayesian logistic regression with a Gaussian approx-
imation of the posterior, we can obtain more accurate predictions.

Observe that the final prediction y⋆ is conditionally independent
of the model parameter∫s w given the “latent value” f ⋆ = w⊤x⋆:

p(y⋆ | x⋆, x1:n, y1:n) ≈ ∫ ∫p(y⋆ | x⋆, w)qλ(w) dw

= ∫ p(y⋆ | f∫⋆)p( f ⋆ | x⋆, w)qλ(w) dw d f ⋆ once more, using the sum rule (1.7)

= p(y⋆ | f ⋆) p( f ⋆ | x⋆, w)qλ(w) dw d f ⋆. rearranging terms

(5.21)



88 probabilistic artificial intelligence

The outer integral can be readily approximated since it is only
one-dimensional! The challenging part is the inner integral, which
is a high-dimensional integral over the model weights w. Since the
posterior over weights qλ(w) = N (w; ŵ, Λ−1) is a Gaussian, we
have d∫ue to the closedness properties of Gaussians (1.78) that

p( f ⋆ | x⋆, w)qλ(w) dw = N (ŵ⊤x⋆, x⋆⊤Λ−1x⋆). (5.22)

Crucially, this is a one-dimensional Gaussian in function-space as
opposed to the d-dimensional Gaussian qλ in weight-space!

As we have seen in Equation (5.11), for Bayesian logistic regres-
sion, the prediction y⋆ depends deterministically on the predicted
latent value f ⋆: p(y⋆ | f ⋆) = σ(y⋆ f ⋆). Combining Equations (5.21)
and (5.22), we obtain ∫

p(y⋆ | x⋆, x1:n, y1:n) ≈ σ(y⋆ f ⋆) · N ( f ⋆; ŵ⊤x⋆, x⋆⊤Λ−1x⋆) d f ⋆.
(5.23)

We have replaced the high-dimensional integral over the model
parameters θ by the one-dimensional integral over the prediction
of our variational posterior f ⋆. While this integral is generally
still intractable, it can be approximated efficiently using numerical
quadrature methods such as the Gauss-Legendre quadrature or
alternatively with Monte Carlo sampling.

5.3 Blueprint of Variational Inference

General probabilistic inference poses the challenge of approximating
the posterior distribution with limited memory and computation, re-
source constraints also present in humans and other intelligent sys-
tems. These resource constraints require information to be compressed,
and as we will see, such a compression poses a fundamental tradeoff
between model accuracy (on the observed data) and model complexity
(to avoid overfitting).

Laplace approximation approximates the true (intractable) posterior
with a simpler one, by greedily matching mode and curvature around
it. Can we find “less greedy” approaches? We can view variational
probabilistic inference more generally as a family of approaches aim-
ing to approximate the true posterior distribution by one that is closest
(according to some criterion) among a “simpler” class of distributions.
To this end, we need to fix a class of distributions and define suit-
able criteria, which we can then optimize numerically. The key ben-



variational inference 89

efit is that we can reduce the (generally intractable) problem of high-
dimensional integration to the (often much more tractable) problem of
optimization.

Definition 5.4 (Variational family). Let P be the class of all probability p
q⋆

distributions. A variational family Q ⊆ P is a class of distributions such
that each distribution q ∈ Q is characterized by unique variational Q
parameters λ ∈ Λ. P

Figure 5.4: An illustration of variational
Example 5.5: Family of independent Gaussians inference in the space of distributions P .

The variational distribution q⋆ ∈ Q is the
A straightforward example for a variational family is the family optimal approximation of the true poste-
of independent Gau{ssians, } rior p.

Q .
= q(θ) = N (θ; µ, diagi∈[d]{σ2

i }) , (5.24)

which is parameterized by .
λ = [µ1:d, σ2

1:d]. Such a multivariate dis-
tribution where all variables are independent is called a mean-field
distribution. Importantly, this family of distributions is character-
ized by only 2d parameters!

Note that Figure 5.4 is a generalization of the canonical distinction be-
tween estimation error and approximation error from Figure 1.7, only
that here, we operate in the space of distributions over functions as op-
posed the space of functions. A common notion of distance between
two distributions q and p is the Kullback-Leibler divergence KL(q∥p)
which we will define in the next section. Using this notion of distance,
we need to solve the following optimization problem:

q⋆ .
= arg min KL(q∥p) = arg min KL(qλ∥p). (5.25)

q∈Q λ∈Λ

In Section 5.4, we introduce information theory and the Kullback-
Leibler divergence. Then, in Section 5.5, we discuss how the opti-
mization problem of Equation (5.25) can be solved efficiently.

5.4 Information Theoretic Aspects of Uncertainty

One of our main objectives throughout this manuscript is to capture
the “uncertainty” about events A in an appropriate probability space.
One very natural measure of uncertainty is their probability, P(A). In
this section, we will introduce an alternative measure of uncertainty,
namely the so-called “surprise” about the event A.



90 probabilistic artificial intelligence

5.4.1 Surprise

The surprise about an event with probability u is defined as
S[u]

S .
[u] = − log u. (5.26)

4
Observe that the surprise is a function from R≥0 to R, where we
let S[0] ≡ ∞. Moreover, for a discrete random variable X, we have 2

that p(x) ≤ 1, and hence, S[p(x)] ≥ 0. But why is it reasonable to
measure surprise by − log u? 0

0.00 0.25 0.50 0.75 1.00

Remarkably, it can be shown that the following natural axiomatic char- u

acterization leads to exactly this definition of surprise. Figure 5.5: Surprise S[u] associated with
an event of probability u.

Theorem 5.6 (Axiomatic characterization of surprise). The axioms
1. S[u] > S[v] =⇒ u < v (anti-monotonicity) we are more surprised by unlikely events
2. S is continuous, no jumps in surprise for infinitesimal

changes of probability
3. S[uv] = S[u] + S[v] for independent events, the surprise of independent events is

characterize S up to a positive constant factor. additive

Proof. Observe that the third condition looks similar to the product
rule of logarithms: log(uv) = log v + log v. We can formalize this
intuition by remembering Cauchy’s functional equation, f (x + y) =
f (x) + f (y), which has the unique family of solutions { f : x 7→ cx :
c ∈ R} if f is required to be continuous. Such a solution is called an
“additive function”. Consider the function g .

(x) = f (ex). Then, g is
additive if and only if

f (exey) = f (ex+y) = g(x + y) = g(x) + g(y) = f (ex) + f (ey).

This is precisely the third axiom of surprise for f = S and ex = u!
Hence, the second and third axioms of surprise imply that g must be
additive and that g(x) = S[ex] = cx for any c ∈ R. If we replace ex

by u, we obtain S[u] = c log u. The first axiom of surprise implies that
c < 0, and thus, S[u] = −c′ log u for any c′ > 0.

Importantly, surprise offers a different perspective on uncertainty as
opposed to probability: the uncertainty about an event can either be
interpreted in terms of its probability or in terms of its surprise, and “

ability” su
“prob rprise”

the two “spaces of uncertainty” are related by a log-transform. This
− log S[P(A)]

relationship is illustrated in Figure 5.6. Information theory is the study P(A)

of uncertainty in terms of surprise. information
pr t

o h
b e

a o
b ry

ility theory

Throughout this manuscript we will see many examples where mod- Figure 5.6: Illustration of the probability

eling uncertainty in terms of surprise (i.e., the information-theoretic space and the corresponding “surprise
space”.

interpretation of uncertainty) is useful. One example where we have



variational inference 91

already encountered the “surprise space” was in the context of like-
lihood maximization (cf. Section 1.3.1) where we used that the log-
transform linearizes products of probabilities. We will see later in
Chapter 6 that in many cases the surprise S[p(x)] can also be inter-
preted as a “cost” or “energy” associated with the state x. H[Bern(p)]

1.00

5.4.2 Entropy 0.75

0.50
The entropy of a distribution p is the average surprise about samples
from p. In this way, entropy is a notion of uncertainty associated with 0.25

the distribution p: if the entropy of p is large, we are more uncertain 0.00

about x ∼ p than if the entropy of p were low. Formally, 0.0 0.5 1.0
p

H[p .
] = Ex∼p[S[p(x)]] = Ex∼p[− log p(x)]. (5.27) Figure 5.7: Entropy of a Bernoulli exper-

iment with success probability p.
When X ∼ p is a random vector distributed according to p, we write
H .
[X] = H[p]. Observe that by definition, if p is discrete then H[p] ≥ 0

as p(x) ≤ 1 (∀x).4 For discrete distributions it is common to use the 4 The entropy of a continuous distribu-
logarithm with base 2 rather than the natural logarithm:5 tion can be negative∫. For example,

H[Unif([a, b])] = − 1 1
H lo
[p] = −∑∫ p(x) log2 p(x) (if p is discrete), (5.28a) b− g dx

a b− a
x = log(b− a)

H[p] = − p(x) log p(x) dx (if p is continuous). (5.28b) which is negative if b− a < 1.
5 Recall that log2 x log x

= log 2 , that is, loga-
rithms to a different base only differ by

Let us briefly recall Jensen’s inequality, which is a useful tool when a constant factor.
working with expectations of convex functions such as entropy:6 6 The surprise S[u] is convex in u.

Fact 5.7 (Jensen’s Inequality). ? Given a random variable X and a convex Problem 5.3 (1)
function g : R→ R, we have

g(E[X]) ≤ E[g(X)]. (5.29)
g

E[g(X)]

Example 5.8: Examples of entropy

• Fair Coin H[Bern(0.5)] = −2(0.5 log2 0.5) = 1. g(E[X])

• Unfair Coin
E[X]

H[Bern(0.1)] = −0.1 log2 0.1− 0.9 log2 0.9 ≈ 0.469. Figure 5.8: An illustration of Jensen’s in-
equality. Due to the convexity of g, we

• Uniform Distribution have that g evaluated at E[X] will always
be below the average of evaluations of g.

n
H[Unif({1, . . . , n})] = − 1 1

∑ log2 = log
i n 2 n.
=1 n

The uniform distribution has the maximum entropy among all
discrete distributions supported on {1, . . . , n} ? . Note that a Problem 5.3 (2)
fair coin corresponds to a uniform distribution with n = 2. Also



92 probabilistic artificial intelligence

observe that log2 n corresponds to the number of bits required
to encode the outcome of the experiment.

In general, the entropy H[p] of a discrete distribution p can be
interpreted as the average number of bits required to encode a
sample x ∼ p, or in other words, the average “information” car-
ried by a sample x.

Example 5.9: Entropy of a Gaussian
Let us derive the entropy of a univar(iate Gaussian. Recall the PDF,

1 (x− )
µ)2

N (x; µ, σ2) = exp −
Z 2σ2

√
where Z = 2πσ2. Using the definition of entropy (5.28b), we
obtain,[ ] ∫ (

N 1
H (µ, σ2) = − exp −

Z ((x− )
( µ)2

2σ2 ))
· 1 (x µ 2

p − − )
log∫ ex dx

Z ( 2σ2 )
1 (x− µ)2

= log Z ︸ exp −
Z ( ︷︷ 2σ2 dx

∫ ︸
1

1 (x− µ)2) (x− µ)2
+ exp −

2σ2 dx
Z 2σ2

1 [ ]
= log Z + E (x− µ)2

2σ2 using LOTUS (1.22)

√ 1 [
= log(σ 2π) + using E (x− µ)2] = Var[x] = σ2 (1.34)

√ 2
= log( 2πe). (5.30) √

σ using log e = 1/2

In general, the entropy of a Gaussian is ( )
H[N 1 1

(µ, Σ)] = log det(2πeΣ) = log (2πe)ddet(Σ) . (5.31)
2 2

Note that the entropy is a function of the determinant of the co-
variance matrix Σ. In general, there are various ways of “scalariz-
ing” the notion of uncertainty for a multivariate distribution. The
determinant of Σ measures the volume of the credible sets around
the mean µ, and is also called the generalized variance. Next to
entropy and generalized variance (which are closely related for
Gaussians), a common scalarization is the trace of Σ, which is also
called the total variance.



variational inference 93

5.4.3 Cross-Entropy

How can we use entropy to measure our average surprise when as-
suming the data follows some distribution q but in reality the data
follows a different distribution p?

Definition 5.10 (Cross-entropy). The cross-entropy of a distribution q
relative to the distribution p is

H[p∥q .
] = Ex∼p[S[q(x)]] = Ex∼p[− log q(x)]. (5.32)

Cross-entropy can also be expressed in terms of the KL-divergence (cf.
Section 5.4.4) KL(p∥q) which measures how “different” the distribu-
tion q is from a reference distribution p,

H[p∥q] = H[p] + KL(p∥q) ≥ H[p]. (5.33) KL(p∥q) ≥ 0 is shown in Problem 5.5

Quite intuitively, the average surprise in samples from p with respect
to the distribution q is given by the inherent uncertainty in p and the
additional surprise that is due to us assuming the wrong data distri-
bution q. The “closer” q is to the true data distribution p, the smaller
is the additional average surprise.

5.4.4 Kullback-Leibler Divergence

As mentioned, the Kullback-Leibler divergence is a (non-metric) mea-
sure of distance between distributions. It is defined as follows.

Definition 5.11 (Kullback-Leibler divergence, KL-divergence). Given
two distributions p and q, the Kullback-Leibler divergence (or relative en-
tropy) of q with respect to p,

KL(p∥q .
) = H[p∥q]−H[p] (5.34)
= Eθ∼p[[S[q(θ)]−]S[p(θ)]] (5.35)

p(θ)
= Eθ∼p log , (5.36)

q(θ)

measures how different q is from a reference distribution p.

In words, KL(p∥q) measures the additional expected surprise when ob-
serving samples from p that is due to assuming the (wrong) distribu- 7 The KL-divergence only captures the
tion q and which not inherent in the distribution p already.7 additional expected surprise since the

surprise inherent in p (as measured by

The KL-divergence has the following properties: H[p]) is subtracted.

• KL(p∥q) ≥ 0 for any distributions p and q ? , Problem 5.5 (1)
• KL(p∥q) = 0 if and only if p = q almost surely ? , and Problem 5.5 (2)
• there exist distributions p and q such that KL(p∥q) ̸= KL(q∥p).



94 probabilistic artificial intelligence

The KL-divergence can simply be understood as a shifted version of
cross-entropy, which is zero if we consider the divergence between two
identical distributions.

We will briefly look at another interpretation for how KL-divergence
measures “distance” between distributions. Suppose we are presented
with a sequence θ1, . . . , θn of independent samples from either a dis-
tribution p or a distribution q, both of which are known. Which of
p or q was used to generate the data is, however, unknown to us,
and we would like to find out. A natural approach is to choose the
distribution whose data likelihood is larger. That is, we choose p if
p(θ1:n) > q(θ1:n) and vice versa. Assuming that the samples are inde-
pendent and rewriting the inequality slightly, we choose p if

n p(θ n (θ )
∏ i) p

> 1, or equivalently if log i > 0. (5.37) taking the logarithm
i q(θ
=1 i)

∑
i 1 q(θ
= i)

Assume without loss of generality that θi ∼ p. Then, using the law of
large numbers (A.36),

1 n [ ]
p(θ (θ)

n ∑ log i) a→.s. p
Eθ∼p log = KL(p∥q) (5.38)

i q(θ q(θ)
=1 i)

as n→ ∞. Plugging this into our decision criterion from Equation (5.37),
we find that [ ]

n p(θ
E ∑ log i) = nKL(p∥q). (5.39)

i q(θ
=1 i)

In this way, KL(p∥q) measures the observed “distance” between p
and q. Recall that assuming p ̸= q we have that KL(p∥q) > 0 with
probability 1, and therefore we correctly choose p with probability 1
as n→ ∞. Moreover, Hoeffding’s inequality (A.41) can be used to de-
termine “how quickly” samples converge to this limit, that is, how
quickly we can distinguish between p and q.

Example 5.12: KL-divergence of Bernoulli random variables
Suppose we are given two Bernoulli distributions Bern(p) and
Bern(q). Then, their KL-divergence is

Bern(x; p)
KL(Bern(p)∥Bern(q)) = ∑ Bern(x; p) log

x∈{0,1} Bern(x; q)

p (1− p)
= p log + (1− p) log . (5.40)

q (1− q)

Observe that KL(Bern(p)∥Bern(q)) = 0 if and only if p = q.



variational inference 95

Example 5.13: KL-divergence of Gaussians
Suppose we are given two Gaussian distributions p .

= N (µp, Σ p)
and q .

= N (µq, Σq) with dimension d. The KL-divergence of p
and q is given b(y ? Problem 5.8

KL(p∥ 1
q) = tr(Σ−1Σ ) + ( ⊤ −1 )

2 q p µp − µq) Σq (µp −(µq)

( )) (5.41

− det Σ
d + log q) .

det Σ p 0.8

⋆
0.6 q2

For independent Gaussians with unit variance, Σ p = Σq = I, the
expression simplifies to the squared Euclidean distance, 0.4

1 ∥∥ ∥ q⋆1
0.2 p

KL(p∥q) = µ ∥ . ( . 2
2 q − 2

µp 2 5 4 )
0.0

−2 0 2 4 6

If we approximate independent Gaussians with variances σ2
i , x

p .
= N (µ, diag{σ2 x2

1 , . . . , σ2
d}),

by a standard normal distribution, q .
= N (0, I), the expression 2

simplifies to 1

0
1 d

KL(p∥q) =
2 ∑(σ2

i + µ2 2
i − 1− log σi ). (5.43) −1

i=1 −2

Here, the term µ2
i penalizes a large mean of p, the term σ2

i penal-
izes a large variance of p, and the term − log σ2

i penalizes a small −2 0 2
2

variance of p. As expected, KL(p∥q) is proportional to the amount 1
of information we lose by approximating p with the simpler dis-

0
tribution q.

−1

−2

5.4.5 Forward and Reverse KL-divergence
−2 0 2

KL(p∥q) is also called the forward (or inclusive) KL-divergence. In x1

contrast, KL(q∥p) is called the reverse (or exclusive) KL-divergence. Figure 5.9: Comparison of the forward
Figure 5.9 shows the approximations of a general Gaussian obtained KL-divergence q⋆1 and the reverse KL-
when Q is the family of diagonal (independent) Gaussians. Thereby, divergence q⋆2 when used to approxi-

mate the true posterior p. The first plot
shows the PDFs in a one-dimensional

q⋆ .
1 = arg min KL(p∥q) and q⋆ .

2 = arg min KL(q∥p). feature space where p is a mixture of
q∈Q q∈Q two univariate Gaussians. The second

plot shows contour lines of the PDFs in
q⋆1 is the result when using the forward KL-divergence and q⋆2 is the re- a two-dimensional feature space where
sult when using reverse KL-divergence. It can be seen that the reverse the non-diagonal Gaussian p is approx-

imated by diagonal Gaussians q⋆
KL-divergence tends to greedily select the mode and underestimat- 1 and

q⋆2 . It can be seen that q⋆1 selects the
ing the variance which, in this case, leads to an overconfident predic- variance and q⋆2 selects the mode of p.

The approximation q⋆1 is more conserva-
tive than the (overconfident) approxima-
tion q⋆2 .



96 probabilistic artificial intelligence

tion. The forward KL-divergence, in contrast, is more conservative and
yields what one could consider the “desired” approximation.

Recall that in the blueprint of variational inference (5.25) we used
the reverse KL-divergence. This is for computational reasons. Ob-
serve that to approximate the KL-divergence KL(p∥q) using Monte
Carlo sampling, we would need to obtain samples from p yet p is
the intractable posterior distribution which we were trying to approx-
imate in the first place. Crucially, observe that if the true posterior
p(· | x1:n, y1:n) is in the variational family Q, then

arg min KL(q∥p(· | x , y a.s.
1:n 1:n)) = p(· | x1: min a.s.

n, y1:n), ( . as q∈Q KL(q∥p(· | x1:n, y1:n)) = 0
5 44)

q∈Q

so minimizing reverse-KL still recovers the true posterior almost surely.

Remark 5.14: Greediness of reverse-KL
As in the previous example, consider the independent Gaussians

p .
= N (µ, diagi∈[d]{σ2

i }),
which we seek to approximate by a standard normal distribu-
tion q .

= N (0, I). Using (5.41), we obtain for the reverse KL-
divergence,

1 ( ( )
KL(q∥p) = tr diag{σ−2 ⊤

2 i } + µ diag{(σ−2
i }µ− d))

( + log det)diag{σ2
i }

1 d 2
= σ−2 µi log σ2

2 ∑ i + − 1 + i . (5.45)
i=1 σ2

i

Here, σ−2
i penalizes small variance, µ2

i/σ2
i penalizes a large mean,

and log σ2
i penalizes large variance. Compare this to the expres-

sion for the forward KL-divergence KL(p∥q) that we have seen in
Equation (5.43). In particular, observe that reverse-KL penalizes
large variance less strongly than forward-KL.

Note, however, that reverse-KL is not greedy in the same sense
as Laplace approximation, as it does still take the variance into
account and does not purely match the mode of p.

5.4.6 Interlude: Minimizing Forward KL-Divergence

Before completing the blueprint of variational inference in Section 5.5
by showing how reverse-KL can be efficiently minimized, we will di-
gress briefly and relate minimizing forward-KL to two other well-known



variational inference 97

inference algorithms. This discussion will deepen our understanding
of the KL-divergence and its role in probabilistic inference, but feel free
to skip ahead to Section 5.5 if you are eager to complete the blueprint.

Minimizing forward-KL as maximum likelihood estimation: First, we ob-
serve that minimizing the forward KL-divergence is equivalent to max-
imum likelihood estimation on an infinitely large sample size. The
classical application of this result is in the setting where p(x) is a gen-
erative model, and we aim to estimate its density with the parameter-
ized model qλ.

Lemma 5.15 (Forward KL-divergence as MLE). Given some generative
model p(x) and a likelihood qλ(x) = q(x | λ) (that we use to approximate
the true data distribution), we have

n
arg min KL(p∥q a.s. 1

λ) = arg max l→im ∑ log q(xi | λ), (5.46)
λ∈Λ n ∞

λ∈Λ n i=1

where x i
i ∼id p are independent samples from the true data distribution.

Proof.

KL(p∥qλ) = H[p∥qλ]−H[p] using the definition of KL-divergence
(5.34)

= E(x,y)∼p[− log q(x | λ)] + const dropping H[p] and using the definition
of cross-entropy (5.32)

a.s.
= − 1 n

lim
n→ n ∑ log q(xi | λ) + const using Monte Carlo sampling, i.e., the

∞
i=1 law of large numbers (A.36)

where x i∼idi p are independent samples.

This tells us that any maximum likelihood estimate qλ minimizes the
forward KL-divergence to the empirical data distribution. Note that
here, we aim to learn model parameters λ for estimating the probabil-
ity of x, whereas in the setting of variational probabilistic inference, we
want to learn parameters λ of a distribution over θ and θ parameterizes
a distribution over x. This interpretation is therefore not immediately
useful for probabilistic inference (i.e., in the setting where p is a pos-
terior distribution over model parameters θ) as a maximum likelihood
estimate requires i.i.d. samples from p which we cannot easily obtain
in this case.8 8 It is possible to obtain “approximate”

samples using Markov chain Monte
Carlo (MCMC) methods which we dis-

Example 5.16: Minimizing cross-entropy cuss in Chapter 6.
Minimizing the KL-divergence between p and qλ is equivalent to
minimizing cross-entropy since KL(p∥qλ) = H[p∥qλ]−H[p] and
H[p] is constant with respect to p.



98 probabilistic artificial intelligence

Lets consider an example in a binary classification problem with
the label y ∈ {0, 1} and predicted class probability ŷ ∈ [0, 1] for
some fixed input. It is natural to use cross-entropy as a measure
of dissimilarity between y and ŷ,

.
ℓbce(ŷ; y) = H[Bern(y)∥Bern(ŷ)]

= − ∑ Bern(x; y) log Bern(x; ŷ) (5.47)
x∈{0,1}

= −y log ŷ− (1− y) log(1− ŷ).

This loss function is also known as the binary cross-entropy loss and
we will discuss it in more detail in Section 7.1.3 in the context of
neural networks.

Minimizing forward-KL as moment matching: Now to a second inter-
pretation of minimizing forward-KL. Moment matching (also known as
the method of moments) is a technique for approximating an unknown
distribution p with a parameterized distribution qλ where λ is cho-
sen such that qλ matches the (estimated) moments of p. For example,
given the estimates a and B of the first and second moment of p,9 and 9 These estimates are computed using
if qλ is a Gaussian with parameters λ = {µ, Σ}, then moment matching the samples from p. For example, using

a sample mean and a sample variance
chooses λ as the solution to to compute the estimates of the first and

second moment.

[Ep[θ]] ≈ a !
= µ = Eq [θ]

λ [ ]
!

Ep θθ⊤ ≈ B = Σ + µµ⊤ = Eq θθ⊤ . using the definition of variance (1.35)
λ

In general the number of moments to be matched (i.e., the number of
equations) is adjusted such that it is equal to the number of parameters
to be estimated. We will see now that the “matching” of moments
is also ensured when qλ is obtained by minimizing the forward KL-
divergence within the family of Gaussians.

The Gaussian PDF can be expressed as

N 1
(θ; µ, Σ) = [ exp(λ⊤s(θ)) where (5.48)

Z(λ) ]
. Σ−1µ

λ = [ (5.49)
vec[Σ−1] ]

. θ
s(θ) = (5.50)

vec[− 1 ⊤
2 θθ ]

and Z . ∫
(λ) = exp(λ⊤s(θ)) dθ, and we will confirm this in just a mo-

ment.10 The family of distributions with densities of the form (5.48) — 10 Given a matrix A ∈ Rn×m, we use
with an additional scaling constant h(θ) which is often 1 — is called vec[A] ∈ Rn·m

to denote the row-by-row concatenation
of A yielding a vector of length n ·m.



variational inference 99

the exponential family of distributions. Here, s(θ) are the sufficient statis-
tics, λ are called the natural parameters, and Z(λ) is the normalizing
constant. In this context, Z(λ) is often called the partition function.

To see that the Gaussian is indeed part of the exponential family as
promised in Equations (5(.49) and (5.50), consider )

N −1
(θ; µ, Σ) ∝ exp( ((θ− µ)⊤Σ−1(θ− µ)

2 ) )
1

∝ exp(tr(− θ⊤Σ−1θ)+ θ⊤Σ−1µ) expanding the inner product and using
2 that tr(x) = x for all x ∈ R
1

= exp( tr − θθ⊤Σ−1 + θ⊤Σ−1µ ) using that the trace is invariant under
2 cyclic permutations

= exp vec[− 1
2 θθ⊤]⊤vec[Σ−1] + θ⊤Σ−1µ . using tr(AB) = vec[A]⊤vec[B] for any

A, B ∈ Rn×n

This allows us to expr∫ess the forward KL-divergence as

KL(p∥ p(θ)
qλ) = ∫p(θ) log dθ

qλ(θ)

= − ∫
p(θ) · λ⊤s(θ) dθ+ log Z(λ) + const. using that p(θ) log p(θ) dθ is constant

Differentiating with re∫spect to the natural pa

1 ∫rameters λ gives

∇λ KL(p∥qλ) = − p(θ)s(θ) dθ+ s(θ) exp(λ⊤s(θ)) dθ
Z(λ)

= −Eθ∼p[s(θ)] + Eθ∼q [s(θ)].
λ

Hence, for any minimizer of KL(p∥qλ), we have that the sufficient
statistics under p and qλ match:

Ep[s(θ)] = Eq [s(θ)]. (5.51)
λ

Therefore, in the Gaussian case, [ ] [ ]
Ep[θ] = Eq [θ] and E 1 ⊤

p − 2 θθ = Eq − 1
2 θθ⊤ ,

λ λ

implying that [ ]
Ep[θ] = µ and Var ⊤

p[θ] = Ep θθ −Ep[θ] ·Ep[θ]
⊤ = Σ (5.52) using Equation (1.35)

where µ and Σ are the mean and variance of the approximation qλ, re-
spectively. That is, a Gaussian qλ minimizing KL(p∥qλ) has the same
first and second moment as p. Combining this insight with our obser-
vation from Equation (5.46) that minimizing forward-KL is equivalent
to maximum likelihood estimation, we see that if we use MLE to fit a
Gaussian to given data, this Gaussian will eventually match the first
and second moments of the data distribution.



100 probabilistic artificial intelligence

5.5 Evidence Lower Bound

Let us return to the blueprint of variational inference from Section 5.3.
To complete this blueprint, it remains to show that the reverse KL-
divergence can be minimized effici[ently. We have ]

q(θ)
KL(q∥p(· | x1:n, y1:n)) = Eθ∼q[log using the definition of the

p(θ | x1:n, y1:n) ] KL-divergence (5.34)
p(y | x )q(θ

log 1:n 1:n )
= Eθ∼q p(y1:n, θ | using the definition of conditional

x1:n) probability (1.8)
= log p(y1:n | x1:n) using linearity of expectation (1.20)

−︸ Eθ∼q[log p(y1:︷n︷, θ | x1:n)]−H[q︸]
−L(q,p;Dn)

where L(q, p;Dn) is called the evidence lower bound (ELBO) given the
data Dn = {(xi, yi)}n

i=1. This gives the relationship

L(q, p;Dn) = l︸og p(y︷1:︷n | x1:n︸)− KL(q∥p(· | x1:n, y1:n)). (5.53)
const

Thus, maximizing the ELBO coincides with minimizing reverse-KL.

Maximizing the ELBO,

L(q, p;D .
n) = Eθ∼q[log p(y1:n, θ | x1:n)] + H[q], (5.54)

selects q that has large joint likelihood p(y1:n, θ | x1:n) and large en-
tropy H[q]. The ELBO can also be expressed in various other forms:

L(q, p;Dn) = Eθ∼q[log p(y1:n, θ | x1:n)− log q(θ)] (5.55a) using the definition of entropy (5.27)

= Eθ∼q[log p(y1:n | x1:n, θ) + log p(θ)− log q(θ)] (5.55b) using the product rule (1.11)

= E︸ θ∼q[log p(︷y︷1:n | x1:n, θ)︸] ︸− KL(︷q︷∥p(·)︸) . (5.55c) using the definition of KL-divergence
(5.34)

log-likelihood proximity to prior

where we denote by p(·) the prior distribution. Equation (5.55c) high-
lights the connection to probabilistic inference, namely that maximiz-
ing the ELBO selects a variational distribution q that is close to the
prior distribution p(·) while also maximizing the average likelihood of
the data p(y1:n | x1:n, θ) for θ ∼ q. This is in contrast to maximum a
posteriori estimation, which picks a single model θ that maximizes the
likelihood and proximity to the prior. As an example, let us look at
the case where the prior is noninformative, i.e., p(·) ∝ 1. In this case,
the ELBO simplifies to Eθ∼q[log p(y1:n, | x1:n, θ)] + H[q] + const. That
is, maximizing the ELBO maximizes the average likelihood of the data
under the variational distribution q while regularizing q to have high



variational inference 101

entropy. Why is it reasonable to maximize the entropy of q? Consider
two distributions q1 and q2 under which the data is “equally” likely
and which are “equally” close to the prior. Maximizing the entropy
selects the distribution that exhibits the most uncertainty which is in
accordance with the maximum entropy principle.11 11 In Section 1.2.1, we discussed the max-

imum entropy principle and the related
Recalling that KL-divergence is non-negative, it follows from Equa- principle of indifference at length. In

tion (5.53) that the evidence lower bound is a (uniform12) lower bound simple terms, the maximum entropy
principle states that without informa-

to the evidence log p(y1:n | x1:n): tion, we should choose the distribution
that is maximally uncertain.

log p(y1:n | x1:n) ≥ L(q, p;Dn). (5.56) 12 That is, the bound holds for any varia-
tional distribution q (with full support).

This indicates that maximizing the evidence lower bound is an ade-
quate method of model selection which can be used instead of max-
imizing the evidence (marginal likelihood) directly (as was discussed
in Section 4.4.2). Note that this inequality lower bounds the logarithm
of an integral by an expectation of a logarithm over some variational
distribution q. Hence, the ELBO is a family of lower bounds — one for
each variational distribution. Such inequalities are called variational
inequalities.

Example 5.17: Gaussian VI vs Laplace approximation
Consider maximizing the ELBO within the variational family of
Gaussians Q .

= {q(θ) = N (θ; µ, Σ)}. How does this relate to
Laplace approximation which also fits a Gaussian approximation
to the posterior

p(θ | D) ∝ p(y1:n, θ | x1:n)?

It turns out that both approximations are closely related. Indeed,
it can be shown that while the Laplace approximation is fitted
locally at the MAP estimate θ̂, satisfying

0 = ∇θ log p(y1:n, θ | x1:n)
(5.57)

Σ−1 = − Hθ log p(y1:n, θ | x1:n)|θ=θ̂ ,

Gaussian variational inference satisfies the conditions of the Laplace
approximation on average with respect to the approximation q ? : Problem 5.10

0 = Eθ∼q[∇θ log p(y1:n, θ | x1:n)]
(5.58)

Σ−1 = −Eθ∼q[Hθ log p(y1:n, θ | x1:n)].

For this reason, the Gaussian variational approximation does not
suffer from the same overconfidence as the Laplace approxima-
tion.13 13 see Figure 5.1



102 probabilistic artificial intelligence

Example 5.18: ELBO for Bayesian logistic regression
Recall that Bayesian logistic regression uses the prior distribution
w ∼ N (0, I).14 14 We omit the scaling factor σ2

p here for
simplicity.

Suppose we use the variational family Q of all diagonal Gaussians
from Example 5.5. We have already seen in Equation (5.43) that
for a prior p ∼ N (0, I) and a variational distribution

qλ ∼ N (µ, diag 2
i∈[d]{σi }),

we have

1 d
KL(q∥p(·)) = (σ2 2 1− log σ2

2 ∑ i + µi − i ).
i=1

It remains to find the expected likelihood under models from our
approximate posterior: [ ]

n
Ew∼q [log p(y

λ 1:n | x1:n, w)] = Ew∼q [∑ log p(yi | xi, w) using independence of the data
λ

i=1 ]
n

= Ew∼q −∑ ℓlog(w; xi, yi) . substituting the logistic loss (5.13)
λ

i=1
(5.59)

5.5.1 Gradient of Evidence Lower Bound

We have yet to discuss how the optimization problem of maximizing
the ELBO can be solved efficiently. A suitable tool is stochastic gradi-
ent descent (SGD), however, SGD requires unbiased gradient estimates
of the loss .

ℓ(λ;Dn) = −L(qλ, p;Dn). That is, we need to obtain gradi-
ent estimates of

∇λ L(qλ, p;Dn) = ∇λ Eθ∼q [log p(y p(·)). using the definition of the ELBO (5.55c)
λ 1:n | x1:n, θ)]−∇λ KL(qλ∥

(5.60)

Typically, the KL-divergence (and its gradient) can be computed ex-
actly for commonly used variational families. For example, we have
already seen a closed-form expression of the KL-divergence for Gaus-
sians in Equation (5.41).

Obtaining the gradient of the expected log-likelihood is more diffi-
cult. This is because the expectation integrates over the measure qλ,
which depends on the variational parameters λ. Thus, we cannot move
the gradient operator inside the expectation as commonly done (cf.
Appendix A.1.5). There are two main techniques which are used to



variational inference 103

rewrite the gradient in such a way that Monte Carlo sampling becomes
possible.

One approach is to use score gradients via the “score function trick”:

∇λ Eθ∼q [log p(y1:n | x1:n, θ)]
λ

= Eθ∼q [log p(y1:n | x1:n, θ)∇︸ λ lo︷g︷qλ(θ︸)], (5.61)
λ

score function

which we introduce in Section 12.3.2 in the context of reinforcement
learning. More common in the context of variational inference is the
so-called “reparameterization trick”.

Theorem 5.19 (Reparameterization trick). Given a random variable ε ∼ ϕ
(which is independent of λ) and given a differentiable and invertible function
g : Rd → Rd. We let .

θ = g(ε; λ). Then,

qλ(θ) = ϕ(ε) · |det(Dεg(ε; λ))|−1 , (5.62)
Eθ∼q [ f (θ)] = Eε∼ϕ[ f (g(ε; λ))] (5.63)

λ

for a “nice” function f : Rd → Re.

Proof. By the change of variables∣ formula (1.43) and using ε = g−1(θ; λ),

qλ(θ) = ϕ(ε) · ∣∣∣
( )∣

∣det(D −
θg 1 ∣

(θ; λ) ∣)∣
= ϕ(ε) · ∣det (Dεg(ε; λ))−1 ∣∣ by the inverse function theorem,

Dg−1(y) (= Dg(x)−1

= ϕ(ε) · |det(Dεg(ε; λ))|−1 . using det A−1) = det(A)−1

Equation (5.63) is a direct consequence of the law of the unconscious
statistician (1.22).

In other words, the reparameterization trick allows a change of “den-
sities” by finding a function g(·; λ) and a reference density ϕ such that
qλ = g(·; λ)♯ϕ is the pushforward of ϕ under perturbation g. Apply-
ing the reparameterization trick, we can swap the order of gradient
and expectation,

∇λ Eθ∼q [ f (θ)] = Eε∼ϕ[∇λ f (g(ε; λ))]. (5.64) using Equation (A.5)
λ

We call a distribution qλ reparameterizable if it admits reparameteriza-
tion, i.e., if we can find g and a suitable reference density ϕ which is
independent of λ.



104 probabilistic artificial intelligence

Example 5.20: Reparameterization trick for Gaussians
Suppose we use a Gaussian variational approximation,

q .
λ(θ) = N (θ; µ, Σ),

where we assume Σ to have full rank (i.e., be invertible). We have
seen in Equation (1.78) that a Gaussian random vector ε ∼ N (0, I)
following a standard normal distribution can be transformed to
follow the Gaussian distribution qλ by using the linear transfor-
mation,

. 1
θ = g(ε; λ) = Σ /2ε + µ. (5.65)

In particular, we have

ε = g−1 1
(θ; λ∣∣ ) =(Σ− /)2(∣θ− µ) and (5.66) by solving Equation (5.65) for ε

1
ϕ(ε) = qλ(θ) · ∣det Σ /2 ∣∣ . (5.67) using the reparameterization trick (i.e.,

the change of variables formula) (5.62)

In the following, we write C .
= Σ1/2. Let us now derive the gradient

estimate for the evidence lower bound assuming the Gaussian vari-
ational approximation from Example 5.20. This approach extends to
any reparameterizable distribution.

∇λ Eθ∼q [log p(y θ)]
λ 1[:n | x1:n, ]

= ∇C,µ Eε∼N (0,I) lo[g p(y1:n | x1:n, θ)|θ=Cε+µ ] (5.68) using the reparameterization trick (5.63)

1 n
= n ·∇C,µ Eε∼N (0,I) og p(y

n ∑ l
1 [ i | xi, θ)|θ=Cε+µ ] using independence of the data and

i= extending with n/n

= n ·∇C,µ Eε∼N (0,I)Ei∼Un[if([n]) log p(yi | xi, θ)|θ=Cε+µ ] interpreting the sum as an expectation

= n ·Eε∼N (0,I)Ei∼Unif([n]) ∇C,µ log p(yi | xi, θ)|θ=Cε+µ (5.69) using Equation (A.5)

m ∣
≈ n · 1

∑ ∇ log p(y | x
m C,µ ij ij ,

∣
θ)∣ (5.70) using Monte Carlo sampling

j =Cε
=1 θ j+µ

where i
εj ∼idN (0, I) and i i∼idj Unif([n]). This yields an unbiased gra-

dient estimate, which we can use with stochastic gradient descent to
maximize the evidence lower bound. We have successfully recast the
difficult problems of learning and inference as an optimization prob-
lem!

The procedure of approximating the true posterior using a variational
posterior by maximizing the evidence lower bound using stochastic
optimization is also called black box stochastic variational inference (Ran-
ganath et al., 2014; Titsias and Lázaro-Gredilla, 2014; Duvenaud and



variational inference 105

Adams, 2015). The only requirement is that we can obtain unbiased
gradient estimates from the evidence lower bound (and the likelihood).
We have just discussed one of many approaches to obtain such gradi-
ent estimates (Mohamed et al., 2020). If we use the variational family
of diagonal Gaussians, we only require twice as many parameters as
other inference techniques like MAP estimation. The performance can
be improved by using natural gradients and variance reduction tech-
niques for the gradient estimates such as control variates.

5.5.2 Minimizing Surprise via Exploration and Exploitation

Now that we have established a way to optimize the ELBO, let us dwell
a bit more on its interpretation. Observe that the evidence can also be
interpreted as the negative surprise about the observations under the
prior distribution p(θ), and by negating Equation (5.56), we obtain the
variational upper bound

S[p(y1:n | x1:n)] ≤ E︸ θ∼q[S[p(y︷1︷:n, θ | x1:n)]︸]−H[q]
called energy (5.71)

= −L(q, p;Dn).

Here, −L(q, p;Dn) is commonly called the (variational) free energy with
respect to q. Free energy can also be characterized as

−L(q, p;Dn) = E︸ θ∼q[S[p(y︷1︷:n | x1:n, θ)]︸] + K︸ L(q︷∥︷p(·)︸) , (5.72) analogously to Equation (5.55c)

average surprise proximity to prior

and therefore its minimization is minimizing the average surprise about
the data under the variational distribution q while maximizing prox-
imity to the prior p(·). Systems that minimize the surprise in their
observations are widely studied in many areas of science.15 15 Refer to the free energy principle which

was originally introduced by the neuro-
To minimize surprise, the free energy makes apparent a natural trade- scientist Karl Friston (Friston, 2010).

off between two extremes: On the one hand, models q(θ) that “overfit”
to observations (e.g., by using a point estimate of θ), and hence, result
in a large surprise when new observations deviate from this specific
belief. On the other hand, models q(θ) that “underfit” to observa-
tions (e.g., by expecting outcomes with equal probability), and hence,
any observation results in a non-negligible surprise. Either of these
extremes is undesirable.

As we have alluded to previously when introducing the ELBO, the two
terms constituting free energy map neatly onto this tradeoff. Therein,
the entropy (which is maximized) encourages q to have uncertainty, in
other words, to “explore” beyond the finite data. In contrast, the energy
(which is minimized) encourages q to fit the observed data closely, in



106 probabilistic artificial intelligence

other words, to “exploit” the finite data. This tradeoff is ubiquitous in
approximations to probabilistic inference that deal with limited com-
putational resources and limited time, and we will encounter it many
times more. We will point out these connections as we go along.

Since this tradeoff is so fundamental and appears in many branches of
science under different names, it is difficult to give it an appropriate
unifying name. The essence of this tradeoff can be captured as a prin-
ciple of curiosity and conformity, which suggests that reasoning under
uncertainty requires curiosity to entertain and pursue alternative ex-
planations of the data and conformity to make consistent predictions.

Discussion

We have explored variational inference, where we approximate the
intractable posterior distribution of probabilistic inference with a sim-
pler distribution. We operationalized this idea by turning the infer-
ence problem, which requires computing high-dimensional integrals,
into a tractable optimization problem. Gaussians are frequently used
as variational distributions due to their versatility and compact repre-
sentation.

Nevertheless, recall from Figure 5.4 that while the estimation error in
variational inference can be small, choosing a variational family that is
too simple can lead to a large approximation error. We have seen that
for posteriors that are multimodal or have heavy tails, Gaussians may
not provide a good approximation. In the next chapter, we will explore
alternative techniques for approximate inference that can handle more
complex posteriors.

Problems

5.1. Logistic loss.
1. Derive the gradient of ℓlog as given in Equation (5.14).
2. Show that

Hwℓlog(w
⊤x; y) = xx⊤ · σ(w⊤x) · (1− σ(w⊤x)). (5.73)

Hint: Begin by deriving the first derivative of the logistic function, and
use the chain rule of multivariate calculus,

︸Dx(︷f︷◦ g︸) = (︸Dg(x) f ︷◦︷g) · Dxg︸ (5.74)
Rn→Rm×n

Rn→Rm×k ·Rk×n

where g : Rn → Rk and f : Rk → Rm.
3. Is the logistic loss ℓlog convex in w?



variational inference 107

5.2. Gaussian process classification.

In this exercise, we will study the use of Gaussian processes for clas-
sification tasks, commonly called Gaussian process classification (GPC).
Linear logistic regression is extended to GPC by replacing the Gaus-
sian prior over weights with a GP prior on f ,

f ∼ GP(0, k), y | x, f ∼ Bern(σ( f (x))) (5.75)

where σ : R→ (0, 1) is some logistic-type function. Note that Bayesian
logistic regression is the special case where k is the linear kernel and σ
is the logistic function. This is analogous to the relationship of Bayesian
linear regression and Gaussian process regression.

In the GP regression setting of Chapter 4, yi was assumed to be a
noisy observation of f (xi). In the classification setting, we now have
that yi ∈ {−1,+1} is a binary class label and f (xi) ∈ R is a latent
value. We study the setting where σ(z) = Φ(z; 0, σ2

n) is the CDF of a
univariate Gaussian with mean 0 and variance σ2

n, also called a probit
likelihood.

To make probabilistic predictions for a query point x⋆, we first com-
pute the distribution of th∫e latent variable f ⋆,

p( f ⋆ | x1:n, y1:n, x⋆) = p( f ⋆ | x1:n, x⋆, f )p( f | x1:n, y1:n) d f (5.76) using sum rule (1.7) and product rule
(1.11), and f ⋆ ⊥ y1:n | f

where p( f | x1:n, y1:n) is the posterior over the latent variables.
1. Assuming that we can efficiently compute p( f ⋆ | x1:n, y1:n, x⋆) (ap-

proximately), describe how we can find the predictive posterior
p(y⋆ = +1 | x1:n, y1:n, x⋆).

2. The posterior over the latent variables is not a Gaussian as we used
a non-Gaussian likelihood, and hence, the integral of the latent
predictive posterior (5.76) is analytically intractable. A common
technique is to approximate the latent posterior p( f | x1:n, y1:n)
with a Gaussian using a Laplace approximation q .

= N ( f̂ , Λ−1). It
is generally not possible to obtain an analytical representation of
the mode of the Laplace approximation f̂ . Instead, f̂ is commonly
found using a second-order optimization scheme such as Newton’s
method.
(a) Find the precision matrix Λ of the Laplace approximation.

Hint: Observe that for a label yi ∈ {−1,+1}, the probability of a
correct classification given the latent value fi is p(yi | fi) = σ(yi fi),
where we use the symmetry of the probit likelihood around 0.

(b) Assume that k(x, x′) = x⊤x′ is the linear kernel (σp = 1) and
that σ is the logistic function (5.9). Show for this setting that
the matrix Λ derived in (a) is equivalent to the precision matrix



108 probabilistic artificial intelligence

Λ′ of the Laplace approximation of Bayesian logistic regression
(5.17).16 You may assume that f̂i = ŵ⊤x 16

i. This should not be surprising since —
Hint: First derive under which condition Λ and Λ′ are “equivalent”. as already mentioned — Gaussian pro-

cess classification is a generalization of
(c) Observe that the (approx∫imate) latent predictive posterior Bayesian logistic regression.

q( f ⋆ | x1:n, y1:n, x⋆ .
) = p( f ⋆ | x ⋆

1:n, x , f )q( f | x1:n, y1:n) d f

which uses the Laplace approximation of the latent posterior is
Gaussian.17 Determine its mean and variance. 17 Using the Laplace-approximated la-
Hint: Condition on the latent variables f using the laws of total ex- tent posterior, [ f ⋆ f ] are jointly Gaus-

sian. Thus, it directly follows from The-
pectation and variance. orem 1.24 that the marginal distribution

(d) Compare the prediction p( f ⋆ | x1:n, y1:n, x⋆) you obtained in over f ⋆ is also a Gaussian.
(1) (but now using the Laplace-approximated latent predictive
posterior) to the prediction σ(E f ⋆∼q[ f ⋆]). Are they identical? If
not, describe how they are different.

3. The use of the probit likelihood may seem arbitrary. Consider the
following model which may be more natural,

f ∼ GP(0, k), y = 1{ ︸f (x︷)︷+ ︸ε ≥ 0}, ε ∼ N (0, σ2
n). (5.77)

GP regression

Show that the model from Equation (5.75) using a noise-free latent
process with probit likelihood Φ(z; 0, σ2

n) is equivalent (in expecta-
tion over ε) to the model from Equation (5.77).

5.3. Jensen’s inequality.
1. Prove the finite form of Jensen’s inequality.

Theorem 5.21 (Jensen’s inequality, finite form). Let f : Rn → R be a
convex function. Suppose that x1, . . . , xk ∈ Rn and θ1, . . . , θk ≥ 0 with
θ1 + · · ·+ θk = 1. Then,

f (θ1x1 + · · ·+ θkxk) ≤ θ1 f (x1) + · · ·+ θk f (xk). (5.78)

Observe that if X is a random variable with finite support, the
above two versions of Jensen’s inequality are equivalent.

2. Show that for any discrete distribution p supported on a finite do-
main of size n, H[p] ≤ log2 n. This implies that the uniform distri-
bution has maximum entropy.

5.4. Binary cross-entropy loss.

Show that the logistic loss (5.13) is equivalent to the binary cross-
entropy loss with ŷ = σ( f̂ ). That is,

ℓlog( f̂ ; y) = ℓbce(ŷ; y). (5.79)



variational inference 109

5.5. Gibbs’ inequality.
1. Prove KL(p∥q) ≥ 0 which is also known as Gibbs’ inequality.
2. Let p and q be discrete distributions with finite identical support

A. Show that KL(p∥q) = 0 if and only if p ≡ q.
Hint: Use that if a function f : Rn → R is strictly convex and x1, . . . , xk ∈
Rn, θ1, . . . , θk ≥ 0, θ1 + · · ·+ θk = 1, we have that

f (θ1x1 + · · ·+ θkxk) = θ1 f (x1) + · · ·+ θk f (xk) (5.80)

iff x1 = · · · = xk. This is a slight adaptation of Jensen’s inequality in
finite-form, which you proved in Problem 5.3.

5.6. Maximum entropy principle.

In this exercise we will prove that the normal distribution is the distri-
bution with maximal entropy among all (univariate) distributions sup-
ported on R with fixed mean µ and variance σ2. Let g .

(x) = N (x; µ, σ2),
and f (x) be any distribution on R with mean µ and variance σ2.
1. Prove that KL( f ∥g) = H[g]−H[ f ].

Hint: Equivalently, show that H[ f ∥g] = H[g]. That is, the expected
surprise evaluated based on the Gaussian g is invariant to the true distri-
bution f .

2. Conclude that H[g] ≥ H[ f ].

5.7. Probabilistic inference as a consequence of the maximum en-
tropy principle.

Consider the family of generative models of the random vectors X in X
and Y in Y : { ∣∣ ∫ }

∆X×Y = q : X ×Y → R≥0 ∣∣ q(x, y) dx dy = 1 .
X×Y

Suppose that we observe Y to be y′, and are looking for a (new) gen-
erative model that is consiste∫nt with this information, that is,

q(y) = q(x, y) dx = δ ( ) us ng t e s
X y′ y i h um rule (1.7)

where δy′ denotes the point density at y′. The product rule (1.11)
implies that q(x, y) = δy′(y) · q(x | y), but any choice of q(x | y) is
possible.

We will derive that given any fixed generative model pX,Y, the “pos-
terior” distribution qX(·) = pX|Y(· | y′) minimizes the relative entropy
KL(qX,Y∥pX,Y) subject to the constraint Y = y′. In other words, among
all distributions qX,Y that are consistent with the observation Y = y′,
the posterior distribution qX(·) = pX|Y(· | y′) is the one with “maxi-
mum entropy”.



110 probabilistic artificial intelligence

1. Show that the optimization problem

arg min KL(qX,Y∥pX,Y)
q∈∆X×Y

subject to q(y) = δy′(y) ∀y ∈ Y

is solved by q(x, y) = δy′(y) · p(x | y).
Hint: Solve the dual problem.

2. Conclude that q(x) = p(x | y′).

5.8. KL-divergence of Gaussians.

Derive Equation (5.41).

Hint: If X ∼ N (µ, Σ) in d dimensions, then we have that for any m ∈ Rd

and A [∈ Rd×d, ]
E (X−m)⊤A(X−m) = (µ−m)⊤A(µ−m) + tr(AΣ) (5.81)

5.9. Forward vs reverse KL.
1. Consider a factored approximation q(x, y) = q(x)q(y) to a joint

distribution p(x, y). Show that to minimize the forward KL(p∥q)
we should set q(x) = p(x) and q(y) = p(y), i.e., the optimal ap-
proximation is a product of marginals.

2. Consider the following joint distribution p, where the rows repre-
sent y and the columns x:

1 2 3 4

1 1/8 1/8 0 0
2 1/8 1/8 0 0
3 0 0 1/4 0
4 0 0 0 1/4

Show that the reverse KL(q∥p) for this p has three distinct minima.
Identify those minima and evaluate KL(q∥p) at each of them.

3. What is the value of KL(q∥p) if we use the approximation q(x, y) = p(x)p(y)?

5.10. Gaussian VI vs Laplace approximation.

In this exercise, we compare the Laplace approximation from Sec-
tion 5.1 to variational inference with the variational family of Gaus-
sians,

Q .
= {q(θ) = N (θ; µ, Σ)}.

1. Let p be any distribution on R, and let q⋆ = arg minq∈Q KL(p∥q).
Show that q⋆ differs from the Laplace approximation of p.



variational inference 111

Minimizing forward-KL is typically intractable, and we have seen that
it is therefore common to minimize the reverse-KL instead:

q̃ = arg min KL(q∥p(· | Dn)).
q∈Q

2. Show that q̃ = N (µ, Σ) satisfies Equation (5.58):

0 = Eθ∼q̃[∇θ log p(y1:n, θ | x1:n)]

Σ−1 = −Eθ∼q̃[Hθ log p(y1:n, θ | x1:n)].

Hint 1: For any positive definite and symmetric matrix A, it holds that
∇A log det(A) = A−1.
Hint 2: For any function f and Gaussian p = N (µ, Σ),

∇µ Ex∼p[ f (x)] = Ex∼p[∇x f (x)]

∇ 1 (5.82)
Σ Ex∼p[ f (x)] = E

2 x∼p[Hx f (x)].

Recall the conditions satisfied by the Laplace approximation of the
posterior p(θ | D) ∝ p(y1:n, θ | x1:n) as detailed in Equation (5.57). The
Laplace approximation is fitted locally at the MAP estimate θ̂. Compar-
ing Equation (5.57) to Equation (5.58), we see that Gaussian variational
inference satisfies the conditions of the Laplace approximation on av-
erage. For more details, refer to Opper and Archambeau (2009).

5.11. Gradient of reverse-KL.

Suppose p .
= N (0, σ2

p I) and a tractable distribution described by

q .
= N (µ, diag{σ2

λ 1 , . . . , σ2
d})

where .
µ = [µ1 · · · µd] and .

λ = [µ1 · · · µd σ1 · · · σd]. Show that the
gradient of KL(qλ∥p(·)) with respect to λ is given by

∇µ KL(qλ∥p(·)) = σ[−2
p µ, and ] (5.83a)

∇[σ1 ··· σd ]
KL(qλ∥p(· σ1 − 1 . . σ

)) = d −
σ2 . 1

2
p σ1 σp σd . (5.83b)

5.12. Reparameterizable distributions.
1. Let X ∼ Unif([a, b]) for any a≤ b. That is,


pX(x)  1

b−a if x ∈ [a, b]
= (5.84)

0 otherwise.

Show that X can be reparameterized in terms of Unif([0, 1]). Hint:
You may use that for any Y ∼ Unif([a, b]) and c ∈ R,
• Y + c ∼ Unif([a + c, b + c]) and
• cY ∼ Unif([c · a, c · b]).



112 probabilistic artificial intelligence

2. Let Z ∼ N (µ, σ2) and X .
= eZ. That is, X is logarithmically nor-

mal distributed with parameters µ and σ2. Show that X can be
reparameterized in terms of N (0, 1).

3. Show that Cauchy(0, 1) can be reparameterized in terms of Unif([0, 1]).
Finally, let us apply the reparameterization trick to compute the gra-
dient of an expectation.
4. Let ReLU(z .

) = max{0, z} and w > 0. Show that

d
E

d x∼N (µ,1)ReLU(wx) = wΦ(µ)
µ

where Φ denotes the CDF of the standard normal distribution.



6
Markov Chain Monte Carlo Methods

Variational inference approximates the entire posterior distribution.
However, note that the key challenge in probabilistic inference is not
learning the posterior distribution, but using the posterior distribution
for predictions, ∫

p(y⋆ | x⋆, x1:n, y1:n) = p(y⋆ | x⋆, θ)p(θ | x1:n, y1:n) dθ. (6.1)

This integral can be interpreted as an expectation over the posterior
distribution,

= Eθ∼p(·|x1:n ,y1:n)
[p(y⋆ | x⋆, θ)]. (6.2)

Observe that the likelihood f .
(θ) = p(y⋆ | x⋆, θ) is easy to evaluate. The

difficulty lies in sampling from the posterior distribution. Assuming
we can obtain independent samples from the posterior distribution,
we can use Monte Carlo sampling to obtain an unbiased estimate of
the expectation,

≈ 1 m
∑ f (θ(i)) (6.3)

m i=1

for independent θ(i)
i∼id p(· | x1:n, y1:n). The law of large numbers (A.36)

and Hoeffding’s inequality (A.41) imply that this estimator is consis-
tent and sharply concentrated.1 1 For more details, see Appendix A.3.3.

Obtaining samples of the posterior distribution is therefore sufficient
to perform approximate inference. Recall that the difficulty of comput-
ing the posterior p exactly, was in finding the normalizing constant Z,

1
p(x) = q(x). (6.4)

Z

The joint likelihood q is typically easy to obtain. Note that q(x) is pro-
portional to the probability density associated with x, but q does not



114 probabilistic artificial intelligence

integrate to 1. Such functions are also called a finite measure. Without
normalizing q, we cannot directly sample from it.

Remark 6.1: The difficulty of sampling — even with a PDF
Even a decent approximation of Z does not yield a general effi-
cient sampling method. For example, one very common approach
to sampling is inverse transform sampling (cf. Appendix A.1.3)
which requires an (approximate) quantile function. Computing
the quantile function given an arbitrary PDF requires solving in-
tegrals over the domain of the PDF which is what we were trying
to avoid in the first place.

The key idea of Markov chain Monte Carlo methods is to construct
a Markov chain, which is efficient to simulate and has the stationary
distribution p.

6.1 Markov Chains

To start, let us revisit the fundamental theory behind Markov chains.

Definition 6.2 (Markov chain). A (finite and discrete-time) Markov chain
over the state space

S .
= {0, . . . , n− 1} (6.5)

is a stochastic process2 (Xt)t∈N0 valued in S such that the Markov prop- 2 A stochastic process is a sequence of ran-
erty is satisfied: dom variables.

X1 X2 X3 · · ·
Xt+1 ⊥ X0:t−1 | Xt. (6.6)

Figure 6.1: Directed graphical model of
a Markov chain. The random variable

Intuitively, the Markov property states that future behavior is indepen- Xt+1 is conditionally independent of the
random variables X0:t−1 given Xt.dent of past states given the present state.

Remark 6.3: Generalizations of Markov chains
One can also define continuous-state Markov chains (for example,
where states are vectors in Rd) and the results which we state for
(finite) Markov chains will generally carry over. For a survey, refer
to “General state space Markov chains and MCMC algorithms”
(Roberts and Rosenthal, 2004).

Moreover, one can also consider continuous-time Markov chains.
One example of such a continuous-space and continuous-time
Markov chain is the Wiener process (cf. Remark 6.23).

We restrict our attention to time-homogeneous Markov chains,3 which 3 That is, the transition probabilities do
not change over time.



markov chain monte carlo methods 115

can be characterized by a transition function,

p(x′ | x . ( )
) = P X ′

t+1 = x | Xt = x . (6.7)

As the state space is finite, we can describe the transition function by
the transition matrix,

 
P . p(x | x ) · · · (xn x

 1 1 p | 1)

= .. .
. . . 

. .  ∈ Rn×n
. . (6.8)

p(x1 | xn) · · · p(xn | xn)

Note that each row of P must always sum to 1. Such matrices are also
called stochastic.

The transition graph of a Markov chain is a directed graph consisting of
vertices S and weighted edges represented by the adjacency matrix P.

The current state of the Markov chain at time t is denoted by the
probability distribution qt over states S, that is, Xt ∼ qt. In the finite
setting, qt is a PMF, which is often written explicitly as the row vec-
tor qt ∈ R1×|S|. The initial state (or prior) of the Markov chain is given
as X0 ∼ q0. One iteration of the Markov chain can then be expressed
as follows: ? Problem 6.1

qt+1 = qtP. (6.9)

It is implied directly that we can write the state of the Markov chain
at time t + k as

qt+k = qtPk. (6.10)

The entry Pk(x, x′) corresponds to the probability of transitioning from
state x ∈ S to state x′ ∈ S in exactly k steps ? . We denote this entry Problem 6.2
by p(k)(x′ | x).

In the analysis of Markov chains, there are two main concepts of inter-
est: stationarity and convergence. We begin by introducing stationar-
ity.

6.1.1 Stationarity

Definition 6.4 (Stationary distribution). A distribution π is stationary
with respect to the transition function p iff

π(x) = ∑ p(x | x′)π(x′) (6.11)
x′∈S

holds for all x ∈ S. It follows from Equation (6.9) that equivalently, π
is stationary w.r.t. a transition matrix P iff

π = πP. (6.12)



116 probabilistic artificial intelligence

After entering a stationary distribution π, a Markov chain will always
remain in the stationary distribution. In particular, suppose that Xt is
distributed according to π, then for all k ≥ 0, Xt+k ∼ π.

Remark 6.5: When does a stationary distribution exist?
In general, there are Markov chains with infinitely many station-
ary distributions or no stationary distribution at all. You can find
some examples in Figure 6.2.

It can be shown that there exists a unique stationary distribution π
if the Markov chain is irreducible, that is, if every state is reach-
able from every other state with a positive probability when the
Markov chain is run for enough steps. Formally,

∀x, x′ ∈ S. ∃k ∈N. p(k)(x′ | x) > 0. (6.13)

Equivalently, a Markov chain is irreducible iff its transition graph
is strongly connected.

6.1.2 Convergence

Let us now consider Markov chains with a unique stationary distribu-
tion.4 A natural next question is whether this Markov chain converges 4 Observe that the stationary distribu-
to its stationary distribution. We say that a Markov chain converges to tion of an irreducible Markov chain must

have full support, that is, assign positive
its stationary distribution iff we have probability to every state.

lim qt = π, (6.14)
t→∞

irrespectively of the initial distribution q0.

Remark 6.6: When does a Markov chain converge?
Even if a Markov chain has a unique stationary distribution, it
does not have to converge to it. Consider example (3) in Fig-
ure 6.2. Clearly, π = ( 1

2 , 1
2 ) is the unique stationary distribution.

However, observe that if we start with a suitable initial distribu-
tion such as q0 = (1, 0), at no point in time will the probability
of all states be positive, and in particular, the chain will not con-
verge to π. Instead, the chain behaves periodically, i.e., its state
distributions are q2t = (0, 1) and q2t+1 = (1, 0) for all t ∈ N. It
turns out that if we exclude such “periodic” Markov chains, then
the remaining (irreducible) Markov chains will always converge
to their stationary distribution.



markov chain monte carlo methods 117

Formally, a Markov chain is aperiodic if for all states x ∈ S,

∃k0 ∈N. ∀k ≥ k0. p(k)(x | x) > 0. (6.15)

In words, a Markov chain is aperiodic iff for every state x, the
transition graph has a closed path from x to x with length k for all
k ∈N greater than some k0 ∈N.

This additional property leads to the concept of ergodicity.

Definition 6.7 (Ergodicity). A Markov chain is ergodic iff there exists a
t ∈N0 such that for any x, x′ ∈ S we have

p(t)(x′ | x (1) 1 1 2 1
) > 0, (6.16)

whereby p(t)(x′ | x) is the probability to reach x′ from x in exactly t 1
(2) 1 2 1

steps. Equivalent conditions are
1. that there exists some t ∈ N0 such that all entries of Pt are strictly 1

(3) 1 2
positive; and 1

2. that it is irreducible and aperiodic. 1/2
(4) 1/2 1 2

1
Example 6.8: Making a Markov chain ergodic Figure 6.2: Transition graphs of Markov
A commonly used strategy to ensure that a Markov chain is er- chains: (1) is not ergodic as its transi-

tion diagram is not strongly connected;
godic is to add “self-loops” to every vertex in the transition graph. (2) is not ergodic for the same reason; (3)
That is, to ensure that at any point in time, the Markov chain re- is irreducible but periodic and therefore

not ergodic; (4) is ergodic with station-
mains with positive probability in its current state. ary distribution π(1) = 2/3, π(2) = 1/3.

Take a (not necessarily ergodic) but irreducible Markov chain with
transition matrix P. We define the new Markov chain

P′ . 1 1
= P + I. (6.17)

2 2

It is a simple exercise to confirm that P′ is stochastic, and hence
a valid transition matrix. Also, it follows directly that P′ is irre-
ducible (as P is irreducible) and aperiodic as every vertex has a
closed path of length 1 to itself, and therefore the chain is ergodic.

Take now π to be a stationary distribution of P. We have that π
is also a stationary distribution of P′ as

1 1 1 1
πP′ = πP + πI = π + π = π. (6.18) using (6.12)

2 2 2 2

Fact 6.9 (Fundamental theorem of ergodic Markov chains, theorem 4.9
of Levin and Peres (2017)). An ergodic Markov chain has a unique station-
ary distribution π (with full support) and

lim qt = π (6.19)
t→∞



118 probabilistic artificial intelligence

irrespectively of the initial distribution q0.

This naturally suggests constructing an ergodic Markov chain such
that its stationary distribution coincides with the posterior distribu-
tion. If we then sample “sufficiently long”, Xt is drawn from a distri-
bution that is “very close” to the posterior distribution.

Remark 6.10: How quickly does a Markov chain converge?
The convergence speed of Markov chains is a rich field of research.
“Sufficiently long” and “very close” are commonly made precise
by the notions of rapidly mixing Markov chains and total variation
distance.

Definition 6.11 (Total variation distance). The total variation dis-
tance between two probability distributions µ and ν on A is de-
fined by

∥ .
µ− ν∥TV = 2 sup |µ(A)− ν(A)| . (6.20)

A⊆A

It defines the distance between µ and ν to be the maximum dif-
ference between the probabilities that µ and ν assign to the same
event.

As opposed to the KL-divergence (5.34), the total variation dis-
tance is a metric. In particular, it is symmetric and satisfies the
triangle inequality. It can be show√n that

∥µ− ν∥TV ≤ 2KL(µ∥ν) (6.21)

which is known as Pinsker’s inequality. Moreover, if µ and ν are
discrete distributions over the set S, it can be shown that

∥µ− ν∥TV = ∑ |µ(i)− ν(i)| . (6.22)
i∈S

Definition 6.12 (Mixing time). For a Markov chain with stationary
distribution π, its mixing time with respect to the total variation
distance for any ϵ > 0 is

.
τTV(ϵ) = min{t | ∀q0 : ∥qt − π∥TV ≤ ϵ}. (6.23)

Thus, the mixing time measures the time required by a Markov
chain for the distance to stationarity to be small. A Markov chain
is typically said to be rapidly mixing if for any ϵ > 0,

τTV(ϵ) ∈ O(poly(n, log(1/ϵ))). (6.24)



markov chain monte carlo methods 119

That is, a rapidly mixing Markov chain on n states needs to be
simulated for at most poly(n) steps to obtain a “good” sample
from its stationary distribution π.

You can find a thorough introduction to mixing times in chapter
4 of “Markov chains and mixing times” (Levin and Peres, 2017).
Later chapters introduce methods for showing that a Markov chain
is rapidly mixing.

6.1.3 Detailed Balance Equation

How can we confirm that the stationary distribution of a Markov chain
coincides with the posterior distribution? The detailed balance equa-
tion yields a very simple method.

Definition 6.13 (Detailed balance equation / reversibility). A Markov
chain satisfies the detailed balance equation with respect to a distribu-
tion π iff

π(x)p(x′ | x) = π(x′)p(x | x′) (6.25)

holds for any x, x′ ∈ S. A Markov chain that satisfies the detailed
balance equation with respect to π is called reversible with respect to π.

Lemma 6.14. Given a finite Markov chain, if the Markov chain is reversible
with respect to π then π is a stationary distribution.5 5 Note that reversibility of π is only a suf-

ficient condition for stationarity of π, it
Proof. Let .

π = qt. We have, is not necessary! In particular, there are
irreversible ergodic Markov chains.

qt+1(x) = ∑ p(x | x′)qt(x′) using the Markov property (6.6)
x′∈S

= ∑ p(x | x′)π(x′)
x′∈S

= ∑ p(x′ | x)π(x) using the detailed balance equation
x′∈S (6.25)

= π(x) ∑ p(x′ | x)
x′∈S

= π(x). using that ∑x′∈S p(x′ | x) = 1

That is, if we can show that the detailed balance equation (6.25) holds
for some distribution q, then we know that q is the stationary distribu-
tion of the Markov chain.

Next, reconsider our posterior distribution p(x) = 1
Z q(x) from Equa-

tion (6.4). If we substitute the posterior for π in the detailed balance
equation, we obtain

1 1
q(x)p(x′ | x) = q(x′)p(x | x′), (6.26)

Z Z



120 probabilistic artificial intelligence

or equivalently,

q(x)p(x′ | x) = q(x′)p(x | x′). (6.27)

In words, we do not need to know the true posterior p to check that
the stationary distribution of our Markov chain coincides with p, it
suffices to know the finite measure q!

6.1.4 Ergodic Theorem

If we now suppose that we can construct a Markov chain whose sta-
tionary distribution coincides with the posterior distribution — we
will see later that this is possible — it is not apparent that this allows
us to estimate expectations over the posterior distribution. Note that
although constructing such a Markov chain allows us to obtain sam-
ples from the posterior distribution, they are not independent. In fact,
due to the structure of a Markov chain, by design, they are strongly
dependent. Thus, the law of large numbers and Hoeffding’s inequality
do not apply. By itself, it is not even clear that an estimator relying on
samples from a single Markov chain will be unbiased.

Theoretically, we could simulate many Markov chains separately and
obtain one sample from each of them. This, however, is extremely
inefficient. It turns out that there is a way to generalize the (strong)
law of large numbers to Markov chains.

Theorem 6.15 (Ergodic theorem, appendix C of Levin and Peres (2017)).
Given an ergodic Markov chain (Xt)t∈N0 over a finite state space S with sta-
tionary distribution π and a function f : S→ R,

1 n 1

∑ f (x a→.s.
i) ∑ π(x) f (x) = Ex∼π [ f (x)] (6.28)

n i=1 x∈S

p
as n→ ∞ where xi ∼ Xi | xi−1.

This result is the fundamental reason for why Markov chain Monte
Carlo methods are possible. There are analogous results for continu-
ous domains. 0

t0

Note, however, that the ergodic theorem only tells us that simulating t

a single Markov chain yields an unbiased estimator. It does not tell Figure 6.3: Illustration of the “burn-in”
us anything about the rate of convergence and variance of such an time t0 of a Markov chain approximat-

estimator. The convergence rate depends on the mixing time of the ing the posterior p(y⋆ = 1 | X, y) of
Bayesian logistic regression. The true

Markov chain, which is difficult to establish in general. posterior p is shown in gray. The dis-
tribution of the Markov chain at time t is

In practice, one observes that Markov chain Monte Carlo methods have shown in red.

a so-called “burn-in” time during which the distribution of the Markov



markov chain monte carlo methods 121

chain does not yet approximate the posterior distribution well. Typi-
cally, the first t0 samples are therefore discarded,

1 T
E[ f (X)] ≈ )

T − t ∑ f (Xt). (6.29

0 t=t0+1

It is not clear in general how T and t0 should be chosen such that the
estimator is unbiased, rather they have to be tuned.

Another widely used heuristic is to first find the mode of the posterior
distribution and then start the Markov chain at that point. This tends
to increase the rate of convergence drastically, as the Markov chain
does not have to “walk to the location in the state space where most
probability mass will be located”.

6.2 Elementary Sampling Methods

We will now examine methods for constructing and sampling from a
Markov chain with the goal of approximating samples from the pos-
terior distribution p. Note that in this setting the state space of the
Markov chain is Rn and a single state at time t is described by the
random vector X .

= [X1, . . . , Xn].

6.2.1 Metropolis-Hastings Algorithm

Suppose we are given a proposal distribution r(x′ | x) which, given
we are in state x, proposes a new state x′. Metropolis and Hastings
showed that using the acceptance d{istribution Bern(α}(x′ | x)) where

x′)
α(x′ | (x′)r(x |

x . p
) = min{1, (6.30)

p(x)r(x′ | x)}
q(x′)r(x | x′)

= min 1, (6.31) imilarly to the detailed balance
q(x)r(x′ | s

x) equation, the normalizing constant Z
cancels

to decide whether to follow the proposal yields a Markov chain with
stationary distribution p(x) = 1

Z q(x).

Algorithm 6.16: Metropolis-Hastings algorithm
1 initialize x ∈ Rn

2 for t = 1 to T do
3 sample x′ ∼ r(x′ | x)
4 sample u ∼ Unif([0, 1])
5 if u ≤ α(x′ | x) then update x← x′
6 else update x← x



122 probabilistic artificial intelligence

Intuitively, the acceptance distribution corrects for the bias in the pro-
posal distribution. That is, if the proposal distribution r is likely to
propose states with low probability under p, the acceptance distri-
bution will reject these proposals frequently. The following theorem
formalizes this intuition.

Theorem 6.17 (Metropolis-Hastings theorem). Given an arbitrary pro-
posal distribution r whose support includes the support of q, the stationary
distribution of the Markov chain simulated by the Metropolis-Hastings algo-
rithm is p(x) = 1

Z q(x).

Proof. First, let us define the transition probabilities of the Markov
chain. The probability of transitioning from a state x to a state x′ is
given by r(x′ | x)α(x′ | x) if x ̸= x′ and the probability of proposing to
remain in state x, r(x | x), plus the probability of denying the proposal,
otherwise. 

p(x′ | x) r(x′ | x)α(x′ | x) if x ̸= x′
=

r(x | x) + ∑x′′ ̸=x r(x′′ | x)(1− α(x′′ | x)) otherwise.
(6.32)

We will show that the stationary distribution is p by showing that p
satisfies the detailed balance equation (6.25). Let us fix arbitrary states
x and x′. First, observe that if x = x′, then the detailed balance equa-
tion is trivially satisfied. Without loss of generality we assume

q(x′)r(x | x′
α(x | x′) = 1, α(x′ | )

x) = .
q(x)r(x′ | x)

For x ̸= x′, we then have,

1
p(x) · p(x′ | x) = q(x)p(x′ | x) using the definition of the distribution p

Z
1

= q(x)r(x′ | x)α(x′ | x) using the transition probabilities of the
Z Markov chain
1 q(x′)r(x | x′)

= q(x)r(x′ | x) using the definition of the acceptance
Z q(x)r(x′ | x) distribution α
1

= q(x′)r(x | x′)
Z
1

= q(x′)r(x | x′)α(x | x′) using the definition of the acceptance
Z distribution α
1

= q(x′)p(x | x′) using the transition probabilities of the
Z Markov chain

= p(x′) · p(x | x′). using the definition of the distribution p

Note that by the fundamental theorem of ergodic Markov chains (6.19),
for convergence to the stationary distribution, it is sufficient for the



markov chain monte carlo methods 123

Markov chain to be ergodic. Ergodicity follows immediately when
the transition probabilities p(· | x) have full support. For example,
if the proposal distribution r(· | x) has full support, the full support
of p(· | x) follows immediately from Equation (6.32). The rate of con-
vergence of Metropolis-Hastings depends strongly on the choice of the
proposal distribution, and we will explore different choices of proposal
distribution in the following.

6.2.2 Gibbs Sampling

A popular example of a Metropolis-Hastings algorithm is Gibbs sam-
pling as presented in Algorithm 6.18.

Algorithm 6.18: Gibbs sampling
1 initialize x = [x1, . . . , xn] ∈ Rn

2 for t = 1 to T do
3 pick a variable i uniformly at random from {1, . . . , n}

.
4 set x−i = [x1, . . . , xi−1, xi+1, . . . , xn]
5 update xi by sampling according to the posterior distribution

p(xi | x−i)

Intuitively, by re-sampling single coordinates according to the pos-
terior distribution given the other coordinates, Gibbs sampling finds
states that are successively “more” likely. Selecting the index i uni-
formly at random ensures that the underlying Markov chain is ergodic
provided the conditional distributions p(· | x−i) have full support.

Theorem 6.19 (Gibbs sampling as Metropolis-Hastings). Gibbs sam-
pling is a Metropolis-Hastings algorithm. For any fixed i ∈ [n], it has
proposal distribution

 ′ ′
r . ( | i x d f e

i(x′ | x = p x ′
i x − ) i f rs from x only in entry i

) (6.33)
0 otherwise

and acceptance distribution .
αi(x′ | x) = 1.

Proof. We show that αi(x′ | x) = 1 follows from the definition of an
acceptance distribution in Metropolis-Hastings (6.31) and the choice of
proposal distribution (6.33).

By (6.31), { }
p(x′)r (x | x′)

αi(x′ | x) = min 1, i
p(x)ri(x′ | x)



124 probabilistic artificial intelligence

Note that p(x) = p(xi, x−i) = p(xi | x−i)p(x−i) using the product rule
(1.11). Therefore, { }

p(x′ | x′ x′ ) (x
= mi 1, i −i)p( −i ri | x′)

n{ p(xi | x−i)p(x−i)ri(x′ | x) }
p(x′ | x′ )p(x′ )

= min 1, i −i −i p(xi | x−i)

{ p(xi | x
p(x′

}−i)p(x−i)p(x′i | x′
using the proposal distribution (6.33)

−i)

= min 1, −i)
p(x−i)

= 1. using that p(x′−i) = p(x−i)

If the index i is chosen uniformly at random as in Algorithm 6.18, then
the proposal distribution is r(x′ | x) = 1

n ∑n
i=1 ri(x′ | x), which analo-

gously to Theor{em 6.19 has the a}ssociated{acceptance distribution
p(x′)r(x | x′) ∑n p(x′)r (x | x′

}
α(x′ | )

x) = min 1, = min 1, i=1 i
p(x)r(x′ | ∑n = 1.

x) i=1 p(x)ri(x′ | x)

Corollary 6.20 (Convergence of Gibbs sampling). As Gibbs sampling
is a specific example of an MH-algorithm, the stationary distribution of the
simulated Markov chain is p(x).

Note that for the proposals of Gibbs sampling, we have

p(xi |
p(x

x i, x−i) q(x
−i) = = i, x−i) . (6.34) using the definition of condition

∑x p(x , x
i i −i) ∑x q(x x

i i, −i) probability (1.8) and the sum rule (1.7)

Under many models, this probability can be efficiently evaluated due
to the conditioning on the remaining coordinates x−i. If Xi has finite
support, the normalizer can be computed exactly.

6.3 Sampling using Gradients

In this section, we discuss more advanced sampling methods. The
main idea that we will study is the interpretation of sampling as an
optimization problem. We will build towards an optimization view
of sampling step-by-step, and first introduce what is commonly called
the “energy” of a distribution.

6.3.1 Gibbs Distributions

Gibbs distributions are a special class of distributions that are widely
used in machine learning, and which are characterized by an energy.

Definition 6.21 (Gibbs distribution). Formally, a Gibbs distribution (also
called a Boltzmann distribution) is a continuous distribution p whose
PDF is of the form

1
p(x) = exp(− f (x)). (6.35)

Z



markov chain monte carlo methods 125

f : Rn → R is also called an energy function. When the energy func-
tion f is convex, its Gibbs distribution is called log-concave.

A useful property is that Gibbs distributions always have full support.6 6 This can easily be seen as exp(·) > 0.
It is often easier to reason about “energies” rather than probabilities as
they are neither restricted to be non-negative nor do they have to inte-
grate to 1. Note that the Gibbs distribution belongs to the exponential
family (5.48) with sufficient statistic − f (x).

Observe that the posterior distribution can always be interpreted as a
Gibbs distribution as long as prior and likelihood have full support,

1
p(θ | x1:n, y1:n) = p(θ)p(y , θ u i

Z 1:n | x1:n ) s ng Bayes’ rule (1.45)

1
= exp(−[− log p(θ)− log p(y

Z 1:n | x1:n, θ)]). (6.36)

Thus, defining the energy function

f .
(θ) = − log p(θ)− log p(y1:n | x1:n, θ) (6.37)

n
= − log p(θ)−∑ log p(yi | xi, θ), (6.38)

i=1

yields

1
p(θ | x1:n, y1:n) = exp(− f (θ)). (6.39)

Z
Note that f coincides with the loss function used for MAP estimation
(1.62). For a noninformative prior, the regularization term vanishes
and the energy reduces to the negative log-likelihood ℓnll(θ;D) (i.e.,
the loss function of maximum likelihood estimation (1.57)).

Using that the posterior is a Gibbs distribution, we can rewrite the
acceptance distribution of {Metropolis-Hastings,

r(x | x′
}

α(x′ | )
x) = min 1, ( (

r(x′ | exp f x)− f (x′)) . (6.40) this is obtained by substituting the PDF
x) of a Gibbs distribution for the posterior

Example 6.22: Metropolis-Hastings with Gaussian proposals
Let us consider Metropolis-Hastings with the Gaussian proposal
distribution,

r(x′ | x .
) = N (x′; x, τI). (6.41)

Due to the symmetry of Gaussians, we have

r(x | x′) N (x; x′, τI)
= N = 1.

r(x′ | x) (x′; x, τI)



126 probabilistic artificial intelligence

Hence, the acceptance distrib{ution is defined by

α(x′
}

| x) = min 1, exp( f (x)− f (x′)) . (6.42)

Intuitively, when a state with lower energy is proposed, that is
f (x′) ≤ f (x), then the proposal will always be accepted. In con- f (θ)

trast, if the energy of the proposed state is higher, the accep-
tance probability decreases exponentially in the difference of en-
ergies f (x)− f (x′). Thus, Metropolis-Hastings minimizes the en-
ergy function, which corresponds to minimizing the negative log-
likelihood and negative log-prior. The variance in the proposals τ
helps in getting around local optima, but the search direction is
uniformly random (i.e., “uninformed”).

θ0

θ

6.3.2 From Energy to Surprise (and back) Figure 6.4: Metropolis-Hastings and
Langevin dynamics minimize the en-
ergy function f (θ) shown in blue. Sup-

Energy-based models are a well-known class of models in machine pose we start at the black dot θ0, then
learning where an energy function f is learned from data. These en- the black and red arrows denote pos-
ergy functions do not need to originate from a probability distribution, sible subsequent samples. Metropolis-

Hastings uses an “uninformed” search
yet they induce a probability distribution via their Gibbs distribution direction, whereas Langevin dynamics
p(x) ∝ exp(− f (x)). As we will see in Problem 6.7, this Gibbs distribu- uses the gradient of f (θ) to make “more

promising” proposals. The random pro-
tion is the associated maximum entropy distribution. Observe that the posals help get past local optima.
surprise about x under this distribution is given by

S[p(x)] = f (x) + log Z. (6.43)

That is, up to a constant shift, the energy of x coincides with the sur-
prise about x. Energies are therefore sufficient for comparing the “like-
lihood” of points, and they do not require normalization.7 7 Intuitively, an energy can be used to

compare the “likelihood” of two points x
What kind of energies could we use? In Section 6.3.1, we discussed and x′ whereas the probability x makes

the use of the negative log-posterior or negative log-likelihood as en- a statement about the “likelihood” of x
relative to all other points.

ergies. In general, any loss function ℓ(x) can be thought of as an
energy function with an associated maximum entropy distribution
p(x) ∝ exp(−ℓ(x)).

6.3.3 Langevin Dynamics

Until now, we have looked at Metropolis-Hastings algorithms with
proposal distributions that do not explicitly take into account the cur-
vature of the energy function around the current state. Langevin dy-
namics adapts the Gaussian proposals of the Metropolis-Hastings al-
gorithm we have seen in Example 6.22 to search the state space in an
“informed” direction. The simple idea is to bias the sampling towards
states with lower energy, thereby making it more likely that a proposal
is accepted.



markov chain monte carlo methods 127

A natural idea is to shift the proposal distribution perpendicularly to
the gradient of the energy function. This yields the following proposal
distribution,

r(x′ | x) = N (x′; x− ηt∇ f (x), 2ηt I). (6.44)

The resulting variant of Metropolis-Hastings is known as the Metropo-
lis adjusted Langevin algorithm (MALA) or Langevin Monte Carlo (LMC).
It can be shown that, as ηt → 0, we have for the acceptance probabil-
ity α(x′ | x) → 1 using that the acceptance probability is 1 if x′ = x.
Hence, the Metropolis-Hastings acceptance step can be omitted once
the rejection probability becomes negligible. The algorithm which al-
ways accepts the proposal of Equation (6.44) is known as the unadjusted
Langevin algorithm (ULA).

Observe that if the stationary d(istribution is the posterior dist)ribution

1 n
p(θ | x1:n, y1:n) = exp log p(θ) + log p

Z ︸ ∑︷︷ (yi | xi, θ)
i=1 ︸
− f (θ)

with energy f as we discussed in Section 6.3.1, then the proposal θ′ of
MALA/LMC can be equivalently formulated as

θ′ ← θ− ηt(∇ f (θ) + ε )
n (6.45)

= θ+ ηt ∇ log p(θ) + ∑ ∇ log p(yi | xi, θ) + ε
i=1

where ε ∼ N (0, 2ηt I).

Remark 6.23: ULA is a discretized diffusion
The unadjusted Langevin algorithm can be seen as a discretiza-
tion of Langevin dynamics, which is a continuous-time stochastic
process with a drift and with random stationary and independent
Gaussian increments. The randomness is modeled by a Wiener
process.

Definition 6.24 (Wiener process). The Wiener process (also known
as Brownian motion) is a sequence of random vectors {Wt}t≥0 such
that
1. W0 = 0,
2. with probability 1, Wt is continuous in t,
3. the process has independent increments,8 and 8 That is, the “future” increments
4. Wt+u −Wt ∼ N (0, uI). Wt+u −Wt for u ≥ 0 are independent

of past values Ws for s < t.

Consider the continuous-time stochastic process θ defined by the



128 probabilistic artificial intelligence

stochastic differential equation (SDE)

dθt = −︸ ︷︷ ︸ √
∇ f (θt) dt+ ︸ 2︷d︷W︸t, (6.46)

drift noise

Such a stochastic process is also called a diffusion (process) and
Equation (6.46) specifically is called Langevin dynamics. Here, the
first term is called the “drift” of the process, and the second term
is called its “noise”. Note that if the noise term is zero then Equa-
tion (6.46) is simply an ordinary differential equation (ODE).

A diffusion can be discretized using the Euler-Maruyama method
(also called “forward Euler”) to obtain a discrete approximation
θk ≈ θ(τk) where τk denotes the k-th time step. Choosing the time
steps such that ∆t .

k = τk+1 − τk = ηk yields the approximation
√

θk+1 = θk −∇ f (θk)∆tk + 2 ∆Wk (6.47)

where ∆W .
k = Wτk W

+1 − τk ∼ N (0, ∆tk I). Observe that this co-
incides with the update rule of Langevin dynamics from Equa-
tion (6.45).

The appearance of drift and noise is closely related to the principle
of curiosity and conformity, we encountered in the previous chapter on
variational inference. The noise term (also called the diffusion) leads
to exploration of the state space (i.e., curiosity about alternative expla-
nations of the data), whereas the drift term (also called the distillation)
leads to minimizing the energy or loss (i.e., conformity to the data).
Interestingly, this same principle appears in both variational inference
and MCMC, two very different approaches to approximate probabilis-
tic inference. In the remainder of this manuscript we will find this to
be a reoccurring theme.

For log-concave distributions, the mixing time of the Markov chain
underlying Langevin dynamics can be shown to be polynomial in the
dimension n (Vempala and Wibisono, 2019). You will prove this result
for strongly log-concave distributions in ? and see that the analysis is Problem 6.9
analogous to the canonical convergence analysis of classical optimiza-
tion schemes.

6.3.4 Stochastic Gradient Langevin Dynamics

Note that computing the gradient of the energy function, which corre-
sponds to computing exact gradients of the log-prior and log-likelihood,
in every step can be expensive. The proposal step of MALA/LMC can
be made more efficient by approximating the gradient with an unbi-



markov chain monte carlo methods 129

ased gradient estimate, leading to stochastic gradient Langevin dynamics
(SGLD) shown in Algorithm 6.25 (Welling and Teh, 2011). Observe that
SGLD (6.48) differs from MALA/LMC (6.45) only by using a sample-
based approximation of the gradient.

Algorithm 6.25: Stochastic gradient Langevin dynamics, SGLD
1 initialize θ
2 for t = 1 to T do
3 sample i1, . . . , im ∼ Unif({1, . . . , n}) independently
4 sample ε ∼(N (0, 2ηt I) )
5 θ← θ+ ηt ∇ log p(θ) + n

m ∑m
j=1 ∇ log p(yij | xij , θ) + ε // (6.48)

Intuitively, in the initial phase of the algorithm, the stochastic gra-
dient term dominates, and therefore, SGLD corresponds to a variant
of stochastic gradient ascent. In the later phase, the update rule is
dominated by the injected noise ε, and will effectively be Langevin
dynamics. SGLD transitions smoothly between the two phases.

Under additional assumptions, SGLD is guaranteed to converge to the
posterior distribution for decreasing learning rates ηt = Θ(t−1/3) (Ra-
ginsky et al., 2017; Xu et al., 2018). SGLD does not use the acceptance
step from Metropolis-Hastings as asymptotically, SGLD corresponds
to Langevin dynamics and the Metropolis-Hastings rejection probabil-
ity goes to zero for a decreasing learning rate.

6.3.5 Hamiltonian Monte Carlo

As MALA and SGLD can be seen as a sampling-based analogue of
GD and SGD, a similar analogue for (stochastic) gradient descent with
momentum is the (stochastic gradient) Hamiltonian Monte Carlo (HMC)
algorithm, which we discuss in the following (Duane et al., 1987; Chen
et al., 2014).

We have seen that if we want to sample from a distribution

p(x) ∝ exp(− f (x))

with energy function f , we can construct a Markov chain whose dis-
tribution converges to p. We have also seen that for this approach to
work, the chain must move through all areas of significant probability
with reasonable speed.

If one is faced with a distribution p which is multimodal (i.e., that
has several “peaks”), one has to ensure that the chain will explore all
modes, and can therefore “jump between different areas of the space”.



130 probabilistic artificial intelligence

Figure 6.5: A commutative diagram of
HMC SG-HMC sampling and optimization algorithms.

Langevin dynamics (LD) is the non-
sampling stochastic variant of SGLD.

GD / Mom. SGD / Mom.
momentum

LD SGLD

stochastic optimization
GD SGD

So in general, local updates are doomed to fail. Methods such as
Metropolis-Hastings with Gaussian proposals, or even Langevin Monte
Carlo might face this issue, as they do not jump to distant areas of the
state space with significant acceptance probability. It will therefore
take a long time to move from one peak to another.

The HMC algorithm is an instance of Metropolis-Hastings which uses
momentum to propose distant points that conserve energy, with high
acceptance probability. The general idea of HMC is to lift samples x
to a higher-order space by considering an auxiliary variable y with
the same dimension as x. We also lift the distribution p to a distribu-
tion on the (x, y)-space by defining a distribution p(y | x) and setting
p(x, y .

) = p(y | x)p(x). It is common to pick p(y | x) to be a Gaussian
with zero mean and variance m(I. Hence, )

1
p(x, y) ∝ exp − ∥y∥2 − f (x) . (6.49)

2m 2

Physicists might recognize the above as the canonical distribution of a
Newtonian system if one takes x as the position and y as the momen-
tum. H .

(x, y) = 1
2m ∥y∥

2
2 + f (x) is called the Hamiltonian. HMC then

takes a step in this higher-order space according to the Hamiltonian
dynamics,9 9 That is, HMC follows the trajectory of

these dynamics for some time.
dx dy

= ∇ −∇x H, (6.50)
dt y H, =

dt

reaching some new point (x′, y′) and projecting back to the state space
by selecting x′ as the new sample. This is illustrated in Figure 6.6.
In the next iteration, we resample the momentum y′ ∼ p(· | x′) and
repeat the procedure.

In an implementation of this algorithm, one has to solve Equation (6.50)
numerically rather than exactly. Typically, this is done using the Leapfrog



markov chain monte carlo methods 131

y Figure 6.6: Illustration of Hamiltonian
Monte Carlo. Shown is the contour plot
of a distribution p, which is a mixture of

move acc. to Hamiltonian dynamics
2 two Gaussians, in the (x, y)-space.

First, the initial point in the state
space is lifted to the (x, y)-space. Then,

1 we move according to Hamiltonian dy-
namics and finally project back onto the

project lift state space.
start

0
new proposal

−1

−2

−3 −2 −1 0 1 2 3
x

method, which for a step size τ computes

y(t + τ/2) = y(t)− τ∇x f (x(t)) (6.51a)
2
τ

x(t + τ) = x(t) + y(t + τ/2) (6.51b)
m

y(t + τ) = y(t + τ/2)− τ∇ (x t + τ ). ( .
2 x f ( ) 6 51c)

Then, one repeats this procedure L times to arrive at a point (x′, y′).
To correct for the resulting discretization error, the proposal is either
accepted or rejected in a final Metropolis-Hastings acceptance step. If
the proposal distribution is symmetric (which we will confirm in a
moment), the acceptance probability is

α((x′, y′ .
) | (x, y)) = min{1, exp(H(x′, y′)− H(x, y))}. (6.52)

It follows that p(x, y) is the stationary distribution of the Markov chain
underlying HMC. Due to the independence of x and y, this also im-
plies that the projection to x yields a Markov chain with stationary
distribution p(x).

So why is the proposal distribution symmetric? This follows from
the time-reversibility of Hamiltonian dynamics. It is straightforward
to check that the dynamics from Equation (6.50) are identical if we
replace t with −t and y with −y. Intuitively, unlike the position x,
the momentum y is reversed when time is reversed as it depends on
the velocity which is the time-derivative of the position.10 In simpler 10 The momentum of the i-th coordinate
terms, time-reversibility states that if we observe the evolution of a sys- is yi = mivi where mi is the mass and

vi =
dxi
dt is the velocity.

tem (e.g., two billiard balls colliding), we cannot distinguish whether



132 probabilistic artificial intelligence

we are observing the system evolve forward or backward in time. The
Leapfrog method maintains the time-reversibility of the dynamics.

Symmetry of the proposal distribution is ensured by proposing the
point (x′,−y′).11 Intuitively, this is simply to ensure that the system is 11 More formally, the proposal distribu-
run backward in time as often as it is run forward in time. Recall that tion is the Dirac delta at (x′,−y′).

the momentum is resampled before each iteration (i.e., the proposed
momentum is “discarded”) and observe that p(x′,−y′) = p(x′, y′),12 12 We use here that p(y | x) was chosen
so we can safely disregard the direction of time when computing the to be symmetric around zero.

acceptance probability in Equation (6.52).

Discussion

To summarize, Markov chain Monte Carlo methods use a Markov
chain to approximately sample from an intractable distribution. Note
that unlike for variational inference, the convergence of many methods
can be guaranteed. Moreover, for log-concave distributions (e.g., with
Bayesian logistic regression), the underlying Markov chain converges
quickly to the stationary distribution. Methods such as Langevin dy-
namics and Hamiltonian Monte Carlo aim to accelerate mixing by
proposing points with a higher acceptance probability than Metropolis-
Hastings with “undirected” Gaussian proposals. Nevertheless, in gen-
eral, the convergence (mixing time) may be slow, meaning that, in
practice, accuracy and efficiency have to be traded.

Optional Readings
• Ma, Chen, Jin, Flammarion, and Jordan (2019).

Sampling can be faster than optimization.
• Teh, Thiery, and Vollmer (2016).

Consistency and fluctuations for stochastic gradient Langevin dy-
namics.

• Chen, Fox, and Guestrin (2014).
Stochastic gradient hamiltonian monte carlo.

Problems

6.1. Markov chain update.

Prove Equation (6.9), i.e., that one iteration of the Markov chain can be
expressed as qt+1 = qtP.

6.2. k-step transitions.

Prove that the entry Pk(x, x′) corresponds to the probability of transi-
tioning from state x ∈ S to state x′ ∈ S in exactly k steps.



markov chain monte carlo methods 133

6.3. Finding stationary distributions.

A news station classifies each day as “good”, “fair”, or “poor” based
on its daily ratings which fluctuate with what is occurring in the news.
Moreover, the following table shows the probabilistic relationship be-
tween the type of the current day and the probability of the type of the
next day conditioned on the type of the current day.

next day
good fair poor

good 0.60 0.30 0.10
current day fair 0.50 0.25 0.25

poor 0.20 0.40 0.40

In the long run, what percentage of news days will be classified as
“good”?

6.4. Example of Metropolis-Hastings.

Consider the state space {0, 1}n of binary strings having length n. Let
the proposal distribution be r(x′ | x) = 1/n if x′ differs from x in
exactly one bit and r(x′ | x) = 0 otherwise. Suppose we desire a sta-
tionary distribution p for which p(x) is proportional to the number of
ones that occur in the bit string x. For example, in the long run, a ran-
dom walk should visit a string having five 1s five times as often as it
visits a string having only a single 1. Provide a general formula for the
acceptance probability α(x′ | x) that would be used if we were to ob-
tain the desired stationary distribution used the Metropolis-Hastings
algorithm.

6.5. Practical examples of Gibbs sampling.

In this exercise, we look at some examples where Gibbs sampling is
useful.
1. Consider the d(istr)ibution

. n
p(x, y) = yx+α−1(1− y)n−x+β−1, x ∈ [n], y ∈ [0, 1].

x Gamma distribution The PDF of the
gamma distribution Gamma(α, β) is de-

Convince yourself that it is hard to sample directly from p and fined as

prove that it is an easy task if one uses Gibbs sampling. That is, Gamma(x; α, β) ∝ xα−1e−βx , x ∈ R>0.

show that the conditional distributions p(x | y) and p(y | x) are A random variable X ∼ Gamma(α, β)

easy to sample from. measures the waiting time until α > 0
events occur in a Poisson process with

Hint: Take a look at the Beta distribution (1.50). rate β > 0. In particular, when α = 1
2. Consider the following generative model p(µ, λ, x1:n) given by the then the gamma distribution coincides

likelihood x i with the exponential distribution with
1:n | µ, ∼idλ N (µ, λ−1) and the independent priors rate β.

µ ∼ N (µ0, λ−1
0 ) and λ ∼ Gamma(α, β).



134 probabilistic artificial intelligence

We would like to sample from the posterior p(µ, λ | x1:n). Show
that

µ | λ, x1:n ∼ N (mλ, l−1
λ ) and λ | µ, x1:n ∼ Gamma(aµ, bµ),

and derive mλ, lλ, aµ, bµ. Such a prior is called a semi-conjugate prior
to the likelihood, as the prior on µ is conjugate for any fixed value
of λ and vice-versa.

3. Let us assume that x1:n | α, c i∼id Pareto(α, c) and assume the im-
proper prior p(α, c) ∝ 1{α, c > 0}which corresponds to a noninfor-
mative prior. Derive the posterior p(α, c | x1:n). Then, also derive
the conditional distributions p(α | c, x1:n) and p(c | α, x1:n), and
observe that they correspond to known distributions / are easy to
sample from.

6.6. Energy function of Bayesian logistic regression.

Recall from Equation (5.12) that the energy function of Bayesian logis-
tic regression is

n
f (w) = λ ∥w∥2

2 + ∑ log(1 + exp(−y ⊤
iw xi)), (6.53)

i=1

which coincided with the standard optimization objective of (regular-
ized) logistic regression.

Show that the posterior distribution of Bayesian logistic regression is
log-concave.

6.7. Maximum entropy property of Gibbs distribution.
1. Let X be a random variable supported on the finite set T ⊂ R.13 13 The same result can be shown to hold

Show that the Gibbs distribution with energy function 1
T f (x) for for arbitrary compact subsets.

some temperature scalar T ∈ R is the distribution with maximum
entropy of all distributions supported on T that satisfy the con-
straint E[ f (X)] < ∞.
Hint: Solve the dual problem (analogously to Problem 5.7).

2. What happens for T → {0, ∞}?
6.8. Energy reduction of Gibbs sampling.

Let p(x) be a probability density over Rd, which we want to sample
from. Assume that p is a Gibbs distribution with energy function
f : Rd → R.

In this exercise, we will study a single round of Gibbs sampling with
initial state x and final state x′ where


x′j x′i if j = i

=
xj otherwise



markov chain monte carlo methods 135

for some fixed index i and x′i ∼ p(· | x−i).

Show that [ ]
Ex′ )− S[p(x )

i∼p(·|x− (x′) ≤ f (x
i)

f i | x−i)] + H[p(· | x−i)]. (6.54

That is, the energy is expected to decrease if the surprise of xi given
x−i is larger than the expected surprise of the new x′i given x−i, i.e.,
S[p(xi | x−i)] ≥ H[p(· | x−i)].

Hint: Recall the framing of Gibbs sampling as a variant of Metropolis-Hastings
and relate this to the acceptance distribution of Metropolis-Hastings when p
is a Gibbs distribution.

6.9. Mixing time of Langevin dynamics.

In this exercise, we will show that for certain Gibbs distributions,
p(θ) ∝ exp(− f (θ)), Langevin dynamics is rapidly mixing. To do this,
we will observe that Langevin dynamics can be seen as a continuous-
time optimization algorithm in the space of distributions.

First, we consider a simpler and more widely-known optimization al-
gorithm, namely the gradient flow

dxt = −∇ f (xt) dt. (6.55)

Note that gradient descent is simply the discrete-time approximation
of gradient flow just as ULA is the discrete-time approximation of
Langevin dynamics. In the analysis of ODEs such as the gradient flow,
so-called Lyapunov functions are commonly used to prove convergence
of xt to a fixed point (also called an equilibrium).

Let us assume that f is α-strongly convex for some α > 0, that is,
α

f (y) ≥ f (x) +∇ f (x)⊤(y− x) + ∥y− x∥2
2 ∀x, y ∈ Rn. (6.56)

2
In words, f is lower bounded by a quadratic function with curvature α.
Moreover, assume w.l.o.g. that f minimized at f (0) = 0.14 14 This can always be achieved by shift-

. Show that f satisfies the Polyak-Łojasiewicz (PL) inequality, i.e., ing the coordinate system and subtract-
1

ing a constant from f .
1

f (x) ≤ ∥∇ f (x)∥2 ∀x ∈ Rn. (6.57)
2α 2

2. Prove d
dt f (xt) ≤ −2α f (xt).

Thus, 0 is the fixed point of Equation (6.55) and the Lyapunov func-
tion f is monotonically decreasing along the trajectory of xt. We
recall Grönwall’s inequality which states that for any real-valued con-
tinuous functions g(t) and β(t) on the interval [0, T] ⊂ R such that
d
dt g(t) ≤ β(t)g(t) for all t ∈ [0(, T∫] we have

t )
g(t) ≤ g(0) exp β(s) ds ∀t ∈ [0, T]. (6.58)

0



136 probabilistic artificial intelligence

3. Conclude that f (xt) ≤ e−2αt f (x0).

Now that we have proven the convergence of gradient flow using f as
Lyapunov function, we will follow the same template to prove the con-
vergence of Langevin dynamics to the distribution p(θ) ∝ exp(− f (θ)).
We will use that the evolution of {θt}t≥0 following the Langevin dy-
namics (6.46) is equivalently characterized by their densities {qt}t≥0
following the Fokker-Planck equation

∂qt
= ∇ · (qt∇ f ) + ∆qt. (6.59)

∂t
Here, ∇· and ∆ are the divergence and Laplacian operators, respec-
tively.15 Intuitively, the first term of the Fokker-Planck equation cor- 15 For ease of notation, we omit the ex-
responds to the drift and its second term corresponds to the diffu- plicit dependence of qt, p, and f on θ.

sion (i.e., the Gaussian noise).

Remark 6.26: Intuition on vector calculus
Recall that the divergence ∇ · F of a vector field F measures the
change of volume under the flow of F. That is, if in the small
neighborhood of a point x, F points towards x, then the divergence
at x is negative as the volume shrinks. If F points away from x,
then the divergence at x is positive as the volume increases.
The Laplacian ∆φ = ∇ · (∇φ) of a scalar field φ can be under-
stood intuitively as measuring “heat dissipation”. That is, if φ(x)
is smaller than the average value of φ in a small neighborhood of
x, then the Laplacian at x is positive.

Regarding the Fokker-Planck equation (6.59), the second term ∆qt
can therefore be understood as locally dissipating the probability
mass of qt (which is due to the diffusion term in the SDE). On the
other hand, the term ∇ · (qt∇ f ) can be understood as a Laplacian
of f “weighted” by qt. Intuitively, the vector field ∇ f moves flow
in the direction of high energy, and hence, its divergence is larger
in regions of lower energy and smaller in regions of higher energy.
This term therefore corresponds to a drift from regions of high
energy to regions of low energy.

4. Show that ∆qt = ∇ · (qt∇ log qt), implying that the Fokker-Planck
equation simplifies to ( )

∂qt
= ∇ · q

qt∇ log t . (6.60)
∂t p

Hint: The Laplacian of a scalar field φ is ∆ .
φ = ∇ · (∇φ).

Observe that the Fokker-Planck equation already implies that p is in-
deed a stationary distribution, as if qt = p then ∂qt

∂t = 0. Moreover, note



markov chain monte carlo methods 137

the similarity of the integrand of KL(qt∥p), qt log qt
p , to Equation (6.60).

We will therefore use the KL-divergence as Lyapunov function.
5. Prove d

dt KL(qt∥p) = −J(qt∥p). H[e∥re,

J . ∥ ∥ ]
qt(θ)∥

(qt∥p) = E ∥
θ∼q ∥

t ∥ ∥2
∇ log (6.61)

p(θ) 2

denotes the relative Fisher information of p with respect to qt. Hint:
For any distribut∫ion q on Rn, ∫

(∇ · qF)φ dx = − q ∇φ · F dx (6.62)
Rn Rn

follows for any vector field F and scalar field φ from the divergence theo-
rem and the product rule of the divergence operator.

Thus, the relative Fisher information can be seen as the negated time-
derivative of the KL-divergence, and as J(qt∥p) ≥ 0 it follows that the
KL-divergence is decreasing along the trajectory.

The log-Sobolev inequality (LSI) is satisfied by a distribution p with a
constant α > 0 if for all q:

KL(q∥p) ≤ 1
J(q∥p). (6.63)

2α

It is a classical result that if f is α-strongly convex then p satisfies the
LSI with constant α (Bakry and Émery, 2006).
6. Show that if f is α-strongly convex for some α > 0 (we say that p

is “strongly log-concave”), then KL(qt∥p) ≤ e−2αtKL(q0∥p).
7. Conclude that under the same assumption on f , Langevin dynam-

ics is rapidly mixing, i.e., τTV(ϵ) ∈ O(poly(n, log(1/ϵ))).
To summarize, we have seen that Langevin dynamics is an optimiza-
tion scheme in the space of distributions, and that its convergence can
be analyzed analogously to classical optimization schemes. Notably,
in this exercise we have studied continuous-time Langevin dynamics.
Convergence guarantees for discrete-time approximations can be de-
rived using the same techniques. If this interests you, refer to “Rapid
convergence of the unadjusted Langevin algorithm: Isoperimetry suf-
fices” (Vempala and Wibisono, 2019).

6.10. Hamiltonian Monte Carlo.
1. Prove that if the dynamics are solved exactly (as opposed to numer-

ically using the Leapfrog method), then the acceptance probability
of the MH-step is always 1.

2. Prove that the Langevin Monte Carlo algorithm from (6.44) can be
seen as a special case of HMC if only one Leapfrog step is used
(L = 1) and m = 1.






7
Deep Learning

We began our journey through probabilistic machine learning with
linear models. In most practical applications, however, it is seen that
models perform better when labels may nonlinearly depend on the
inputs, and for this reason linear models are often used in conjunction
with “hand-designed” nonlinear features. Designing these features is
costly, time-consuming, and requires expert knowledge. Moreover, the
design of such features is inherently limited by human comprehension
and ingenuity.

7.1 Artificial Neural Networks

One widely used family of nonlinear functions are artificial “deep” neu-
ral networks,1 1 In the following, we will refrain from

using the characterizations “artificial”
f : Rd → Rk, f (x; .

θ) = φ(W · · ·φ(W )))) ( d “deep” for better readability.
Lφ(W an

L−1( 1x 7.1)

where .
θ = [W1, . . . , WL] is a vector of weights (written as matrices

W n
l ∈ R l×nl−1 )2 and φ : R → R is a component-wise nonlinear func- 2 where n0 = d and nL = k

tion. Thus, a deep neural network can be seen as nested (“deep”)
linear functions composed with nonlinearities. This simple kind of
neural network is also called a multilayer perceptron.

In this chapter, we will be focusing mostly on performing probabilis-
tic inference with a given neural network architecture. To this end,
understanding the basic architecture of a multilayer perceptron will
be sufficient for us. For a more thorough introduction to the field of
deep learning and various architectures, you may refer to the books of
Goodfellow et al. (2016) and Prince (2023).

A neural network can be visualized by a computation graph. An exam-
ple for such a computation graph is given in Figure 7.1. The columns
of the computation graph are commonly called layers, whereby the



140 probabilistic artificial intelligence

w(1)

x 1,1 (1) w(2)
1,1 (2) Figure 7.1: Computation graph of a neu-

1 ν w (3
1 ν )

1 1 ral network with two hidden layers.
,1

f1
x (1) (2)

2 ν2 ν2
..

. .

. .

. . .
. ..

fk
x (1) (2)

d νn (3)

w(1) 1 ν
(2) n2 w k,n2

n1,d wn2,n1

input layer hidden layer 1 hidden layer 2 output layer

left-most column is the input layer, the right-most column is the output
layer, and the remaining columns are the hidden layers. The inputs are
(as we have previously) referred to as x .

= [x1, . . . , xd]. The outputs
(i.e., vertices of the output layer) are often referred to as logits and
named f .

= [ f1, . . . , fk]. The activations of an individual (hidden) layer
l of the neural network are described by

ν(l) .
= φ(Wlν

(l−1)) (7.2)

where (
ν(0) = x. The activation of the i-th node is l)

νi = ν(l)(i).

7.1.1 Activation Functions

The nonlinearity φ is called an activation function. The following two
Tanh(x)

activation functions are particularly common:
1

1. The hyperbolic tangent (Tanh) is defined as

. exp(z)− exp(−z) 0
Tanh(z) =

exp(z) + exp(− ∈ (−1, 1). (7.3)
z)

−1

Tanh is a scaled and shifted variant of the sigmoid function (5.9) −2 0 2

which we have previously seen in the context of logistic regression x

as Tanh(z) = 2σ(2z)− 1. ReLU(x)
3

2. The rectified linear unit (ReLU) is defined as
2

ReLU .
(z) = max{z, 0} ∈ [0, ∞). (7.4)

1

In particular, the ReLU activation function leads to “sparser” gra- 0

dients as it selects the halfspace of inputs with positive sign. More- −2 0 2

over, the gradients of ReLU do not “vanish” as z→ ±∞ which can x

lead to faster training. Figure 7.2: The Tanh and ReLU activa-
tion functions, respectively.

It is important that the activation function is nonlinear because other-
wise, any composition of layers would still represent a linear function.
Non-linear activation functions allow the network to represent arbi-
trary functions. This is known as the universal approximation theorem,



deep learning 141

and it states that any artificial neural network with just a single hidden f2

layer (with arbitrary width) and non-polynomial activation function φ
2

can approximate any continuous function to an arbitrary accuracy.
0

7.1.2 Classification
−2

Although we mainly focus on regression, neural networks can equally
well be used for classification. If we want to classify inputs into c sepa- −2 0 2

rate classes, we can simply construct a neural network with c outputs, f1

f = [ f1, . . . , fc], and normalize them into a probability distribution. Figure 7.3: Softmax σ1( f1, f2) for a bi-
Often, the softmax function is used for normalization, nary classification problem. Blue de-

notes a small probability and yellow de-
notes a large probability of belonging to

. exp( f )
σ class 1, respectively.

i( f ) = i (7.5)
∑c

j=1 exp( f j)

where σi( f ) corresponds to the probability mass of class i. The soft-
max is a generalization of the logistic function (5.9) to more than two
classes ? . Note that the softmax corresponds to a Gibbs distribution Problem 7.1
with energies − f .

7.1.3 Maximum Likelihood Estimation

We will study neural networks under the lens of supervised learn-
ing (cf. Section 1.3) where we are provided some independently-
sampled (noisy) data D = {(xi, yi)}n

i=1 generated according to an un-
known process y ∼ p(· | x, θ⋆), which we wish to approximate.

Upon initialization, the network does generally not approximate this
process well, so a key element of deep learning is “learning” a param-
eterization θ that is a good approximation. To this end, one typically
considers a loss function ℓ(θ; y) which quantifies the “error” of the net-
work outputs f (x; θ). In the classical setting of regression, i.e., y ∈ R

and k = 1, ℓ is often taken to be the (empirical) mean squared error,

1 n
ℓmse(θ;D .

) = ( f (x − y 2
n ∑ i; θ) i) . (7.6)

i=1

As we have already seen in Section 2.0.1 in the context of linear re-
gression, minimizing mean squared error corresponds to maximum
likelihood estimation under a Gaussian likelihood.

In the setting of classification where y ∈ {0, 1}c is a one-hot encod-
ing of class membership,3 it is instead common to interpret the out- 3 That is, exactly one component of y is 1
puts of a neural network as probabilities akin to our discussion in and all others are 0, indicating to which

class the given example belongs.
Section 7.1.2. We denote by qθ(· | x) the resulting probability distri-
bution over classes with PMF [σ1( f (x; θ)), . . . , σc( f (x; θ))], and aim to
find θ such that qθ(y | x) ≈ p(y | x). In this context, it is common to



142 probabilistic artificial intelligence

minimize the cross-entropy,

H[p∥qθ] = E(x,y)∼p[− log qθ(y | x)] using the definition of cross-entropy
n (5.32)

≈ − 1

︸ ∑ log︷︷qθ(yi | xi) (7.7)
i=1 ︸ using Monte Carlo sampling

n
.
=ℓce(θ;D)

which can be understood as minimizing the surprise about the training
data under the model. ℓce is called the cross-entropy loss. Disregarding
the constant 1/n, we can rewrite the cross-entropy loss as

n
∝ −∑ log qθ(yi | xi) = ℓnll(θ;D) (7.8)

i=1

Recall that ℓnll(θ;D) is the negative log-likelihood of the training data,
and thus, empirically minimizing cross-entropy can equivalently be
interpreted as maximum likelihood estimation.4 Furthermore, recall 4 We have previously seen this equiv-
from Problem 5.1 that for a two-class classification problem the cross- alence of MLE and empirically mini-

mizing KL-divergence in Section 5.4.6
entropy loss is equivalent to the logistic loss. (minimizing the cross-entropy H[p∥qθ]

is equivalent to minimizing forward-KL
KL(p∥qθ)). Note that this interpreta-

7.1.4 Backpropagation tion is not exclusive to the canonical
cross-entropy loss from Equation (7.7),

A crucial property of neural networks is that they are differentiable. but holds for any MLE. For example,
That is, we can compute gradients ∇θ ℓ of f with respect to the pa- minimizing mean squared error corre-

sponds to empirically minimizing the
rameterization of the model θ = W1:L and some loss function ℓ( f ; y). KL-divergence with a Gaussian likeli-
Being able to obtain these gradients efficiently allows for “learning” a hood.
particular function from data using first-order optimization methods.

The algorithm for computing gradients of a neural network is called
backpropagation and is essentially a repeated application of the chain
rule. Note that using the chain rule for every path through the network
is computationally infeasible, as this quickly leads to a combinatorial
explosion as the number of hidden layers is increased. The key insight
of backpropagation is that we can use the feed-forward structure of our
neural network to memoize computations of the gradient, yielding
a linear time algorithm. Obtaining gradients by backpropagation is
often called automatic differentiation (auto-diff). For more details, refer
to Goodfellow et al. (2016).

Computing the exact gradient for each data point is still fairly expen-
sive when the size of the neural network is large. Typically, stochastic
gradient descent is used to obtain unbiased gradient estimates using
batches of only m of the n data points, where m≪ n.

7.2 Bayesian Neural Networks x (1) (2)
1 ν1 ν1

x (1) (2) f1
2 ν2 ν2 .

How can we perform probabilistic inference in neural networks? We .. .
. . . .

. . .
.

adopt the same strategy which we already used for Bayesian linear f
x (1) (2) k

d νn ν
1 n2

Figure 7.4: Bayesian neural networks
model a distribution over the weights of
a neural network.



deep learning 143

regression, we impose a Gaussian prior on the weights,

θ ∼ N (0, σ2
p I). (7.9)

Similarly, we can use a Gaussian likelihood to describe how well the
data is described by the model f ,

y | x, θ ∼ N ( f (x; θ), σ2
n). (7.10)

Thus, instead of considering weights as point estimates which are
learned exactly, Bayesian neural networks learn a distribution over the
weights of the network. In principle, other priors and likelihoods can
be used, yet Gaussians are typically chosen due to their closedness
properties, which we have seen in Section 1.2.3 and many times since.

7.2.1 Maximum a Posteriori Estimation

Before studying probabilistic inference, let us first consider MAP esti-
mation in the context of neural networks.

The MAP estimate of the weights is obtained by

n
θ̂MAP = arg max log p(θ) + ∑ log p(yi | xi, θ). (7.11)

θ i=1

In Section 7.1.3, we have seen that the negative log-likelihood under
a Gaussian likelihood (7.10) is the squared error between label and
prediction,

log p(yi | xi, θ) = − (yi − f (xi; θ))2
+ const. (7.12)

2σ2
n

Obtaining the MAP estimate instead, simply corresponds to adding an
L2-regularization term to the squared error loss,

1
θ̂MAP = arg min ∥ 1 n

θ∥2 +
2σ2 2 2 ∑(y

p 2σ i − f (xi; θ))2. (7.13) using that for an isotropic Gaussian
θ n i=1 prior, log p(θ) = − 1 ∥θ∥2

2σ2 2 + const
p

As we have already seen in Remark A.31 and the context of Bayesian
linear regression (and ridge regression), using a Gaussian prior is
equivalent to applying weight decay.5 Using gradient ascent, we ob- 5 Recall that weight decay regularizes
tain the following update rule, weights by shrinking them towards zero.

n
θ← θ(1− ληt) + ηt ∑ ∇ log p(yi | xi, θ) (7.14)

i=1

where .
λ = 1/σ2

p. The gradients of the likelihood can be obtained using
automatic differentiation.



144 probabilistic artificial intelligence

7.2.2 Heteroscedastic Noise

Equation (7.10) uses the scalar parameter σ2
n to model the aleatoric un-

certainty (label noise), similarly to how we modeled the label noise y
in Bayesian linear regression and Gaussian processes. Such a noise
model is called homoscedastic noise as the noise is assumed to be uni-
form across the domain. In many settings, however, the noise is in-
herently heteroscedastic noise. That is, the noise varies depending on
the input and which “region” of the domain the input is from. This
behavior is visualized in Figure 7.5.

There is a natural way of modeling heteroscedastic noise with Bayesian x
neural networks. We use a neural network with two outputs f1 and f2
and define Figure 7.5: Illustration of data with vari-

able (heteroscedastic) noise. The noise
increases as the inputs increase in mag-

y | x, θ ∼ N (µ(x; θ), σ2(x; θ)) where (7.15a) nitude. The noise-free function is shown
.

µ(x; θ) = f in black
1(x; θ), (7.15b) .

σ2(x; .
θ) = exp( f2(x; θ)). (7.15c) we exponentiate f2 to ensure

non-negativity of the variance

Using this model, the likelihood term from Equation (7.11) is

log p(yi | xi, θ) = logN (yi; µ(xi; θ), σ2(xi; θ))

= log √ 1 − (yi − µ(xi; θ))2

[ note that the normalizing constant
2πσ2(xi; θ) 2σ2(xi; θ) ] depends on the noise model!

√1 1 (xi; θ))2

︸ ︷︷ − (y − µ
= log log σ2(xi; θ) + i .

2π︸ 2 σ2(xi; θ)
const

(7.16)

Hence, the model can either explain the label yi by an accurate model
µ(xi; θ) or by a large variance σ2(xi; θ), yet, it is penalized for choos-
ing large variances. Intuitively, this allows to attenuate losses for some
data points by attributing them to large variance (when no model re-
flecting all data points simultaneously can be found). This allows the
model to “learn” its aleatoric uncertainty. However, recall that the
MAP estimate still corresponds to a point estimate of the weights, so
we forgo modeling the epistemic uncertainty.

7.3 Approximate Probabilistic Inference

Naturally, we want to understand the epistemic uncertainty of our
model too. However, learning and inference in Bayesian neural net-
works are generally intractable (even when using a Gaussian prior
and likelihood) when the noise is not assumed to be homoscedastic
and known.6 Thus, we are led to the techniques of approximate infer- 6 In this case, the conjugate prior to
ence, which we discussed in the previous two chapters. a Gaussian likelihood is not a Gaus-

sian. See, e.g., https://en.wikipedia.
org/wiki/Conjugate_prior.



deep learning 145

7.3.1 Variational Inference

As we have discussed in Chapter 5, we can apply black box stochas-
tic variational inference which — in the context of neural networks
— is also known as Bayes by Backprop. As variational family, we use
the family of independent Gaussians which we have already encoun-
tered in Example 7

5.5. Recall the fundamental objective of variational 7 Independent Gaussians are useful be-
inference ( cause they can be encoded using only

5.25),
2d parameters, which is crucial when the

| size of the neural network is large.
arg min KL(q∥p(· x1:n, y1:n))

q∈Q
= arg max L(q, p;D) using Equation (5.53)

q∈Q
= arg max Eθ∼q[log p(y1:n | x1:n, θ)]−KL(q∥p(·)). using Equation (5.55c)

q∈Q

The KL-divergence KL(q∥p(·)) can be expressed in closed-form for
Gaussians.8 Recall that we can obtain unbiased gradient estimates of 8 see Equation (5.41)
the expectation using the reparameteri[zation trick (5.68), ]
Eθ∼q[log p(y1:n | x1:n, θ)] = Eε∼N (0,I) log p(y1:n | x1:n, θ)|

θ=Σ1/2 .
ε+µ

As Σ is the diagonal matrix diag{σ2
1 , . . . , σ2 2

d}, Σ1/ = diag{σ1, . . . , σd}.
The gradients of the likelihood can be obtained using backpropaga-
tion. We can now use the variational posterior qλ to perform approxi-
mate inference, ∫

p(y⋆ | x⋆, x1:n, y1:n) = p(y⋆ | x⋆, θ)p(θ | x1:n, y1:n) dθ using the sum rule (1.7) and product
rule (1.11)

= Eθ∼p(·|x p(y⋆ | x⋆, θ)] interpreting the integral as an
1:n ,y1:n)

[
expectation over the posterior

≈ Eθ∼q [p(y⋆ | x⋆, θ)] approximating the posterior with the
λ

m variational posterior qλ

≈ 1
p(y⋆ | x⋆, θ(i)) (7.17) using Monte Carlo sampling

m ∑
i=1

for independent samples i
θ(i) ∼id qλ,

1 m
= N (y⋆; µ(x⋆; θ(i)), σ2(x⋆; θ(i))). (7.18) using the neural network

m ∑
i=1

Intuitively, variational inference in Bayesian neural networks can be
interpreted as averaging the predictions of multiple neural networks
drawn according to the variational posterior qλ.

Using the Monte Carlo samples θ(i), we can also estimate the mean of
our predictions,

1 m
E[y⋆ | x⋆, x .

1:n, y1:n] ≈ µ(x⋆; θ(i)) = µ(x⋆), (7.19)
m ∑

i=1



146 probabilistic artificial intelligence

and the variance of our predictions, ]
Var[y⋆ | x⋆, x1:n, y1:n] = Eθ[[ ] [

Var ⋆
y⋆ [y ]| x⋆, θ] + Var ⋆

θ Ey⋆ [y | x⋆, θ] using the law of total variance (1.41)

= E ⋆
θ σ2(x ; θ) + Varθ[µ(x⋆; θ)]. using the likelihood (7.15a)

Recall from Equation (2.18) that the first term corresponds to the ale-
atoric uncertainty of the data and the second term corresponds to the
epistemic uncertainty of the model. We can approximate them using
the Monte Carlo samples θ(i),

m
Var[y⋆ | x⋆, x 2

1:n, y1:n] ≈
1

(x⋆; θ(i)) (7.20)
m ∑ σ

i=1
1 m

+ ∑(µ x⋆; θ(i))− µ(x⋆))2
m− (

1 i=1

using a sample mean (A.14) and sample variance (A.16).

7.3.2 Markov Chain Monte Carlo

As we have discussed in Chapter 6, an alternative method to approxi-
mating the full posterior distribution is to sample from it directly. By
the ergodic theorem (6.28), we can use any of the discussed sampling
strategies to obtain samples θ(t) such that

1 T
p(y⋆ | x⋆, x (t)

1:n, y1:n) ≈ (y⋆ | x⋆, θ ). see (6.29)
T ∑ p

t=1

Here, we omit the offset t0 which is commonly used to avoid the
“burn-in” period for simplicity. Algorithms such as SGLD or SG-HMC
are often used as they rely only on stochastic gradients of the loss func-
tion which can be computed efficiently using automatic differentiation.

Typically, for large networks, we cannot afford to store all T samples of
models. Thus, we need to summarize the iterates.9 One approach is to 9 That is, combine the individual sam-
keep track of m snapshots of weights [θ(1), . . . , θ(m)] according to some ples of weights θ(i).

schedule and use those for inference (e.g., by averaging the predictions
of the corresponding neural networks). This approach of sampling a
subset of some data is generally called subsampling.

Another approach is to summarize (that is, approximate) the weights
using sufficient statistics (e.g., a Gaussian).10 In other words, we learn 10 A statistic is sufficient for a family of
the Gaussian approximation, probability distributions if the samples

from which it is calculated yield no more
information than the statistic with re-

θ ∼ N (µ, Σ), where (7.21a) spect to the learned parameters. We pro-
vide a formal definition in Section 8.2.

. 1 T
µ = θ(i), (7.21b) using a sample mean (A.14)

T ∑
i=1



deep learning 147

. 1 T
Σ = ( )

T − ( µ
1 ∑ θ i − )(θ(i) − µ)⊤, (7.21c) using a sample variance (A.16)

i=1

using sample means and sample (co)variances. This can be imple-
mented efficiently using running averages of the first and second mo-
ments,

µ← 1
(Tµ + θ) and A← 1

(TA + θθ⊤) (7.22)
T + 1 T + 1

upon observing the fresh sample θ. Σ can easily be calculated from
these moments,

T
Σ = (7.23) the characterization of sample

T − (A− µµ⊤). using
1 variance in terms of estimators of the

To predict, we can sample weights θ from the learned Gaussian. first and second moment (A.17)

Remark 7.1: Stochastic weight averaging
It turns out that this approach works well even without inject-
ing additional Gaussian noise during training, e.g., when using
SGD rather than SGLD. Simply taking the mean of the iterates of
SGD is called stochastic weight averaging (SWA). Describing the iter-
ates of SGD by Gaussian sufficient statistics (analogously to Equa-
tion (7.21)), is known as the stochastic weight averaging-Gaussian
(SWAG) method (Izmailov et al., 2018).

7.3.3 Dropout and Dropconnect

We will now discuss two approximate inference techniques that are x (1) (2)
1 ν1 ν1

tailored to neural networks. The first is “dropout”/“dropconnect” reg- x (1) (2) f1
2 ν2 ν2 .

ularization. Traditionally, dropout regularization (Hinton et al., 2012; Sri- .. . . .
. . .

. ..
vastava et al., 2014) randomly omits vertices of the computation graph f

x (1) (2) k
d νn ν

1 n
during training, see Figure 7.7. In contrast, dropconnect regularization 2

(Wan et al., 2013) randomly omits edges of the computation graph. Figure 7.6: Illustration of dropout reg-
ularization. Some vertices of the com-

The key idea that we will present here is to interpret this type of reg- putation graph are randomly omitted.
ularization as performing variational inference. In contrast, dropconnect regularization

randomly omits edges of the computa-
In our exposition, we will focus on dropconnect, but the same ap- tion graph.

proach also applies to dropout (Gal and Ghahramani, 2016). Suppose qj
that we omit an edge of the computation graph (i.e., set its weight to
zero) with probability p. Then our variational posterior is given by 1− p

d
q(θ | .

λ) = ∏ qj(θj | λj) (7.24)
j=1 p

where d is the number of weights of the neural network and
0 λj

q .
j(θj | λj) = pδ0(θj) + (1− p)δλj(θj). (7.25) θj

Figure 7.7: Interpretation of dropconnect
regularization as variational inference.
The only coordinates where the varia-
tional posterior qj has positive probabil-
ity are 0 and λj.



148 probabilistic artificial intelligence

Here, δα is the Dirac delta function with point mass at α.11 The vari-
ational parameters λ correspond to the “original” weights of the net-
work. In words, the variational posterior expresses that the j-th weight
has value 0 with probability p and value λj with probability 1 − p. 11 see Appendix A.1.4
For fixed weights λ, sampling from the variational posterior qλ corre-
sponds to sampling a vector z with entries z(i) ∼ Bern(p), yielding
z⊙ λ which is one of 2d possible subnetworks.12 12 A ⊙ B denotes the Hadamard

(element-wise) product.

The weights λ can be learned by maximizing the ELBO, analogously to
black-box variational inference. The KL-divergence term of the ELBO
is not tractable for the variational family described by Equation (7.25),
instead a common approach is to use a mixture of Gaussians:

q .
j(θj | λj) = pN (θj; 0, 1) + (1− p)N (θj; λj, 1). (7.26)

In this case, it can be shown that KL(qλ∥p(·)) ≈ p
2 ∥λ∥

2
2 for sufficiently

large d (Gal and Ghahramani, 2015, proposition 1). Thus,

arg max L(qλ, p;D)
λ∈Λ

= arg max Eθ∼q [log p(y
λ 1:n | x1:n, θ)]−KL(q∥p(·)) using Equation (5.55c)

λ∈Λ

≈ arg min− 1 m ∣ p 2

λ∈ m ∑ log p(y1:n | x1:n, θ)∣
θ=z(i)⊙ ∥λ u i g o

λ+ε(i)
+ ∥ (7.27) s M nte Carlo sampling

2 2 n
Λ i=1

where we reparameterize θ ∼ qλ by θ = z⊙ λ + ε with z(i) ∼ Bern(p)
and ε ∼ N (0, I). Equation (7.27) is the standard L2-regularized loss
function of a neural network with weights λ and dropconnect, and it
is straightforward to obtain unbiased gradient estimates by automatic
differentiation.

Crucially, for the interpretation of dropconnect regularization as vari-
ational inference to be valid, we also need to perform dropconnect
regularization during inference,

p(y⋆ | x⋆, x1:n, y1:n) ≈ Eθ∼q [p(y⋆ | x⋆, θ)]
λ

≈ 1 m
p(y | x⋆

m ∑ ⋆ , θ(i)) (7.28) using Monte Carlo sampling
i=1

where θ(i)
i∼id qλ are independent samples. This coincides with our ear-

lier discussion of variational inference for Bayesian neural networks
in Equation (7.17). In words, we average the predictions of m neural
networks for each of which we randomly “drop out” weights.



deep learning 149

Remark 7.2: Masksembles
A practical problem of dropout is that for any reasonable choice
of dropout probability, the dropout masks z(i) will overlap signif-
icantly, which tends to make the predictions p(y⋆ | x⋆, θ(i)) highly
correlated. This can lead to underestimation of epistemic uncer-
tainty. Masksembles (Durasov et al., 2021) mitigate this issue by
choosing a fixed set of pre-defined dropout masks, which have
controlled overlap, and alternating between them during training.
In the extreme case of “infinitely many” masks, masksembles are
equivalent to dropout since each mask is only seen once.

7.3.4 Probabilistic Ensembles

We have seen that variational inference in the context of Bayesian neu-
ral networks can be interpreted as averaging the predictions of m neu-
ral networks drawn according to the variational posterior.

A natural adaptation of this idea is to immediately learn the weights
of m neural networks. The idea is to randomly choose m training sets
by sampling uniformly from the data with replacement. Then, using
our analysis from Section 7.2.1, we obtain m MAP estimates of the
weights θ(i), yielding the approximation

p(y⋆ | x⋆, x1:n, y1:n) = Eθ∼p(·|x1:n ,y1:n)
[p(y⋆ | x⋆, θ)]

≈ 1 m
∑ p(y⋆ | x⋆, θ(i)). (7.29) using bootstrapping

m i=1

Here, the randomly chosen m training sets induce “diversity” of the
models. In practice, in the context of deep neural networks where
the global minimizer of the loss can rarely be identified, it is com-
mon to use the full training data to train each of the m neural net-
works. Random initialization and random shuffling of the training
data is typically enough to ensure some degree of diversity between
the individual models (Lakshminarayanan et al., 2017). We can con-
nect ensembles to the other approximate inference techniques we have
discussed: First, ensembles can be seen as a specific kind of masksem-
ble, where the masks are non-overlapping,13 which mitigates the issue 13 That is, the m models do not share
of correlated predictions from Remark 7.2. Second, ensembling can be any of their parameters. Ensembles and

dropout lie on opposite ends of this
combined with other approximate inference techniques such as varia- spectrum.
tional inference, Laplace approximation, or SWAG to get a mixture of
Gaussians as the posterior approximation.

Note that Equation (7.29) is not equivalent to Monte Carlo sampling,
although it looks very similar. The key difference is that this approach
does not sample from the true posterior distribution p, but instead



150 probabilistic artificial intelligence

from the empirical posterior distribution p̂ given the (re-sampled) MAP
estimates. Intuitively, this can be understood as the difference between
sampling from a distribution p directly (Monte Carlo sampling) versus
sampling from an approximate (empirical) distribution p̂ (correspond-
ing to the training data), which itself is constructed from samples of the
true distribution p. This approach is known as bootstrapping or bagging
(short for bootstrap aggregating) and plays a central role in model-free
reinforcement learning. We will return to this concept in Section 11.4.1.

7.3.5 Diverse Probabilistic Ensembles

Probabilistic ensembles can be loosely interpreted as randomly initial-
izing m “particles” {θ(i)}m

i=1 and then pushing each particle towards
regions of high posterior probability. A potential issue with this ap-
proach is that if the initialization of particles is not sufficiently diverse,
the particles may “collapse” since every particle eventually converges
to a local optimum of the loss function. A natural approach to mitigate
this issue is to alter the objective of each particle from simply aiming
to minimize the loss − log p(θ | x1:n, y1:n), which we will abbreviate by
− log p(θ), to also “push” the particles away from each other. We will
see next that this can be interpreted as a form of variational inference
under a very flexible variational family.

In our discussion of variational inference, we have seen that minimiz-
ing reverse-KL is equivalent to maximizing the evidence lower bound,
and used this to derive an optimization algorithm to compute approx-
imate posteriors. We will discuss the alternative approach of directly
computing the gradient of the KL-divergence. Consider the variational
family of all densities that can be expressed as smooth transformations
of points sampled from a reference density ϕ. That is, we consider
Q .

ϕ = {T♯ϕ | T : Θ→ Θ is smooth} where T♯ϕ is the density of the
random vector θ′ = T(θ) with θ ∼ ϕ.14 The density ϕ can be thought 14 Refer back to Section 1.1.11 for a recap
of as the initial distribution of the particles, and the smooth map T on pushforwards.

as the dynamics that push the particles towards the target density p.
It can be shown that for “almost any” reference density ϕ, this varia-
tional family Qϕ is expressive enough to closely approximate “almost
arbitrary” distributions.15 A natural approach is therefore to learn the 15 For a more detailed discussion, refer
appropriate smooth map T between the reference density ϕ and the to “Stein variational gradient descent: A

general purpose Bayesian inference algo-
target density p. rithm” (Liu and Wang, 2016).

Example 7.3: Gaussian variational inference
We have seen in Section 5.5 that if ϕ is standard normal and
T(x; {µ, Σ1/2}) = µ+Σ1/2x an affine transformation then the ELBO



deep learning 151

can be maximized efficiently using stochastic gradient descent.
However, in this case, Qϕ can only approximate Gaussian-like dis-
tributions since the expressivity of the map T is limited under the
fixed reference density ϕ.

An alternative approach to Gaussian variational inference is the fol-
lowing algorithm known as Stein variational gradient descent (SVGD),
where we recursively apply carefully chosen smooth maps to the cur-
rent variational approximation:

T⋆ ⋆

q −→0 T T⋆

q −1 2
1 → q2 −→ · · · where q . ⋆

0 t+1 = Tt ♯qt. (7.30)

We consider maps T = id + f where id .
(θ) = θ denotes the identity

map and f (x) represent(s a (sma)ll) perturbation. Recall that at time t
we seek to minimize KL T♯qt∥p , so we choos)e the smooth map as

T⋆ . ( ∣
t = id− ηt ∇f KL T♯qt∥p ∣ f 7. 1)

=0 ( 3

where ηt is a step size. In this way, the SVGD update (7.31) can be
interpreted as a step of “functional” gradient descent.

To be able to compute the gradient of the KL-divergence, we need
to make some structural assumptions on the perturbation f . SVGD
assumes that f = [ f1 · · · fd]

⊤ with fi ∈ Hk(Θ) is from some re-
producing kernel Hilbert space Hk(Θ) of a positive definite kernel k;
we say f ∈ Hd

k (Θ). Within the RKHS, we can compute the gradient
of the KL-divergence exactly. Liu and Wang (2016) show that in this
case, the functional grad(ient of)t∣he KL-divergence can be expressed in
closed-form as −∇f KL T ∣

♯q∥p ⋆
f φ
=0 = q,p where

φ⋆ .
q,p(·) = Eθ∼q[k(·, θ)∇θ log p(θ) +∇θ k(·, θ)]. (7.32)

SVGD then approximates q using the particles {θ(i)}m
i=1 as follows:

Algorithm 7.4: Stein variational gradient descent, SVGD
1 initialize particles {θ(i)}m

i=1
2 repeat
3 for each particle i ∈ [m] do
4 θ(i) ← θ(i) + ηtφ̂

⋆
q,p[(θ(i)) where ]

φ̂⋆ .
q,p(θ) =

1 j
m ∑m (

j=1 k(θ, θ ))∇θ log p(θ) +∇θ(j) k(θ, θ(j))

5 until converged



152 probabilistic artificial intelligence

Often, a Gaussian kernel (4.14) with length scale h is used to model
the perturbations, in which case the repulsion term is

∇ ) 1
θ(j) k(θ, θ(j ) = (θ− θ(j))k(θ, θ(j)) (7.33)

h2

and the negative functional gradient simplifies to

1 m [ ]
φ̂⋆

q,p(θ) = k(θ, θ(j)) ∇ (θ) +
m ∑ ︸ θ lo︷g︷ p ︸h−2(θ . (7.34)

j ︷−︷ θ(j)︸)
=1 ︸

drift repulsion

Note that SVGD has similarities to Langevin dynamics, which as seen
in ? can also be interpreted as following a gradient of the KL-divergence. Problem 6.9
Whereas Langevin dynamics perturbs particles according to a drift to-
wards regions of high posterior probability and some random diffu-
sion (cf. Equation (6.46)), the first term of φ̂⋆

q,p(θ) perturbs particles to
drift towards regions of high posterior probability while the second
term leads to a mutual “repulsion” of particles. Notably, the perturba-
tions of Langevin dynamics are noisy, while SVGD perturbs particles
deterministically and the randomness is exclusively in the initializa-
tion of particles. The repulsion term prevents particles from collaps-
ing to a single mode of the posterior distribution, which is a possible
failure mode of other particle-based posterior approximations such as
ensembles.

Note that the above decomposition of φ̂⋆
q,p(θ) is once more an example

of the principle of curiosity and conformity which we have seen to be a
recurring theme in approaches to approximate inference. The repul-
sion term leads to exploration of the particles (i.e., “curiosity” about
alternative explanations), while the drift term leads to minimization of
the loss (i.e., “conformity” to the data).

Lu et al. (2019) show that under some assumptions, SVGD converges
asymptotically to the target density p as ηt → 0. SVGD’s name orig-
inates from Stein’s method which is a general-purpose approach for
characterizing convergence in distribution.16 16 For an introduction to Stein’s method,

read “Measuring sample quality with
Stein’s method” (Gorham and Mackey,

7.4 Calibration 2015).

A key challenge of Bayesian deep learning (and also other probabilis-
tic methods) is the calibration of models. We say that a model is well-
calibrated if its confidence coincides with its accuracy across many pre-
dictions. Consider a classification model that predicts that the label
of a given input belongs to some class with probability 80%. If the
model is well-calibrated, then the prediction is correct about 80% of
the time. In other words, during calibration, we adjust the probability
estimation of the model.



deep learning 153

We will first mention two methods of estimating the calibration of a
model, namely the marginal likelihood and reliability diagrams. Then,
in Section 7.4.3, we survey commonly used heuristics for empirically
improving the calibration.

7.4.1 Evidence

A popular method (which we already encountered multiple times) is
to use the evidence of a validation set xval

1:m of size m given the training
set xtrain

1:n of size n for estimating the model calibration. Here, the ev-
idence can be understood as describing how well the validation set is
described by the model trained on the training set. We obtain,

log p(y∫val in rain
1:m | xval

1:m, xtra
1:n , yt

1:n )

= log ∫ p(yval
1:m | xval rain

1:m, θ)p(θ | xt
1:n , ytrain

1:n ) dθ using the sum rule (1.7) and product
rule (1.11)

≈ log val val

∫ p(y1:m | x1:m, θ)qλ(θ) dθ approximating with the variational
posterior

m
= log ∏ p(yval al

i | xv
i , θ)qλ(θ) dθ (7.35) using the independence of the data

i=1

The resulting integrals are typically very small which leads to numer-
ical instabilities. Therefore, it is common to maximize a lower bound
to the evidence instead, [ ]

m
= log Eθ[∼q p(yval

] interpreting the integral as an
λ ∏ i | xval

i , θ)
i=1 expectation over the variational

m posterior
≥ Eθ∼q log p(yval val, θ) (7.36) using Jensen’s inequality (5.29)

λ ∑ i | xi
i=1

≈ 1 k m
val ) (7.37) g

k ∑ ∑ log p(yval
i | xi , θ(j ) using Monte Carlo samplin

j=1 i=1

for independent samples i
θ(j) ∼id qλ.

7.4.2 Reliability Diagrams

Reliability diagrams take a frequentist perspective to estimate the cal-
ibration of a model. For simplicity, we assume a calibration problem
with two classes, 1 and −1 (similarly to logistic regression).17 17 Reliability diagrams generalize be-

yond this restricted example.
We group the predictions of a validation set into M interval bins of
size 1/M according to the class probability predicted by the model,
P(Yi = 1 | xi). We then compare within each bin, how often the model
thought the inputs belonged to the class (confidence) with how often
the inputs actually belonged to the class (frequency). Formally, we



154 probabilistic artificial intelligence

1.0

define Bm as the set of samples falling into bin m and let 0.8

freq(B . 1
m) = | = 7 )

B ∑ 0.6
1 Y } 3

m|
{ i 1 ( . 8

i∈Bm 0.4

be the proportion of samples in bin m that belong to class 1 and let 0.2

conf(B . 1 0.0

m) = | = ( .
B ∑ P Y | i 9

m|
( i 1 x ) 7 3 ) 0.00 0.25 0.50 0.75 1.00

i∈Bm conf

be the average confidence of samples belonging to class 1 within the 1.0

bin m. 0.8

Thus, a model is well calibrated if freq(Bm) ≈ conf(Bm) for each bin 0.6

m ∈ [M]. There are two common metrics of calibration that quantify 0.4

how “close” a model is to being well calibrated. 0.2

1. The expected calibration error (ECE) is the average deviation of a 0.0

model from perfect calibration, 0.00 0.25 0.50 0.75 1.00
conf

M
. |Bm|

ℓECE = ∑ |freq(Bm)− conf(Bm)| ( Figure 7.8: Examples of reliability dia-
7.40)

m=1 n grams with ten bins. A perfectly cali-
brated model approximates the diagonal

where n is the size of the validation set. dashed red line. The first reliability dia-
gram shows a well-calibrated model. In

2. The maximum calibration error (MCE) is the maximum deviation of
contrast, the second reliability diagram

a model from perfect calibration among all bins, shows an overconfident model.
.

ℓMCE = max |freq(Bm)− conf(Bm)| . (7.41)
m∈[M]

7.4.3 Heuristics for Improving Calibration

We now survey a few heuristics which can be used empirically to im-
prove model calibration.
1. Histogram binning assigns a calibrated score q .

m = freq(Bm) to each
bin during validation. Then, during inference, we return the cal-
ibrated score qm of the bin corresponding to the prediction of the
model.

2. Isotonic regression extends histogram binning by using variable bin
boundaries. We find a piecewise constant function f .

= [ f1, . . . , fM]
that minimizes the bin-wise squared loss,

M n
min ∑ ∑ 1{am ≤ P(Yi = 1 | xi) < am+1}( fm − yi)

2
M, f ,a m=1 i=1

(7.42a)
subject to 0 = a1 ≤ · · · ≤ aM+1 = 1, (7.42b)

f1 ≤ · · · ≤ fM (7.42c)

where f are the calibrated scores and a .
= [a1, . . . , aM+1] are the

bin boundaries. We then return the calibrated score fm of the bin
corresponding to the prediction of the model.

freq freq



deep learning 155

3. Platt scaling adjusts the logits zi of the output layer to

q .
i = σ(azi + b) (7.43)

and then learns parameters a, b ∈ R to maximize the likelihood.
4. Temperature scaling is a special and widely used instance of Platt

scaling where a .
= 1/T and b .

= 0 fo(r so)me temperature scalar T > 0, 1

. z
qi = σ i . (7.44)

T

Intuitively, for a larger temperature T, the probability is distributed
more evenly among the classes (without changing the ranking),
yielding a more uncertain prediction. In contrast, for a lower tem- 0

A B C
perature T, the probability is concentrated more towards the top
choices, yielding a less uncertain prediction. As seen in Prob- 1

lem 6.7, temperature scaling can be motivated as tuning the mean
of the softmax distribution.

Optional Readings
• Guo, Pleiss, Sun, and Weinberger (2017). 0

On calibration of modern neural networks. A B C

• Blundell, Cornebise, Kavukcuoglu, and Wierstra (2015). Figure 7.9: Illustration of temperature

Weight uncertainty in neural network. scaling for a classifier with three classes.
On the top, we have a prediction with

• Kendall and Gal (2017). a high temperature, yielding a very un-
What uncertainties do we need in Bayesian deep learning for com- certain prediction (in favor of class A).

Below, we have a prediction with a low
puter vision?. temperature, yielding a prediction that

is strongly in favor of class A. Note that
the ranking (A ≻ C ≻ B) is preserved.

Discussion

This chapter concludes our discussion of (approximate) probabilistic
inference. Across the last three chapters, we have seen numerous
methods for approximating the posterior distributions of deep neural
networks:
• Methods such as dropout and stochastic weight averaging are fre-

quently used in practice. Other particle-based approaches such as
ensembles and SVGD are used less frequently since they are compu-
tationally more expensive to train, but are some of the most effective
methods in estimating uncertainty.

• Recently, Laplace approximations regained interest since they can
be applied “post-hoc” after training simply by computing or ap-
proximating the Hessian of the loss function (Daxberger et al., 2021;
Antorán et al., 2022). Still, Laplace approximations come with the
limitations inherent to unimodal Gaussian approximations.

qi qi



156 probabilistic artificial intelligence

• Other work, particularly in fine-tuning, has explored approximating
the posterior distribution of deep neural networks by treating them
as linear functions in a fixed learned feature space, in which case
one can use the tools for exact probabilistic inference from Chap-
ters 2 and 4 (e.g., Hübotter et al., 2025).

Despite large progress in approximate inference over the past decade,
efficient and reliable uncertainty estimation of large models remains
an important open challenge.

Problems

7.1. Softmax is a generalization of the logistic function.

Show that for a two-class classification problem (i.e., c = 2), the soft-
max function is equivalent to the logistic function (5.9) for the univari-
ate model f .

= f1 − f0. That is, σ1( f ) = σ( f ) and σ0( f ) = 1− σ( f ).

Thus, the softmax function is a generalization of the logistic function
to more than two classes.



part II

Sequential Decision-Making






Preface to Part II

In the first part of the manuscript, we have learned about how we can
build machines that are capable of updating their beliefs and reducing
their epistemic uncertainty through probabilistic inference. We have
also discussed ways of keeping track of the world through noisy sen-
sory information by filtering. An important aspect of intelligence is to
use this acquired knowledge for making decisions and taking actions
that have a positive impact on the world.

Already today, we are surrounded by machines that make decisions
and take actions; that is, exhibit some degree of agency. Be it a search
engine producing a list of search results, a chatbot answering a ques-
tion, or a driving-assistance system steering a car: these systems are
all perceiving the world, making decisions, and then taking actions
that in turn have an effect on the world. Figure 7.10 illustrates this
perception-action loop.

Figure 7.10: An illustration of the
perception-action loop. This is a
straightforward extension of our view of
probabilistic inference from Figure 1.10

worldt with the addition of an “action” compo-
nent which is capable of “adaptively” in-

world teracting with the outside world and the
t+1

perception internal world model.

Dt action

model p(θ | D1:t)

In the second part of this manuscript, we will discuss the underpin-
ning principles of building machines that are capable of making se-



160 probabilistic artificial intelligence

quential decisions. We will see that decision-making itself can be cast
as probabilistic inference, obeying the same mechanisms that we used
in the first part to build learning systems.

We discuss various ways of addressing the question:

How to act, given that computational resources and time are limited?

One approach is to act with the aim to reduce epistemic uncertainty,
which is the topic of active learning. Another approach is to act with
the aim to maximize some reward signal, which is the topic of bandits,
Bayesian optimization, and reinforcement learning.

This surfaces the exploration-exploitation dilemma where the agent
has to prioritize either maximizing its immediate rewards or reducing
its uncertainty about the world which might pay off in the future. We
discuss that this dilemma is, in fact, in direct correspondence to the
principle of curiosity and conformity which we discussed extensively
throughout Part I.

Since time is limited, it is critical to be sample-efficient when learning
the most important aspects of the world. At the same time, interactions
with the world are often complex, and some interactions might even
be harmful. We discuss how an agent can use its epistemic uncertainty
to guide the exploration of its environment while mitigating risks and
reasoning about safety.



8
Active Learning

By now, we have seen that probabilistic machine learning is very use-
ful for estimating the uncertainty in our models (epistemic uncer-
tainty) and in the data (aleatoric uncertainty). We have been focus-
ing on the setting of supervised learning where we are given a set
D = {(xi, yi)}n

i=1 of labeled data, yet we often encounter settings
where we have only little data and acquiring new data is costly.

In this chapter — and in the following chapter on Bayesian optimiza-
tion — we will discuss how one can use uncertainty to effectively
collect more data. In other words, we want to figure out where in
the domain we should sample to obtain the most useful information.
Throughout most of this chapter, we focus on the most common way
of quantifying “useful information”, namely the expected reduction in
entropy which is also called the mutual information.

8.1 Conditional Entropy

We begin by introducing the notion of conditional entropy. Recall that
the entropy H[X] of a random vector X can be interpreted as our av-
erage surprise when observing realizations x ∼ X. Thus, entropy can
be considered as a quantification of the uncertainty about a random
vector (or equivalently, its distribution).1 1 We discussed entropy extensively in

Section 5.4.
A natural extension is to consider the entropy of X given the occur-
rence of an event corresponding to another random variable (e.g.,
Y = y for a random vector Y),

H[X | Y = y .
] = Ex∼p(x|y)[− log p(x | y)]. (8.1)

Instead of averaging over the surprise of samples from the distribu-
tion p(x) (like the entropy H[X]), this quantity simply averages over
the surprise of samples from the conditional distribution p(x | y).



162 probabilistic artificial intelligence

Definition 8.1 (Conditional entropy). The conditional entropy of a ran-
dom vector X given the random vector Y is defined as

H[X | Y .
] = Ey∼p(y)[H[X | Y = y]] (8.2)

= E(x,y)∼p(x,y)[− log p(x | y)]. (8.3)

Intuitively, the conditional entropy of X given Y describes our aver-
age surprise about realizations of X given a particular realization of Y,
averaged over all such possible realizations of Y. In other words, con-
ditional entropy corresponds to the expected remaining uncertainty
in X after we observe Y. Note that, in general, H[X | Y] ̸= H[Y | X].

It is crucial to stress the difference between H[X | Y = y] and the con-
ditional entropy H[X | Y]. The former simply corresponds to a proba-
bilistic update of our uncertainty in X after we have observed the real-
ization y ∼ Y. In contrast, conditional entropy predicts how much un-
certainty will remain about X (in expectation) after we will observe Y.

Definition 8.2 (Joint entropy). One can also define the joint entropy of
random vectors X and Y,

H .
[X, Y] = E(x,y)∼p(x,y)[− log p(x, y)], (8.4)

as the combined uncertainty about X and Y. Observe that joint entropy
is symmetric.

This gives the chain rule for entropy,

H[X, Y] = H[Y] + H[X | Y] (8.5) using the product rule (1.11) and the
definition of conditional entropy (8.2)

= H[X] + H[Y | X]. (8.6) using symmetry of joint entropy

That is, the joint entropy of X and Y is given by the uncertainty about X
and the additional uncertainty about Y given X. Moreover, this also
yields Bayes’ rule for entropy,

H[X | Y] = H[Y | X] + H[X]−H[Y]. (8.7) using the chain rule for entropy (8.5)
twice

A very intuitive property of entropy is its monotonicity: when condi-
tioning on additional observations the entropy can never increase,

H[X | Y] ≤ H[X]. (8.8)

Colloquially, this property is also called the “information never hurts”
principle. We will derive a proof in the following section.



active learning 163

8.2 Mutual Information

Recall that our fundamental objective is to reduce entropy, as this cor-
responds to reduced uncertainty in the variables, which we want to
predict. Thus, we are interested in how much information we “gain”
about the random vector X by choosing to observe a random vector Y.
By our interpretation of conditional entropy from the previous section,
this is described by the following quantity.

Definition 8.3 (Mutual information, MI). The mutual information of X
and Y (also known as the information gain) is defined as

I .
(X; Y) = H[X]−H[X | Y] (8.9)

= H[X] + H[Y]−H[X, Y]. (8.10)

In words, we subtract the uncertainty left about X after observing Y
from our initial uncertainty about X. This measures the reduction in
our uncertainty in X (as measured by entropy) upon observing Y. Un-
like conditional entropy, it follows from the definition that mutual in-
formation is symmetric. That is,

I(X; Y) = I(Y; X). (8.11)

Thus, the mutual information between X and Y can be understood as
the approximation error (or information loss) when assuming that X

Figure 8.1: Information gain. The first
and Y are independent. graph shows the prior. The second

graph shows a selection of samples with
In particular, using Gibbs’ inequality (cf. Problem 5.5), this relation- large information gain (large uncertainty
ship shows that I(X; Y) ≥ 0 with equality when X and Y are indepen- reduction). The third graph shows a se-

lection of samples with small informa-
dent, and also proves the information never hurts principle (8.8) as tion gain (small uncertainty reduction).

0 ≤ I(X; Y) = H[X]−H[X | Y]. (8.12) H[X] H[X, Y] H[Y]

Example 8.4: Mutual information of Gaussians H[X | Y] I(X; Y) H[Y | X]

Given the Gaussian random vector X ∼ N (µ, Σ) and the noisy
observation Y = X + ε where ε ∼ N (0, σ2

n I), we want to find the
Figure 8.2: Relationship between mutual

information gain of X when observing Y. Using our definitions information and entropy, expressed as a
from this chapter, we obtain Venn diagram.

I(X; Y) = I(Y; X) using symmetry (8.11)

= H[Y]−H[Y | X] by mutual information (8.9)

= H[Y]−H[ε] given X, the only randomness in Y

1 ( ( )) 1 ( ( )) originates from ε
= log (2πe)ddet Σ + σ2 I

2 n − log (2πe)ddet σ2
n I using the entropy of Gaussians (5.31)

2



164 probabilistic artificial intelligence

( )
1 det Σ + σ2 I

= log n
2 det(σ2

n I)
1 ( )

= log det I + σ−2Σ . (8.1 )
2 n 3

Intuitively, the larger the noise σ2
n in relation to the covariance

of X, the smaller the information gain.

8.2.1 Synergy and Redundancy

It is sometimes useful to write down the mutual information of X
and Y conditioned (in expectation) on a third random vector Z. This
leads us to the following definition.

Definition 8.5 (Conditional mutual information). The conditional mu-
tual information of X and Y given Z is defined as

I(X; Y | Z .
) = H[X | Z]−H[X | Y, Z]. (8.14)
= H[X, Z] + H[Y, Z]−H[Z]−H[X, Y, Z] (8.15) using the relationship of joint and

conditional entropy (8.5)
= I(X; Y, Z)− I(X; Z). (8.16)

Thus, the conditional mutual information corresponds to the reduction
of uncertainty in X when observing Y, given we already observed Z.
It also follows that conditional mutual information is symmetric:

I(X; Y | Z) = I(Y; X | Z) . (8.17)

We have seen in this chapter that entropy is monotonically decreasing
as we condition on new information, and called this the “information
never hurts” principle (8.8). However, the same does not hold for mu-
tual information! That is, information about a random vector Z may
reduce the mutual information between random vectors X and Y ? . Problem 8.2

Remark 8.6: Sufficient statistics and data processing inequality
A related concept is the data processing inequality (8.42) which
you prove in ? and which allows us to formalize a concept which Problem 8.2 (3)
we have seen multiple times already, namely the notion of a suf-
ficient statistic. Consider the Markov chain λ → X → s(X), for
example, λ may be parameters of the distribution of X. By the
data processing inequality (8.42), I(λ; s(X)) ≤ I(λ; X). If the data
processing inequality is satisfied with equality then s(X) is called
a sufficient statistic of X for the inference of λ.



active learning 165

To understand the behavior of mutual information under conditioning,
it is helpful to consider the interaction information

I(X; Y; Z .
) = I(X; Y)− I(X; Y | Z). (8.18)

If the interaction is positive then some information about X that is
provided by Y is also provided by Z (i.e., conditioning on Z decreases
MI between X and Y), and we say that there is redundancy between Y
and Z (with respect to X). Conversely, if the interaction is negative then
learning about Z increases what can be learned from Y about X, and
we say that there is synergy between Y and Z. We will see later in this
chapter that the absence of synergies can lead to efficient algorithms
for maximizing mutual information.

8.2.2 Mutual Information as Utility Function

Following our introduction of mutual information, it is natural to an-
swer the question “where should I collect data?” by saying “wherever
mutual information is maximized”. More concretely, assume we are
given a set X of possible observations of f , where yx denotes a single
such observation at x ∈ X ,

y .
x = fx + εx, (8.19)

f .
x = f (x), and εx is some zero-mean Gaussian noise. For a set of

observations S = {x1, . . . , xn},we c

y 1
.   an write yS = fS + ε where

yS =  x

 . ..  , f .
S =  fx

 1
. ..  , and ε ∼ N (0, σ2

n I).
yxn fxn

Note that both yS and fS are random vectors. Our goal is then to find
a subset S ⊆ X of size n maximizing the information gain between
our model f and yS.

This yields the maximization objective,

I(S .
) = I( fS; yS) = H[ fS]−H[ fS | yS]. (8.20)

Here, H[ fS] corresponds to the uncertainty about fS before obtain-
ing the observations yS and H[ fS | yS] corresponds to the uncertainty
about fS, in expectation, after obtaining the observations yS.

Remark 8.7: Making optimal decisions with intrinsic rewards
Note that this objective function maps neatly onto our initial con-
sideration of making optimal decisions under uncertainty from



166 probabilistic artificial intelligence

Section 1.4. In fact, you can think of maximizing I(S) simply as
computing the optimal decision rule for the utility

r(yS, S) = H[ fS]−H[ fS | YS = yS], (8.21) with YS = yS denoting an event

with I(S) = EyS [r(yS, S)] measuring the expected utility of ob-
servations yS. Such a utility or reward function is often called an
intrinsic reward since it does not measure an “objective” external
quantity, but instead a “subjective” quantity that is internal to the
model of f .

Observe that picking a subset of points S ⊆ X from the domain X is
a combinatorial problem. That is to say, we are optimizing a function
over discrete sets. In general, such combinatorial optimization prob-
lems tend to be very difficult. It can be shown that maximizing mutual
information is NP-hard.

8.3 Submodularity of Mutual Information

We will look at optimizing mutual information in the following sec-
tion. First, we want to introduce the notion of submodularity which is
important in the analysis of discrete functions.

Definition 8.8 (Marginal gain). Given a (discrete) function F : P(X )→ R,
the marginal gain of x ∈ X given A ⊆ X is defined as

∆F(x | A .
) = F(A ∪ {x})− F(A). (8.22)

Intuitively, the marginal gain describes how much “adding” the addi-
tional x to A increases the value of F.

When maximizing mutual information, the marginal gain is ? Problem 8.4

∆I(x | A) = I( fx; yx | yA) (8.23)
= H[yx | yA]−H[εx]. (8.24)

That is, when maximizing mutual information, the marginal gain cor-
x

responds to the difference between the uncertainty after observing yA
and the entropy of the noise H[εx]. Altogether, the marginal gain rep- A

B
resents the reduction in uncertainty by observing {x}. D

Definition 8.9 (Submodularity). A (discrete) function F : P(X ) → R Figure 8.3: Monotone submodularity.
The effect of “adding” x to the smaller

is submodular iff for any x ∈ X and any A ⊆ B ⊆ X it satisfies set A is larger than the effect of adding
x to the larger set B.

F(A ∪ {x})− F(A) ≥ F(B ∪ {x})− F(B). (8.25)



active learning 167

Equivalently, using our definition of marginal gain, we have that F is
submodular iff for any x ∈ X and any A ⊆ B ⊆ X ,

∆F(x | A) ≥ ∆F(x | B). (8.26)

That is, “adding” x to the smaller set A yields more marginal gain
than adding x to the larger set B. In other words, the function F has
“diminishing returns”. In this way, submodularity can be interpreted
as a notion of “concavity” for discrete functions.

Definition 8.10 (Monotone submodularity). A function F : P(X )→ R

is called monotone iff for any A ⊆ B ⊆ X it satisfies

F(A) ≤ F(B). (8.27)

If F is also submodular, then F is called monotone submodular.

Theorem 8.11. The objective I is monotone submodular.
Proof. We fix arbitrary subsets A ⊆ B ⊆ X and any x ∈ X . We have,

I is submodular ⇐⇒ ∆I(x | A) ≥ ∆I(x | B) by submodularity in terms of marginal
gain (8.26)

⇐⇒ H[yx | yA]−H[εx] ≥ H[yx | yB]−H[εx] using Equation (8.24)

⇐⇒ H[yx | yA] ≥ H[yx | yB]. H[εx] cancels

Due to the “information never hurts” principle (8.8) of entropy and as
A ⊆ B, this is always true. Moreover,

I is monotone ⇐⇒ I(A) ≤ I(B) by the definition of monotinicity (8.27)

⇐⇒ I( fA; yA) ≤ I( fB; yB) using the definition of I (8.20)

⇐⇒ I( fB; yA) ≤ I( fB; yB) using I( fB; yA) = I( fA; yA) as
yA ⊥ fB | fA

⇐⇒ H[ fB]−H[ fB | yA] ≤ H[ fB]−H[ fB | yB] using the definition of MI (8.9)

⇐⇒ H[ fB | yA] ≥ H[ fB | yB], H[ fB] cancels

which is also satisfied due to the “information never hurts” princi-
ple (8.8).

The submodularity of I can be interpreted from the perspective of
information theory. It turns out that submodularity is equivalent to
the absence of synergy between observations ? . Intuitively, without Problem 8.5
synergies, acting greedily is enough to find a near-optimal solution.
If there are synergies, then the combinatorial search problem is much
harder, because single-step optimal actions do not necessarily lead us
to the optimal solution. Consider the extreme case of having to solve
a “needle in a haystack” problem, where only a single subset of X
with size k achieves objective value 1, with all other subsets achieving
objective value 0. In this case, we can do nothing but exhaustively
search through all |X |k combinations to find the optimal solution.



168 probabilistic artificial intelligence

8.4 Maximizing Mutual Information

As we cannot efficiently pick a set S ⊆ X to maximize mutual informa-
tion but know that I is submodular, a natural approach is to maximize
mutual information greedily. That is, we pick the locations x1 through
xn individually by greedily finding the location with the maximal mu-
tual information. The following general result for monotone submod-
ular function maximization shows that, indeed, this greedy approach
provides a good approximation.

Theorem 8.12 (Greedy submodular function maximization). If the set
function F : P(X )→ R≥0 is monotone submodular, then greedily maximiz-
ing F is a (1− 1/e)-approximatio(n:2 ) 2 1− 1/e ≈ 0.632

F(Sn) ≥ 1− 1
max F(S). (8.28)

e S⊆X
|S|=n

Proof. Fix any n ≥ 1. Let S⋆ ∈ arg max{F(S) | S ∈ X , |S| ≤ n}. We
can assume |S⋆| = n due to the monotonicity (8.27) of F. We write,
{x⋆1 , . . . , x⋆n}

.
= S⋆. We have,

F(S⋆) ≤ F(S⋆ ∪ St) using monotonicity (8.27)
n

= F(St) + ∑ ∆F(x⋆i | St ∪ {x⋆1 , . . . , x⋆i−1}) using the definition of marginal gain
i=1 (8.22)

≤ F(St) + ∑ ∆F(x⋆ | St) using submodularity (8.26)
x⋆∈S⋆

≤ F(St) + n(F(St+1)− F(St)). using that St+1 = St ∪ {x} is chosen
such that ∆F(x | St) is maximized (8.29)

Let .
δt = F(S⋆)− F(St). Then,

δt = F(S⋆)− F(St) ≤ n(F(St+1)− F(St)) = n(δt − δt+1).

This implies δt+1 ≤ (1− 1/n)δt and δn ≤ (1− 1/n)nδ0 ≤ δ0/e, using the
well-known inequality 1− x ≤ e−x for all x ∈ R.

Finally, observe that δ0 = F(S⋆) − F(∅) ≤ F(S⋆) due to the non-
negativity of F. We obtain,

δn = F(S⋆)− F(Sn) ≤
δ0 ≤ F(S⋆)

.
e e

Rearranging the terms yields the theorem.

Optional Readings
The original proof of greedy maximization for submodular func-
tions was given by “An analysis of approximations for maximiz-



active learning 169

ing submodular set functions” (Nemhauser et al., 1978).

For more background on maximizing submodular functions, see
“Submodular function maximization” (Krause and Golovin, 2014).

Now that we have established that greedy maximization of mutual
information is a decent approximation to maximizing the joint infor-
mation of data, we will look at how this optimization problem can be
solved in practice.

8.4.1 Uncertainty Sampling

When maximizing mutual information, at time t when we have already
picked St = {x1, . . . , xt}, we need to solve the following optimization
problem,

x .
t+1 = arg max ∆I(x | St) (8.29)

x∈X
= arg max I( fx; yx | ySt). (8.30) using Equation (8.23)

x∈X

Note that fx and yx are univariate random variables. Thus, using
our formula for the mutual information of conditional linear Gaus-
sians (8.13), we can simplify to, ( )

1 σ2(x)
= arg max log 1 + t ( .31

x∈X 2 σ2 8 )
n

where σ2
t (x) is the (remaining) variance at x after observing St. As-

suming the label noise is independent of x (i.e., homoscedastic),

= arg max σ2
t (x). (8.32) y

x∈X

Therefore, if f is modeled by a Gaussian and we assume homoscedas-
tic noise, greedily maximizing mutual information corresponds to sim-
ply picking the point x with the largest variance. This algorithm is also
called uncertainty sampling.

x⋆
8.4.2 Heteroscedastic Noise x

Uncertainty sampling is clearly problematic if the noise is heteroscedas- Figure 8.4: Uncertainty sampling with

tic. If there are a particular set of inputs with a large aleatoric un- heteroscedastic noise. The epistemic un-
certainty of the model is shown in a dark

certainty dominating the epistemic uncertainty, uncertainty sampling gray. The aleatoric uncertainty of the
will continuously choose those points even though the epistemic un- data is shown in a light gray. Uncer-

tainty sampling would repeatedly pick
certainty will not be reduced substantially (cf. Figure 8.4). points around x⋆ as they maximize the

epistemic uncertainty, even though the
Looking at Equation (8.31) suggests a natural fix. Instead of only con- aleatoric uncertainty at x⋆ is much larger
sidering the epistemic uncertainty σ2

t (x), we can also consider the ale- than at the boundary.



170 probabilistic artificial intelligence

atoric uncertainty σ2
n(x) by(modeling h)eteroscedastic noise, yielding

. 1 σ2(x) σ2
xt+1 = arg max log 1 + t (x)

= arg max t
x∈X 2 σ2 . (8.33)

n(x) x∈X σ2
n(x)

Thus, we choose locations that trade large epistemic uncertainty with
large aleatoric uncertainty. Ideally, we find a location where the epis-
temic uncertainty is large, and the aleatoric uncertainty is low, which
promises a significant reduction of uncertainty around this location.

8.4.3 Classification

While we focused on regression, one can apply active learning also
for other settings, such as (probabilistic) classification. In this setting,
for any input x, a model produces a categorical distribution over la-
bels yx.3 Here, uncertainty sampling corresponds to selecting samples 3 see Section 1.3
that maximize the entropy of the predicted label yx,

x .
t+1 = arg max H[yx | x1:t, y1:t]. (8.34)

x∈X
The entropy of a categorical distribution is simply a finite sum of
weighted surprise terms.

Figure 8.5: Uncertainty sampling in
classification. The area with high
uncertainty (as measured by entropy)
is highlighted in yellow. The shown
figures display each a sequence of
model updates, each after one new ob-
servation. In the left figure, the classes
are well-separated and uncertainty is
dominated by epistemic uncertainty,
whereas in the right figure the uncer-
tainty is dominated by noise. In the
latter case, if we mostly choose points
x in the area of highest uncertainty (i.e.,
close to the decision boundary) to make
observations, the label noise results in

This approach generally leads to sampling points that are close to the frequently changing models.
decision boundary. Often, the uncertainty is mainly dominated by la-
bel noise rather than epistemic uncertainty, and hence, we do not learn
much from our observations. This is a similar problem to the one we
encountered with uncertainty sampling in the setting of heteroscedas-
tic noise.

This naturally suggests distinguishing between the aleatoric and epis-
temic uncertainty of the model f (parameterized by θ). To this end,
mutual information can be used similarly as we have done with un-
certainty sampling for regression,

x .
t+1 = arg max I(θ; yx | x1:t, y1:t) (8.35)

x∈X



active learning 171

= arg max I(yx; θ | x1:t, y1:t) using symmetry (8.11)
x∈X

= arg max H[yx | x1:t, y1:t]−H[yx | θ, x1:t, y1:t] using the definition of mutual
x∈X information (8.9)

= arg max H[yx | x1:t, y1:t]−Eθ|x [H[y | θ, x
1:t ,y1: x 1:t, y1:t]] (8.36) using the definition of conditional

x∈X t
entropy (8.2)

= arg max H[yx | :t, y1:t ,y y ] . (8.37) using the definition of entropy (5.27)
1:t 1:t

x∈X ︸ ︷x︷1 ︸]−Eθ|x H︸ [ ︷x︷| θ︸ and assuming y
y of ntropy of x ⊥ x

e 1:t, y1:t | θ
entrop

predictive posterior likelihood

The first term measures the entropy of the averaged prediction while
the second term measures the average entropy of predictions. Thus,
the first term looks for points where the average prediction is not con-
fident. In contrast, the second term penalizes points where many of
the sampled models are not confident about their prediction, and thus
looks for points where the models are confident in their predictions.
This identifies those points x where the models disagree about the la-
bel yx (that is, each model is “confident” but the models predict differ-
ent labels). For this reason, this approach is known as Bayesian active
learning by disagreement (BALD).

Note that the second term of the difference acts as a regularizer when
compared to Equation (8.34). The second term mirrors our description
of aleatoric uncertainty from Section 2.2. Recall that we interpreted ale-
atoric uncertainty as the average uncertainty for all models. Crucially,
here we use entropy to “measure” uncertainty, whereas previously we
have been using variance. Therefore, intuitively, Equation (8.36) sub-
tracts the aleatoric uncertainty from the total uncertainty about the
label.

Observe that both terms require approximate forms of the posterior
distribution. In Chapters 5 and 6, we have seen various approaches
from variational inference and MCMC methods which can be used
here. The first term can be approximated by the predictive distribu-
tion of an approximated posterior which is obtained, for example, us-
ing variational inference. The nested entropy of the second term is
typically easy to compute, as it corresponds to the entropy of the (dis-
crete) likelihood distribution p(y | x, θ) of the model θ. The outer
expectation of the second term may be approximated using (approxi-
mate) samples from the posterior distribution obtained via variational
inference, MCMC, or some other method.

Optional Readings
• Gal, Islam, and Ghahramani (2017).

Deep Bayesian active learning with image data.



172 probabilistic artificial intelligence

8.5 Learning Locally: Transductive Active Learning

So far we have explored how to select observations that provide us
with the best predictor f (x) across the entire domain x ∈ X . How-
ever, we typically utilize predictors to make predictions at a particular
location x⋆. It is therefore a natural question to ask how to select ob-
servations that lead to the best individual prediction f (x⋆) at the pre-
diction target x⋆. The distinction between these two settings is closely
related to the distinction between two general approaches to learning:
inductive learning and transductive learning. Inductive learning aims to
extract general rules from the data, that is, to extract and compress
the most bits of information. In contrast, transductive learning aims
to make the best prediction at a particular location, that is, to extract
the most bits of information relevant to the prediction at x⋆. The concept
of transduction was developed by Vladimir Vapnik in the 1980s who
described its essence as follows:

“When solving a problem of interest, do not solve a more general prob-
lem as an intermediate step. Try to get the answer that you really need
but not a more general one.” — Vladimir Vapnik

Remark 8.13: What are the prediction targets x⋆?
Typically, in transductive learning, we cannot directly observe the
value f (x⋆) at the prediction target x⋆, for example, when learn-
ing from a fixed dataset X ′ ⊂ X . If we could observe f (x⋆) di-
rectly (or perturbed by noise), solving the learning task would
only require memorization. Instead, most interesting learning
tasks require generalizing f (x⋆) from the behavior of f at other
locations. Therefore, transductive learning becomes interesting
precisely when we cannot directly observe f (x⋆).4 4 We will discuss an example of this kind

in Problem 8.6.
Note that this is similar to the inductive setting where we, in prin-
ciple, could observe f (x) at any location x, but practically, we can
only observe f (x) at a finite number of locations. Since such an in-
ductive model is then used to make predictions at any location x⋆,
it also needs to generalize from the observations.

Following the transductive paradigm, when already knowing that our
goal is to predict f (x⋆) our objective is to select observations that
provide the most information about f (x⋆). We can express this ob-
jective elegantly using the probabilistic framework from this chapter
(MacKay, 1992; Hübotter et al., 2024):

x .
t+1 = arg max I( fx⋆ ; yx | x1:t, y1:t) = arg min H[ fx⋆ | x1:t, y1:t, yx].

x∈X ′ x∈X ′
(8.38)



active learning 173

Hübotter et al. (2024) show that this objective leads to a remarkably
different selection of observations compared to the inductive active
learning objective we discussed earlier. Indeed, while the inductive
objective focuses on selecting a diverse set of examples, the transductive
objective also takes into account the relevance of the examples to the
prediction target x⋆. We can see this tradeoff between diversity and
relevance by rewriting the transductive objective as

I( fx⋆ ; yx | x1:t, y1:t) = I︸( fx︷⋆︷; yx︸)− I︸( fx⋆ ; yx;︷y︷1:t | x1:t︸) (8.39) using the definition of interaction
information (8.18)

relevance redundancy

where the first term measures the information gain of yx about fx⋆

while the second term is the interaction information which measures
the redundancy of the information in yx and y1:t about fx⋆ . In this
way, transductive active learning describes a middle ground between
traditional search and retrieval methods that focus on relevance on the
one hand and inductive active learning which focuses on diversity on
the other hand.

Optional Readings
• Hübotter, Sukhija, Treven, As, and Krause (2024).

Transductive active learning: Theory and applications.
In modern machine learning, one often differentiates between a
“pre-training” and a “fine-tuning” stage. During pre-training, a
model is trained on a large dataset to extract general knowledge
without a specific task in mind. Then, during fine-tuning, the
model is adapted to a specific task by training on a smaller dataset.
Whereas (inductive) active learning is closely linked to the pre-
training stage, transductive active learning has been shown to be
useful for task-specific fine-tuning:
• Hübotter, Bongni, Hakimi, and Krause (2025).

Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs.
• Bagatella, Hübotter, Martius, and Krause (2024).

Active Fine-Tuning of Generalist Policies.

Discussion

We have discussed how to select the most informative data. Thereby,
we focused mostly on inductive active learning which is applicable to
“pre-training” when we aim to extract general knowledge from data,
but also explored transductive active learning which is useful for “fine-
tuning” when we aim to adapt a model to a specific task.

We focused on quantifying the “informativeness” of data by its infor-
mation gain, which is a common approach, though many other viable



174 probabilistic artificial intelligence

criteria exist:

Remark 8.14: Beyond mutual information
The problem of identifying which experiments to conduct in order
to maximize learning is studied extensively in the field of experi-
mental design where a set of observations S is commonly called a
design. In the presence of a prior and likelihood, and a different
possible posterior distribution for each design S, the field is also
called Bayesian experimental design (Chaloner and Verdinelli, 1995;
Rainforth et al., 2024; Mutnỳ, 2024).

As we highlighted in the beginning of this chapter, how we mea-
sure the utility (i.e., the informativeness) of a design S is crucial.
Such a measure is called a design criterion and a design which
is optimal with respect to a design criterion is called an optimal
design. The literature studies various design criteria beyond max-
imizing mutual information (i.e., minimizing posterior entropy).
One popular alternative is to select the observations S that min-
imize the trace of the posterior covariance matrix, which corre-
sponds to minimizing the posterior average variance.

Next, we will move to the topic of optimization and ask which data we
should select to find the optimum of an unknown function as quickly
as possible. In the following chapter, we will focus on “Bayesian opti-
mization” (also called “bandit optimization”) where our aim is to find
and sample the optimal point. A related task that is slightly more re-
lated to active learning is the “best-arm identification” problem where
we aim only to identify the optimal point without sampling it. This
problem is closely related transductive active learning (with the lo-
cal task being defined by the location of the maximum) and so-called
entropy search methods that minimize the entropy of the posterior dis-
tribution over the location or value of the maximum (akin to Equa-
tion (8.38)) are often used to solve this problem (Hennig and Schuler,
2012; Wang and Jegelka, 2017; Hvarfner et al., 2022).

Problems

8.1. Mutual information and KL divergence.

Show that by expanding the definit[ion of mutual ]information,

p(x, y)
I(X; Y) = E(x,y)∼p log

p(x)p(y)
= KL(p(x, y)∥p(x)p(y)) (8.40)
= Ey∼p[KL(p(x | y)∥p(x))], (8.41)



active learning 175

where p(x, y) denotes the joint distribution and p(x), p(y) denote the
marginal distributions of X and Y.

8.2. Non-monotonicity of cond. mutual information.
1. Show that if X ⊥ Z then I(X; Y | Z) ≥ I(X; Y).
2. Show that if X ⊥ Z | Y then I(X; Y | Z) ≤ I(X; Y).
3. Note that the condition X ⊥ Z | Y is the Markov property, namely,
{X, Y, Z} form a Markov chain with graphical model X → Y → Z.
This situation often occurs when data is processed sequentially.
Prove

I(X; Z) ≤ I(X; Y). (8.42)

which is also known as the data processing inequality, and which
says that processing cannot increase the information contained in
a signal.

8.3. Interaction information.
1. Show that interaction information is symmetric.
2. Let X1, X2 ∼ Bern(p) for some p ∈ (0, 1) and independent. We

denote by Y .
= X1 ⊕ X2 their XOR. Compute I(Y; X1; X2).

8.4. Marginal gain of maximizing mutual information.

Show that in the context of maximizing mutual information, the marginal
gain is

∆I(x | A) = I( fx; yx | yA) = H[yx | yA]−H[εx].

8.5. Submodularity means no synergy.

Show that submodularity is equivalent to the absence of synergy be-
tween observations. That is, show that for all A ⊆ B,

I( fx; yx; yB\A | yA) ≥ 0. (8.43)

8.6. Transductive active learning.

Consider the prior distribution Xi ∼ N (0, 1) with all Xi independent
and consider the “output” variable

100
Z .
= ∑ i · Xi.

i=1

Our observation when Xit is selected in round t is generated by

Y . i
= X id

t it + εt with εt ∼N (0, 1).



176 probabilistic artificial intelligence

For a set of inputs S = {i1, . . . , it} ⊆ {1, . . . , 100}, we define the infor-
mation gain of S as

F .
(S) = I(Z; Y1:t) = H[Z]−H[Z | Y1:t]

where H[Z] is the entropy of Z according to the prior and H[Z | Y1:t]
is the conditional entropy after the observations Y1, . . . , Yt.

Note: The random vector (X1, . . . , X100, Z) is jointly Gaussian due to the
closedness of Gaussians under linear transformations.
1. We define the marginal information gain ∆(j | S) for a new observa-

tion Yj as
∆ .
(j | S) = F(S ∪ {j})− F(S).

Does ∆(j | S) ≥ 0 hold for all j and S?
2. Is maximizing the marginal information gain ∆(i | S) equivalent to

picking the point i ∈ {1, . . . , 100} with maximum variance under
the current posterior over Xi, that is, “equivalent to uncertainty
sampling”?

3. Let us consider the alternative prior where Xi ∼ Bern(0.5) are fair
coin flips which we observe directly (i.e., Yt = Xit ). For which of
the following definitions of Z is the acquisition function S 7→ F(S)
submodular?
(a) Z .

= X1 ∧ · · · ∧ X100 with ∧ denoting the logical AND.
(b) Z .

= X1 ∨ · · · ∨ X100 with ∨ denoting the logical OR.
(c) Z .

= X1 ⊕ · · · ⊕ X100 with ⊕ denoting the logical XOR, i.e., the
exclusive OR which returns 1 iff exactly one of the Xi is 1 and
0 otherwise.



9
Bayesian Optimization

Often, obtaining data is costly. In the previous chapter, this led us
to investigate how we can optimally improve our understanding (i.e.,
reduce uncertainty) of the process we are trying to model. However,
purely improving our understanding is often not good enough. In
many cases, we want to use our improving understanding simultane-
ously to reach certain goals. This is a very common problem in artificial xt yt = f ⋆(xt) + ϵt

intelligence and will concern us for the rest of this manuscript. One f ⋆
common instance of this problem is the setting of optimization. Figure 9.1: Illustration of Bayesian op-

timization. We pass an input xt into the
Given some function f ⋆ : X → R, suppose we want to find the unknown function f ⋆ to obtain noisy ob-

servations yt.
arg max f ⋆(x). (9.1)

x∈X
Now, contrary to classical optimization, we are interested in the setting
where the function f ⋆ is unknown to us (like a “black-box”). We are
only able to obtain noisy observations of f ⋆,

yt = f ⋆(xt) + εt. (9.2)

Moreover, these noise-perturbed evaluations are costly to obtain. We
will assume that similar alternatives yield similar results,1 which is 1 That is, f ⋆ is “smooth”. We will be
commonly encoded by placing a Gaussian process prior on f ⋆. This more precise in the subsequent parts of

this chapter. If this were not the case,
assumed correlation is fundamentally what will allow us to learn a optimizing the function without evaluat-
model of f ⋆ from relatively few samples.2 ing it everywhere would not be possible.

Fortunately, many interesting functions
obey by this relatively weak assumption.

9.1 Exploration-Exploitation Dilemma 2 There are countless examples of this
problem in the “real world”. Instances
are

In Bayesian optimization, we want to learn a model of f ⋆ and use • drug trials
this model to optimize f ⋆ simultaneously. These goals are somewhat • chemical engineering — the develop-
contrary. Learning a model of f ⋆ requires us to explore the input space ment of physical products

• recommender systems
while using the model to optimize f ⋆ requires us to focus on the most • automatic machine learning — auto-
promising well-explored areas. This trade-off is commonly known as matic tuning of model & hyperpa-

rameters
the exploration-exploitation dilemma, whereby • and many more...



178 probabilistic artificial intelligence

• exploration refers to choosing points that are “informative” with re-
spect to the unknown function. For example, points that are far
away from previously observed points (i.e., have high posterior vari-
ance);3 and 3 We explored this topic (with strategies

• exploitation refers to choosing promising points where we expect the like uncertainty sampling) in the previ-
ous chapter.

function to have high values. For example, points that have a high
posterior mean and a low posterior variance.

In other words, the exploration-exploitation dilemma refers to the chal-
lenge of learning enough to understand f ⋆, but not learning too much
to lose track of the objective — optimizing f ⋆.

The exploration-exploitation dilemma is yet another example of the
principle of curiosity and conformity which we introduced in Section 5.5.2
and encountered many times since in our study of approximate prob-
abilistic inference. We will see in subsequent chapters that sequential
decision-making is intimately related to probabilistic inference, and
there we will also make this correspondence more precise.

9.2 Online Learning and Bandits

Bayesian optimization is closely related to a form of online learning.
In online learning we are given a set of possible inputs X and an un-
known function f ⋆ : X → R. We are now asked to choose a sequence
of inputs x1, . . . xT online,4 and our goal is to maximize our cumu- 4 Online is best translated as “sequen-
lative reward ∑T

t=1 f ⋆(xt). Depending on what we observe about f ⋆, tial”. That is, we need to pick xt+1 based
only on our prior observations y1, . . . , yt.

there are different variants of online learning. Bayesian optimization
is closest to the so-called (stochastic) “bandit” setting.

9.2.1 Multi-Armed Bandits

The “multi-armed bandits” (MAB) problem is a classical, canonical for-
malization of the exploration-exploitation dilemma. In the MAB prob-
lem, we are provided with k possible actions (arms) and want to maxi-
mize our reward online within the time horizon T. We do not know the
reward distributions of the actions in advance, however, so we need to
trade learning the reward distribution with following the most promis-
ing action. Bayesian optimization can be interpreted as a variant of the
MAB problem where there can be a potentially infinite number of ac-
tions (arms), but their rewards are correlated (because of the smooth-
ness of the Gaussian process prior). Figure 9.2: Illustration of a multi-armed

bandit with four arms, each with a dif-
There exists a large body of work on this and similar problems in on- ferent reward distribution. The agent
line decision-making. Much of this work develops theory on how to tries to identify the arm with the most

explore and exploit in the face of uncertainty. The shared prevalence beneficial reward distribution shown in
green.

of the exploration-exploitation dilemma signals a deep connection be-



bayesian optimization 179

tween online learning and Bayesian optimization (and — as we will
later come to see — reinforcement learning). Many of the approaches
which we will encounter in the context of these topics are strongly
related to methods in online learning.

One of the key principles of the theory on multi-armed bandits and
reinforcement learning is the principle of “optimism in the face of un-
certainty”, which suggests that it is a good guideline to explore where
we can hope for the best outcomes. We will frequently come back
to this general principle in our discussion of algorithms for Bayesian
optimization and reinforcement learning.

9.2.2 Regret

The key performance metric in online learning is the regret.

Definition 9.1 (Regret). The (cumulative) regret for a time horizon T
associated with choices {xt}T

t=1 is defined as

T ( )
R .

T = ∑ max f ⋆(x)− f ⋆(
t=1 ︸ x ︷︷ xt) ︸ (9.3)

instantaneous regret
T

= T max f ⋆(x)−∑ f ⋆(xt). (9.4)
x t=1

The regret can be interpreted as the additive loss with respect to the
static optimum maxx f ⋆(x).

The goal is to find algorithms that achieve sublinear regret, 5 Metrical task systems are a classical ex-
ample in online algorithms. Suppose we

R
lim T = 0. (9.5) are moving in a (finite) decision space X .

T→∞ T In each round, we are given a “task”
ft : X → R which is more or less costly

Importantly, if we use an algorithm which explores forever, e.g., by go- depending on our state xt ∈ X . In many
ing to a random point x̃ with a constant probability ϵ in each round, contexts, it is natural to assume that it

is also costly to move around in the de-
then the regret will grow linearly with time. This is because the instan- cision space. This cost is modeled by a
taneous regret is at least ϵ(maxx f ⋆(x) − f ⋆(x̃)) and non-decreasing. metric d(·, ·) on X . In metrical task sys-

Conversely, if we use an algorithm which never explores, then we tems, we want to minimize our total cost,

might never find the static optimum, and hence, also incur constant T
∑ ft(xt) + d(xt, xt−1).

instantaneous regret in each round, implying that regret grows lin- t=1

early with time. Thus, achieving sublinear regret requires balancing That is, we want to trade completing our
tasks optimally with moving around in

exploration and exploitation. the state space. Crucially, we do not
know the sequence of tasks ft in ad-

Typically, online learning (and Bayesian optimization) consider sta- vance. Due to the cost associated with
tionary environments, hence the comparison to the static optimum. moving in the decision space, previous

Dynamic environments are studied in online algorithms (see metrical choices affect the future!

task systems5, convex function chasing6, and generalizations of multi- 6 Convex function chasing (or convex body
armed bandits to changing reward distributions) and reinforcement chasing) generalize metrical task systems

to continuous domains X . To make
any guarantees about the performance
in these settings, one typically has to
assume that the tasks ft are convex.
Note that this mirrors our assumption in
Bayesian optimization that similar alter-
natives yield similar results.



180 probabilistic artificial intelligence

learning. When operating in dynamic environments, other metrics
such as the competitive ratio,7 which compares against the best dy- 7 To assess the performance in dynamic
namic choice, are useful. As we will later come to see in Section 13.1 in environments, we typically compare to

a dynamic optimum. As these problems
the context of reinforcement learning, operating in dynamic environ- are difficult (we are usually not able to
ments is deeply connected to a rich field of research called control. guarantee convergence to the dynamic

optimum), one considers a multiplica-
tive performance metric similar to the

9.3 Acquisition Functions approximation ratio, the competitive ratio,

cost(ALG) ≤ α · cost(OPT),
It is common to use a so-called acquisition function to greedily pick the where OPT corresponds to the dynamic
next point to sample based on the current model. optimal choice (in hindsight).

Throughout our description of acquisition functions, we will focus on a
setting where we model f ⋆ using a Gaussian process which we denote
by f . The methods generalize to other means of learning f ⋆ such as
Bayesian deep learning. The various acquisition functions F are used
in the same way as is illustrated in Algorithm 9.2.

Algorithm 9.2: Bayesian optimization (with GPs)
1 initialize f ∼ GP(µ0, k0)
2 for t = 1 to T do
3 choose xt = arg maxx∈X F(x; µt−1, kt−1)
4 observe yt = f (xt) + ϵt
5 perform a probabilistic update to obtain µt and kt

Remark 9.3: Model selection
Selecting a model of f ⋆ in sequential decision-making is much
harder than in the i.i.d. data setting of supervised learning. There
are mainly the following two dangers:
• the data sets collected in active learning and Bayesian optimiza-

tion are small; and
• the data points are selected dependently on prior observations.
This leads to a specific danger of overfitting. In particular, due to
feedback loops between the model and the acquisition function,
one may end up sampling the same point repeatedly.

One approach to reduce the chance of overfitting is the use of
hyperpriors which we mentioned previously in Section 4.4.2. An-
other approach that often works fairly well is to occasionally (ac-
cording to some schedule) select points uniformly at random in-
stead of using the acquisition function. This tends to prevent get-
ting stuck in suboptimal parts of the state space.



bayesian optimization 181

One possible acquisition function is uncertainty sampling (8.31), which
we discussed in the previous chapter. However, this acquisition func-
tion does not at all take into account the objective of maximizing f ⋆
and focuses solely on exploration.

Suppose that our model f of f ⋆ is well-calibrated, in the sense that y
the true function lies within its confidence bounds. Consider the best
lower bound, that is, the maximum of the lower confidence bound. 4

Now, if the true function is really contained in the confidence bounds,
it must hold that the optimum is somewhere above this best lower 2

bound. In particular, we can exclude all regions of the domain where
the upper confidence bound (the optimistic estimate of the function 0

value) is lower than the best lower bound. This is visualized in Fig-
ure 9.3. 0 2 4

x
Therefore, we only really care how the function looks like in the re-

Figure 9.3: Optimism in Bayesian op-
gions where the upper confidence bound is larger than the best lower timization. The unknown function is
bound. The key idea behind the methods that we will explore is to shown in black, our model in blue with
focus exploration on these plausible maximizers. gray confidence bounds. The dotted

black line denotes the maximum lower

Note that it is crucial that our uncertainty about f reflects the “fit” of bound. We can therefore focus our ex-
ploration to the yellow regions where

our model to the unknown function. If the model is not well calibrated the upper confidence bound is higher
or does not describe the underlying function at all, these methods will than the maximum lower bound.

perform poorly. This is where we can use the Bayesian philosophy by
imposing a prior belief that may be conservative.

9.3.1 Upper Confidence Bound

The principle of optimism in the face of uncertainty suggests picking the 2.5 4

point where we can hope for the optimal outcome. In this setting, this
2.0

corresponds to simply maximizing the upper confidence bound (UCB), 2
1.5

x .
t+1 = arg max µt(x) + βt+1σt(x), (9.6) 1.0 0

x∈X
0.5

where . √
σt(x) = kt(x, x) is the standard deviation at x and βt regulates

4
how confident we are about our model f (i.e., the choice of confidence 3.5

interval). 3.0 2

This acquisition function naturally trades exploitation by preferring 2.5

a large posterior mean with exploration by preferring a large poste- 0
2.0

rior variance. Note that if βt = 0 then UCB is purely exploitative,
whereas, if βt → ∞, UCB recovers uncertainty sampling (i.e., is purely

Figure 9.4: Plot of the UCB acquisition
explorative).8 UCB is an example of an optimism-based method, as it function for β = 0.25 and β = 1, respec-
greedily picks the point where we can hope for the best outcome. tively.

8 Due to the monotonicity of (·)2, it does
As can be seen in Figure 9.4, the UCB acquisition function is gener- not matter whether we optimize the vari-
ally non-convex. For selecting the next point, we can use approximate ance or standard deviation at x.

acquisition function acquisition function

GP posterior GP posterior



182 probabilistic artificial intelligence

global optimization techniques like Lipschitz optimization (in low di-
mensions) and gradient ascent with random initialization (in high di-
mensions). Another widely used technique is to sample some random
points from the domain, score them according to this criterion, and
simply take the best one.

The choice of βt is crucial for the performance of UCB. Intuitively,
for UCB to work even if the unknown function f ⋆ is not contained in
the confidence bounds, we use βt to re-scale the confidence bounds
to enclose f ⋆ as shown in Figure 9.5. A theoretical analysis requires y

that βt is chosen “correctly”. Formally, we say that the sequence βt is 3

chosen correctly if it leads to well-calibrated confidence intervals, that is, 2
if with probability at least 1− δ,

1

∀t ≥ 1, ∀x ∈ X : f ⋆(x) ∈ Ct(x .
) = [µt−1(x)± βt(δ) · σt−1(x)]. (9.7) 0

−1
Bounds on βt(δ) can be derived both in a “Bayesian” and in a “fre-
quentist” setting. In the Bayesian setting, it is assumed that f ⋆ is 0 2 4

x
drawn from the prior GP, i.e., f ⋆ ∼ GP(µ0, k0). However, in many
cases this may be an unrealistic assumption. In the frequentist setting, Figure 9.5: Re-scaling the confidence

it is assumed instead that f ⋆ is a fixed element of a reproducing kernel bounds. The dotted gray lines represent
updated confidence bounds.

Hilbert space Hk(X ) which depending on the kernel k can encompass
a large class of functions. We will discuss the Bayesian setting first and
later return to the frequentist setting.

Theorem 9.4 (Bayesian confidence intervals, lemma 5.5 of Srinivas
et al. (2010)). ? Let δ ∈ (0, 1). Assuming f ⋆ ∼ GP(µ0, k0) and Gaussian Problem 9.2
observation noise ϵt ∼ N (0, σ2

n), th(e√sequence )
βt(δ) = O log(|X |t/δ) (9.8)

satisfies P(∀t ≥ 1, x ∈ X : f ⋆(x) ∈ Ct(x)) ≥ 1− δ.

Under the assumption of well-calibrated confidence intervals, we can
bound the regret of UCB.

Theorem 9.5 (Regret of GP-UCB, theorem 2 of Srinivas et al. (2010)).
? If βt(δ) is chosen “correctly” for a fixed δ ∈ (0, 1), with probability at Problem 9.3

least 1− δ, greedily choosing the upper confidence bound yields cumulative
regret ( √ )

RT = O βT(δ) γTT (9.9)

where (
. 1 )

γT = max I( f
X S; yS) = max log det I + σ−2

n KSS (9.10)
S⊆ S⊆X 2
|S|=T |S|=T

is the maximum information gain after T rounds.



bayesian optimization 183

Observe that if the information gain is sublinear in T then we achieve
sublinear regret and, in particular, converge to the true optimum. The
information gain γT measures how much can be learned about f ⋆
within T rounds. If the function is assumed to be smooth (perhaps
even linear), then the information gain is smaller than if the function
was assumed to be “rough”. Intuitively, the smoother the functions
encoded by the prior, the smaller is the class of functions to choose
from and the more can be learned from a single observation about
“neighboring” points.

Theorem 9.6 (Information gain of common kernels, theorem 5 of Srini-
vas et al. (2010) and remark 2 of Vakili et al. (2021)). Due to submodular-
ity, we have the following bounds on the information gain of common kernels:
• linear kernel

γT = O(d log T), (9.11) 40

• Gaussian kernel ( ) 20

γT = O (log T)d+1 , (9.12)
0

• Matérn kernel for ν > 1 ( ) 20 40
2 T

d 2ν
γT = O T 2ν+d (log T) 2ν+d . (9.13) Figure 9.6: Information gain of inde-

pendent, linear, Gaussian, and Matérn
The information gain of common kernels is illustrated in Figure 9.6. (ν ≈ 0.5) kernels with d = 2 (up to con-
Notably, when all points in the domain are independent, the informa- stant factors). The kernels with sublin-

ear information gain have strong dimin-
tion gain is linear in T. This is because when the function f ⋆ may be ishing returns (due to their strong de-
arbitrarily “rough”, we cannot generalize from a single observation to pendence between “close” points). In

contrast, the independent kernel has no
“neighboring” points, and as there are infinitely many points in the dependence between points in the do-
domain X there are no diminishing returns. As one would expect, in main, and therefore no diminishing re-
this case, Theorem turns. Intuitively, the “smoother” the

9.5 does not yield sublinear regret. However, we
class of functions modeled by the kernel,

can see from Theorem 9.6 that the information gain is sublinear for lin- the stronger are the diminishing returns.
ear, Gaussian, and most Matérn kernels. Moreover, observe that unless
the function is linear, the information gain grows exponentially with
the dimension d. This is because the number of “neighboring” points
(with respect to Euclidean geometry) decreases exponentially with the
dimension which is also known as the curse of dimensionality.

As mentioned, the size of the confidence intervals can also be analyzed
under a frequentist assumption on f ⋆.

Theorem 9.7 (Frequentist confidence intervals, theorem 2 of Chowd-
hury and Gopalan (2017)). Let δ ∈ (0, 1). Assuming f ⋆ ∈ Hk(X ), we
have that with probability at least 1− δ√, the sequence

βt(δ) = ∥ f ⋆∥k + σn 2(γt + log(1/δ)) (9.14)

satisfies P(∀t ≥ 1, x ∈ X : f ⋆(x) ∈ Ct(x)) ≥ 1− δ.

upper bound on γT



184 probabilistic artificial intelligence

That is, βt depends on the information gain of the kernel as well as on
the “complexity” of f ⋆ which is measured in terms of its norm in the
underlying reproducing kernel Hilbert space Hk(X ).

Remark 9.8: Bayesian vs frequentist assumption
Theorems 9.4 and 9.7 provide different bounds on βt(δ) based
on fundamentally different assumptions on the ground truth f ⋆:
The Bayesian assumption is that f ⋆ is drawn from the prior GP,
whereas the frequentist assumption is that f ⋆ is an element of a
reproducing kernel Hilbert spaceHk(X ). The frequentist assump-
tion holds uniformly for all functions f ⋆ with ∥ f ⋆∥k < ∞, whereas
the Bayesian assumption holds only under the Bayesian “belief”
that f ⋆ is drawn from the prior GP.

Interestingly, neither assumption encompasses the other. This is
because if f ∼ GP(0, k) then it can be shown that almost surely
∥ f ∥k = ∞, which implies that f ̸∈ Hk(X ) (Srinivas et al., 2010).

We remark that Theorem 9.7 holds also under the looser assumption
that observations are perturbed by σn-sub-Gaussian noise (cf. Equa-
tion (A.39)) instead of Gaussian noise. The bound on γT from Equa-
tion (9.13) for the Matérn kernel does not yield sublinear regret when
combined with the standard regret bound from Theorem 9.5, however,
Whitehouse et al. (2024) show that the regret of GP-UCB is sublinear
also in this case provided σ2

n is chosen carefully.

This concludes our discussion of the UCB algorithm. We have seen
that its regret can be analyzed under both Bayesian and frequentist
assumptions on f ⋆.

9.3.2 Improvement

Another well-known family of methods is based on keeping track of
a running optimum f̂t, and scoring points according to their improve-
ment upon the running optimum. The improvement of x after round t
is measured by

I .
t(x) = ( f (x)− f̂t)+ (9.15)

where we use (·)+ to denote max{0, ·}.
The probability of improvement (PI) picks the point that maximizes the
probability to improve upon the running optimum,

x .
t+1 = arg max P(It(x) > 0 | x1:t, y1:t) (9.16)

x∈X
= arg max P( f (x) > f̂t | x1:t, y1:t) (9.17)

x∈X



bayesian optimization 185

( )
µ ( ) f̂

= arg max Φ t x − t (9.18) using linear transformations of
x∈X σt(x) Gaussians (1.78)

where Φ denotes the CDF of the standard normal distribution and we
use that f (x) | x1:t, y1:t ∼ N (µt(x), σ2

t (x)). Probability of improvement
tends to be biased in favor of exploitation, as it prefers points with
large posterior mean and small posterior variance which is typically 0.5
true “close” to the previously observed maximum f̂t. 4

0.4

Probability of improvement looks at how likely a point is to improve 0.3 2
upon the running optimum. An alternative is to look at how much a
point is expected to improve upon the running optimum. This acqui- 0.2

0
sition function is called the expected improvement (EI), 0.1

. 0.5
xt+1 = arg max E[It(x) | x1:t, y1:t] . (9.19) 4

x∈X 0.4

Intuitively, EI seeks a large expected improvement (exploitation) while 0.3
2

also preferring states with a large variance (exploration). Expected im- 0.2

provement yields the same regret bound as UCB (Nguyen et al., 2017). 0.1 0

The expected improvement acquisition function is often flat which 0.0

makes it difficult to optimize in practice due to vanishing gradients. Figure 9.7: Plot of the PI and EI acquisi-
One approach addressing this is to instead optimize the logarithm of tion functions, respectively.

EI (Ament et al., 2024).

1.0

0.5

0.0

−0.5

−1.0
0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0

σt σt σt

Figure 9.8: Contour lines of acquisition
functions for varying ∆t = µt(x) − f̂t

9.3.3 Thompson Sampling and σt. A brighter color corresponds to
a larger acquisition function value. The

We can also interpret the principle of optimism in the face of uncertainty first graph shows contour lines of UCB
with βt = 0.75, the second of PI, and the

in a slightly different way than we did with UCB (and EI). Suppose third of EI.
we select the next point according to the probability that it is optimal
(assuming that the posterior distribution is an accurate representation
of the uncertainty), ( )

π(x | x1:t, y .
1:t) = P f |x f ( ) m x f (

1:t ,y x = a x′) (9.20)
1:t x′

∆t

acquisition function acquisition function

GP posterior GP posterior



186 probabilistic artificial intelligence

xt+1 ∼ π(· | x1:t, y1:t). (9.21)

This approach of sampling according to the probability of maximality π
is called probability matching. Probability matching is exploratory as it
prefers points with larger variance (as they automatically have a larger
chance of being optimal), but at the same time exploitative as it ef-
fectively discards points with low posterior mean and low posterior
variance. Unfortunately, it is generally difficult to compute π analyti-
cally given a posterior.

Instead, it is common to use a sampling-based approximation of π.
Observe that the density π can be[expressed as an expec]tation,

π(x | x1:t, y1:t) = E 1{ f (x) = max f (x′f |x1:t ,y )} , (9.22)
1:t x′

which we can approximate using Monte Carlo sampling (typically us-
ing a single sample),

≈ 1{ f̃t+1(x) = max f̃t+1(x′)} (9.23)
x′

where f̃t+1 ∼ p(· | x1:t, y1:t) is a sample from our posterior distri-
bution. Observe that this approximation of π coincides with a point
density at the maximizer of f̃t+1.

The resulting algorithm is known as Thompson sampling. At time t + 1,
we sample a function f̃t+1 ∼ p(· | x1:t, y1:t) from our posterior distri-
bution. Then, we simply maximize f̃t+1,

x .
t+1 = arg max f̃t+1(x). (9.24)

x∈X

In many cases, the randomness in the realizations of f̃t+1 is already
sufficient to effectively trade exploration and exploitation. Similar re-
gret bounds to those of UCB can also be established for Thompson
sampling (Russo and Van Roy, 2016; Kandasamy et al., 2018).

9.3.4 Information-Directed Sampling

After having looked at multiple methods that aim to balance exploita-
tion of the current posterior distribution over f for immediate returns
with exploration to reduce uncertainty about f for future returns, we
will next discuss a method that makes this tradeoff explicit.

Denoting the instantaneous regret of choosing x as ∆(x .
) = maxx′ f ⋆(x′)−

f ⋆(x) and by It(x) some function capturing the “information gain” as-
sociated with observing x in iteration t + 1, we can define the informa-
tion ratio,

Ψ . ∆(x)2
t(x) = , (9.25)

It(x)



bayesian optimization 187

which was originally introduced by Russo and Van Roy (2016). Here
exploitation reduces regret while exploration increases information
gain, and hence, points x that minimize the information ratio are those
that most effectively balance exploration and exploitation. We can
make the key observation that the regret ∆(·) decreases when It(·) de-
creases, as a small It(·) implies that the algorithm has already learned
a lot about the function f ⋆. The strength of this relationship is quanti-
fied by the information ratio:

Theorem 9.9 (Proposition 1 of Russo and Van Roy (2014) and theo-
rem 8 of Kirschner and Krause (2018)). For any iteration T ≥ 1, let
∑T

t=1 It−1(xt) ≤ γT and suppose that Ψt−1(xt) ≤ ΨT for all t ∈ [T]. Then,
the cumulative regret is bounded by√

RT ≤ γTΨTT. (9.26)

√
Proof. By Equation (9.25), rt = ∆(xt) = Ψt−1(xt) · It−1(xt). Hence,

T
RT = ∑ rt

t=1
T √

=√∑ Ψt−1(xt) · It−1(xt)
t√√=1

≤√ T T
∑ Ψt−1(xt) ·∑ It−1(xt) using the Cauchy-Schwarz inequality

√ t=1 t=1

≤ γTΨTT. using the assumptions on It(·) and
Ψt(·)

Example 9.10: How to measure “information gain”?
One possibility of measuring the “information gain” is

I .
t(x) = I( fx; yx | x1:t, y1:t) (9.27)

which — as you may recall — is precisely the marginal gain of
the utility I(S) = I( fS; yS) we were studying in Chapter 8. In this
case,

T T
∑ It−1(xt) = ∑ I( fxt ; yxt | x1:t−1, y1:t−1)
t=1 t=1

T
= ∑ I( fx1:T ; yxt | x1:t−1, y1:t−1) using I(X, Z; Y) ≥ I(X; Y) which follows

t=1 from Equations (8.12) and (8.16) and is
called monotonicity of MI

= I( fx1:T ; yx1:T ) by repeated application of
Equation (8.16), also called the chain rule

≤ γ of M
T . by dIefinition of γT (9.10)



188 probabilistic artificial intelligence

The regret bound from Theorem 9.9 suggests an algorithm which in
each iteration chooses the point which minimizes the information ratio
(9.25). However, this is not possible since ∆(·) is unknown due to
its dependence on f ⋆. Kirschner and Krause (2018) propose to use a
surrogate to the regret which is based on the current model of f ⋆,

∆̂ .
t(x) = max u (x′)− l (

x′∈X t t x). (9.28)

Here, u . .
t(x) = µt(x) + βt+1σt(x) and lt(x) = µt(x) − βt+1σt(x) are

the upper and lower confidence bounds of the confidence interval
Ct(x) of f ⋆(x), respectively. Similarly to our discussion of UCB, we
make the assumption that the sequence βt is chosen “correctly” (cf.
Equation (9.7)) so that the confidence interval is well-calibrated and
∆(x) ≤ ∆̂t(x) with high probabili{ty. The resulting }algorithm

.
= arg min Ψ̂ . ∆̂t(x)2

xt+1 t(x) = (9.29)
x∈X It(x)

is known as information-directed sampling (IDS).

Theorem 9.11 (Regret of IDS, lemma 8 of Kirschner and Krause (2018)).
? Let βt(δ) be chosen “correctly” for a fixed δ ∈ (0, 1). Then, if the measure Problem 9.6

of information gain is It(x) = I( fx; yx | x1:t, y1:t), with probability at least
1− δ, IDS has cumulative regret ( √ ) 3 4

RT = O βT(δ) γTT . (9.30)
2 2

Regret bounds such as Theorem 9.11 can be derived also for different
1

measures of information gain. For example, the argument of Prob- 0
lem 9.6 also goes through for the “greedy” measure 0

I .
t(x) = I( fxUCB ; yx | x1:t, y1:t) (9.31)

t 12.5 4

which focuses exclusively on reducing the uncertainty at xUCB
t rather 10.0

than globally. We compare the two measures of information gain in 7.5 2

Figure 9.9. Observe that the acquisition function depends critically 5.0
on the choice of It(·) and is less sensitive to the scaling of confidence 0

2.5
intervals.

IDS trades exploitation and exploration by balancing the (exploitative) 80 4

regret surrogate with a measure of information gain (such as those
60

studied in Chapter 8) that is purely explorative. In this way, IDS can 2

account for kinds of information which are not addressed by alterna- 40

tive algorithms such as UCB or EI (Russo and Van Roy, 2014): Depend- 20 0

ing on the measure of information gain, IDS can select points to obtain
indirect information about other points or cumulating information that 0

does not immediately lead to a higher reward but only when com- Figure 9.9: Plot of the surrogate infor-

bined with subsequent observations. Moreover, IDS avoids selecting mation ratio Ψ̂: IDS selects its minimizer.
The first two plots use the “global” infor-

points which yield irrelevant information. mation gain measure from Example 9.10

with β = 0.25 and β = 0.5, respec-
tively. The third plot uses the “greedy”
information gain measure from Equa-
tion (9.31) and β = 1.

Ψ̂
Ψ̂

Ψ̂

GP posterior GP posterior GP posterior



bayesian optimization 189

9.3.5 Probabilistic Inference
As we mentioned before, computing the probability of maximality π
is generally intractable. You can think of computing π as attempting
to fully solve the probabilistic inference problem associated with de-
termining the optimum of f | x1:t, y1:t. In many cases, it is useful to
determine the probability that a point is optimal under the current
posterior distribution, and we will consider one particular example in
the following.


400 probabilistic artificial intelligence

PETS probabilistic ensembles with trajectory sampling SG-HMC stochastic gradient Hamiltonian Monte Carlo
PI probability of improvement SGD stochastic gradient descent
PI policy iteration SGLD stochastic gradient Langevin dynamics
PILCO probabilistic inference for learning control SLLN strong law of large numbers
PL Polyak-Łojasiewicz SoR subset of regressors
PlaNet deep planning network SVG stochastic value gradients
PMF probability mass function SVGD Stein variational gradient descent
POMDP partially observable Markov decision process SWA stochastic weight averaging
PPO proximal policy optimization SWAG stochastic weight averaging-Gaussian
RBF radial basis function Tanh hyperbolic tangent
ReLU rectified linear unit TD temporal difference
RHC receding horizon control TD3 twin delayed DDPG
RKHS reproducing kernel Hilbert space TRPO trust-region policy optimization
RL reinforcement learning UCB upper confidence bound
RLHF reinforcement learning from human feedback ULA unadjusted Langevin algorithm
RM Robbins-Monro VI variational inference
SAA stochastic average approximation VI value iteration
SAC soft actor-critic w.l.o.g. without loss of generality
SARSA state-action-reward-state-action w.r.t. with respect to
SDE stochastic differential equation WLLN weak law of large numbers



Index

L1-regularization, 318 batch, 317

L2-regularization, 318 batch size, 318

L∞ norm, 202 Bayes by Backprop, 145

Rmax algorithm, 224 Bayes’ rule, 15

σ-algebra, 3 Bayes’ rule for entropy, 162

ε-greedy, 221, 288 Bayesian active learning by disagreement, 171

Bayesian experimental design, 174

acceptance distribution, 121 Bayesian filtering, 51

acquisition function, 180, 287 Bayesian linear regression, 43

activation, 140 Bayesian logistic regression, 85

activation function, 140 Bayesian networks, 9

active inference, 260 Bayesian neural network, 143

actor, 247 Bayesian optimization, 180

actor-critic method, 247 Bayesian reasoning, 2

Adagrad, 319 Bayesian smoothing, 53

Adam, 319 belief, 52, 210
adaptive learning rate, 319 belief-state Markov decision process, 211

additive white Gaussian noise, 40 Bellman error, 236, 254

advantage actor-critic, 248 Bellman expectation equation, 200

advantage function, 244 Bellman optimality equation, 205

aleatoric uncertainty, 44 Bellman update, 205

almost sure convergence, 307 Bellman’s optimality principle, 205

alpha-beta pruning, 276 Bellman’s theorem, 204

aperiodic Markov chain, 117 Bernoulli distribution, 299

approximation error, 22 Bernstein-von Mises theorem, 26

augmented Lagrangian method, 291 bias, 304

automatic differentiation (auto-diff), 142 bias-variance tradeoff, 248

binary cross-entropy loss, 98

backpropagation, 142 binomial distribution, 299

bagging, 150 black box stochastic variational inference, 104

Banach fixed-point theorem, 202 Bochner’s theorem, 73

Barrier function, 292 Boltzmann distribution, 124

baseline, 242 Boltzmann exploration, 222



402 probabilistic artificial intelligence

bootstrapping, 150, 225 convergence in distribution, 308

Borel σ-algebra, 4 convergence in probability, 308

bounded discounted payoff, 239 convergence with probability 1, 308

Bradley-Terry model, 262 convex body chasing, 179

Brownian motion, 127 convex function, 314

burn-in time, 120 convex function chasing, 179

correlation, 12

calibration, 152 cosine similarity, 12

catastrophe principle, 306 covariance, 11

categorical distribution, 299 covariance function, 60

Cauchy distribution, 306 covariance matrix, 13

central limit theorem, 309 Cramér-Rao lower bound, 312

Cesàro mean, 193 credible set, 28

chain rule for entropy, 162 critic, 247

chain rule of mutual information, 187 Cromwell’s rule, 25

chain rule of probability, 7 cross-entropy, 93

change of variables formula, 14 cross-entropy loss, 142

Chebyshev’s inequality, 320 cross-entropy method, 276

Cholesky decomposition, 302 curse of dimensionality, 183

classification, 23

closed-loop control, 279 d-separation, 9

competitive ratio, 180 data processing inequality, 175

completing the square, 334 deep deterministic policy gradients, 253, 280

computation graph, 139 deep planning network, 287

concave function, 315 deep Q-networks, 237

conditional distribution, 7 deep reinforcement learning, 236

conditional entropy, 162 density, 6

conditional expectation, 11 design, 174

conditional independence, 8 design criterion, 174

conditional likelihood, 15 design matrix, 39

conditional linear Gaussian, 22 detailed balance equation, 119

conditional mutual information, 164 differentiation under the integral sign, 301

conditional probability, 7 diffusion, 128

conditional variance, 13 diffusion process, 128

conditioning, 8 Dirac delta function, 300

conjugate prior, 19 direct preference optimization, 265

consistent estimator, 304 directed graphical model, 9

conspiracy principle, 306 directional derivative, 320

constrained Markov decision processes, 291 Dirichlet distribution, 19

context, 261 discounted payoff, 198, 240, 275

continuous setting, 218 discounted state occupancy measure, 246

contraction, 202 distillation, 128

contrapositive, 16 Doob’s consistency theorem, 25

control, 180 double DQN, 238



index 403

downstream return, 243 finite-horizon, 199

Dreamer, 287 first-order characterization of convexity, 315

dropconnect regularization, 147 first-order expansion, 314

dropout regularization, 147 first-order optimality condition, 314

dynamics model, 197 Fisher information, 312

fixed-point iteration, 202

efficient, 312 Fokker-Planck equation, 136

eligibility vector, 268 forward KL-divergence, 95

empirical Bayes, 70 forward sampling, 61

empirical risk, 313 forward-backward algorithm, 54

energy function, 125 foundation model, 261

entropy, 18, 91 Fourier transform, 73

entropy regularization, 190, 256 free energy, 105

entropy search, 174 free energy principle, 105, 259

episode, 218 fully independent training conditional, 76

episodic setting, 218 fully observable environment, 198

epistemic uncertainty, 44 function class, 22

epoch, 318 function-space view, 45

equilibrium, 135 fundamental theorem of ergodic Markov chains,
ergodic theorem, 120

117

ergodicity, 117

estimation, 26

estimation error, gamma distribution, 133
22

estimator, Gaussian, 6, 20
303

Euler’s formula, Gaussian kernel, 63
72

event, Gaussian noise “dithering”, 253, 288
3

event space, Gaussian process, 59
3

evidence, Gaussian process classification, 85, 107
101

exclusive KL-divergence, Gaussian random vector, 20
95

expectation, generalized advantage estimation, 249
10

expected calibration error, generalized variance, 92
154

expected improvement, generative model, 7
185

experience replay, Gibbs distribution, 124
237

experimental design, Gibbs sampling, 123
174

exploration distribution, 252 Gibbs’ inequality, 109

exploration noise, 287 global optimum, 313

exploration-exploitation dilemma, 177, 217, 287 goal-conditioned reinforcement learning, 296

exponential family, 99, 270 gradient, 301

exponential kernel, 63 gradient flow, 135

greedy in the limit with infinite exploration, 222

feature space, 45 greedy policy, 204

feed-forward neural network, 142 group relative policy optimization, 251

filtering, 37, 51, 209, 210 Grönwall’s inequality, 135

finite measure, 114 Gumbel-max trick, 255



404 probabilistic artificial intelligence

Hadamard’s inequality, 319 Jacobian, 316

hallucinated upper confidence reinforcement Jensen’s inequality, 91

learning, 290 joint distribution, 7

Hamiltonian, 130 joint entropy, 162

Hamiltonian Monte Carlo, 129 joint likelihood, 16

heat kernel, 78

heavy-tailed distribution, 305 Kalman filter, 52

Hessian, 316 Kalman gain, 55, 56
heteroscedastic, 23 Kalman smoothing, 53

heteroscedastic noise, 144 Kalman update, 55

hidden layer, 140 kernel function, 46, 60, 62
hidden Markov model, 209 kernel matrix, 46

histogram binning, 154 kernel ridge regression, 79

Hoeffding’s inequality, 310 kernel trick, 46

Hoeffding’s lemma, 310 kernelization, 46

homoscedastic, 23 Kolmogorov axioms, 3

homoscedastic noise, 144 Kullback-Leibler divergence, 93

hyperbolic tangent, 140

hyperprior, 50, 70, 180 labels, 22

Lagrangian model-based agent, 293

importance sampling, 249 Langevin dynamics, 128

improper prior, 18 Langevin Monte Carlo, 127

improvement, 184 Laplace approximation, 84

inclusive KL-divergence, 95 Laplace distribution, 305

independence, 8 Laplace kernel, 63

inducing points method, 75 Laplace’s method, 83

inductive learning, 172 lasso, 42

information gain, 163 law of large numbers, 309

information never hurts, 162, 163 law of the unconscious statistician, 10

information ratio, 186 law of total expectation, 11

information-directed sampling, 188 law of total probability, 8

informative prior, 17 law of total variance, 13

input layer, 140 layer, 139

inputs, 22 Leapfrog method, 131

instantaneous, 199 learning, 27

instantaneous regret, 179 least squares estimator, 39

interaction information, 165 length scale, 63

intrinsic reward, 166 light-tailed distribution, 305

inverse Fourier transform, 73 likelihood, 15

inverse transform sampling, 114, 300 linear kernel, 62

irreducible Markov chain, 116 linear regression, 39

isotonic regression, 154 linearity of expectation, 10

isotropic Gaussian, 20 local optimum, 314

isotropic kernel, 65 Loewner order, 312



index 405

log-concave distribution, 125 mixing time, 118

log-likelihood, 23 mode, 6

log-normal distribution, 306 model predictive control, 275

log-prior, 24 model-based reinforcement learning, 219, 274

log-Sobolev inequality, 137 model-free reinforcement learning, 219

logical inference, 1 moment matching, 98, 281

logistic function, 85 moment-generating function, 33

logistic loss, 86 momentum, 319

logits, 140 monotone submodularity, 167

loss function, 313 monotonicity of mutual information, 187

Lyapunov function, 135 Monte Carlo approximation, 281, 307
Monte Carlo control, 221

Mahalanobis norm, 301 Monte Carlo rollouts, 280

marginal gain, 166 Monte Carlo trajectory sampling, 278

marginal likelihood, 16 Monte Carlo tree search, 276

marginalization, 7 most likely explanation, 210

Markov chain, 114 multi-armed bandits, 178

Markov decision process, 197 multicollinearity, 40

Markov property, 52, 114 multilayer perceptron, 139

Markov’s inequality, 320 multinomial distribution, 19, 299

masksemble, 149 mutual information, 161, 163
matrix inversion lemma, 319

Matérn kernel, 63
natural parameters, 99

maximization bias, 237
negative log-likelihood, 23, 142

maximizing the marginal likelihood, 68

neural fitted Q-iteration, 237
maximum a posteriori estimate, 24

neural network, 139
maximum calibration error, 154

maximum entropy principle, noninformative prior, 17
18

maximum entropy reinforcement learning, normal distribution, 6, 20
256

maximum likelihood estimate, normalizing constant, 16, 27
23

mean, 10

mean function, objective function, 313
60

mean payoff, Occam’s razor, 18
199

mean square continuity, 308 off-policy, 219

mean square convergence, 308 on-policy, 219

mean square differentiability, 309 online, 52

mean squared error, 141, 304 online actor-critic, 248

mean-field distribution, 89 online algorithms, 179

median-of-means, 312 online Bayesian linear regression, 49

method of moments, 98 online learning, 178

metrical task systems, 179 open-loop control, 279

Metropolis adjusted Langevin algorithm, 127 optimal control, 275

Metropolis-Hastings algorithm, 121 optimal decision rule, 29

Metropolis-Hastings theorem, 122 optimal design, 174



406 probabilistic artificial intelligence

optimism in the face of uncertainty, 30, 179, 181, principle of indifference, 17

185, 190, 223, 229, 288 principle of insufficient reason, 17

optimistic exploration, 289 principle of parsimony, 18

optimistic Q-learning, 230 prior, 15

outcome reward, 263 probabilistic artificial intelligence, 2

output layer, 140 probabilistic ensembles with trajectory sampling,
output scale, 62, 64 286

outputs, 22 probabilistic inference, 1, 15, 27, 37

overfitting, 24, 313 probabilistic inference for learning control, 281,
286

Pareto distribution, 306 probability, 4

partially observable Markov decision process, 209 probability density function, 5

partition function, 99 probability matching, 186

perception-action loop, 159 probability measure, 3

performance plan, 294 probability of improvement, 184

Pinsker’s inequality, 118 probability of maximality, 186, 189
planning, 197, 217 probability simplex, 212, 299
plate notation, 10 probability space, 4

Platt scaling, 155 probit likelihood, 107

plausible inference, 16 product rule, 8

plausible model, 288 proposal distribution, 121

point density, 300 proximal policy optimization, 250

point estimate, 26 pseudo-random number generators, 300

point-based policy iteration, 212 pushforward, 15

point-based value iteration, 212

pointwise convergence, 80 Q actor-critic, 247

policy, 198 Q-function, 199

policy gradient method, 239 Q-learning, 229

policy gradient theorem, 246 quadratic form, 301

policy iteration, 206, 251 quantile function, 300

policy search method, 239

policy value function, 239 radial basis function kernel, 63

Polyak averaging, 237 random Fourier features, 72

Polyak-Łojasiewicz inequality, 135 random shooting methods, 276

population risk, 312 random variable, 4

positive definite, 301 random vector, 7

positive definite kernel, 62 rapidly mixing Markov chain, 118

positive semi-definite, 301 realizations of a random variable, 5

posterior, 16, 28 recall, 189

precision matrix, 20 receding horizon control, 275

prediction, 27 rectified linear unit, 140

predictive posterior, 28 recursive Bayesian estimation, 51

principle of curiosity and conformity, 106, 128, redundancy, 165

152, 178 regression, 23



index 407

regret, 179 small tails, 305

Reichenbach’s common cause principle, 9 snapshot, 146

REINFORCE algorithm, 243 soft actor critic, 258, 281

reinforcement learning, 217 soft Q-learning, 258

reinforcement learning from human feedback, 263 soft value function, 258

relative entropy, 93 softmax exploration, 222

relative Fisher information, 137 softmax function, 141

reliability diagram, 153 spectral density, 73

reparameterizable distribution, 103 square root, 302

reparameterization trick, 103, 255, 278 squared exponential kernel, 63

replay buffer, 237 standard deviation, 13

representer theorem, 66 standard normal distribution, 6, 20
reproducing kernel Hilbert space, 66 state of a random variable, 5

reproducing property, 66 state space model, 51

reverse KL-divergence, 95 state value function, 199

reversible Markov chain, 119 state-action value function, 199

reward shaping, 214 state-dependent baseline, 269

reward to go, 243 stationary distribution, 115

ridge regression, 40 stationary kernel, 65

Robbins-Monro algorithm, 317 stationary point, 314

Robbins-Monro conditions, 317 Stein variational gradient descent, 151

robust control, 291 Stein’s method, 152

robust statistics, 311 stochastic average approximation, 277

rollout, 240 stochastic environment, 197

stochastic gradient descent, 318

saddle point, 314 stochastic gradient Langevin dynamics, 129

safety filter, 296 stochastic matrix, 115

safety plan, 294 stochastic process, 114

sample covariance matrix, 304 stochastic semi-gradient descent, 235

sample mean, 303 stochastic value gradients, 255, 280

sample space, 3 stochastic weight averaging, 147

sample variance, 304 stochastic weight averaging-Gaussian, 147

SARSA, 227 strict convexity, 314

score function, 103, 241 sub-Gaussian random variable, 310

score function trick, 241 sublinear regret, 179

score gradient estimator, 103, 241 submodularity, 166

second-order characterization of convexity, 316 subsampling, 146

second-order expansion, 316 subset of regressors, 76

self-conjugacy, 19 sufficient statistic, 99, 146, 164
self-supervised learning, 261 sum rule, 7

semi-conjugate prior, 134 supervised learning, 22, 161

sharply concentrated, 305 support, 5

Sherman-Morrison formula, 319 supremum norm, 202

shift-invariant kernel, 65 surprise, 90



408 probabilistic artificial intelligence

symbolic artificial intelligence, 1 unadjusted Langevin algorithm, 127

synergy, 165 unbiased estimator, 304

uncertainty sampling, 169

tabular setting, 217 uncorrelated, 11

tail distribution function, 305 uniform convergence, 80

tail index, 306 uniform distribution, 299, 300
target space, 4 union bound, 31

temperature scaling, 155 universal approximation theorem, 140

temporal models, 57 universality of the uniform, 300, 353

temporal-difference error, 235 upper confidence bound, 181

temporal-difference learning, 226

testing conditional, 76
value iteration, 208

Thompson sampling, 186, 288
variance, 12

time-homogeneous process, 114
variational family, 89

total variance, 92
variational inference, 88, 145, 150, 281

total variation distance, 118

variational parameters, 83
tower rule, 11

variational posterior, 83
training conditional, 76

trajectory, Viterbi algorithm, 210
218

trajectory sampling, 277

transductive learning, weak law of large numbers, 309
172

transition, 218 weight decay, 319

transition function, 115 weight-space view, 40

transition graph, 115 Weinstein-Aronszajn identity, 320

transition matrix, 115 well-calibrated confidence interval, 182

truncated sample mean, 312 well-specified prior, 25

trust-region policy optimization, 249 Wiener process, 78, 114, 127
twin delayed DDPG, 253 Woodbury matrix identity, 319

two-filter smoothing, 54 world model, 273